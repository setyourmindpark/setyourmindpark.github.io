{"meta":{"title":"jaehunpark","subtitle":null,"description":null,"author":"jaehunpark","url":"https://setyourmindpark.github.io"},"pages":[{"title":"categories","date":"2018-02-20T14:36:50.000Z","updated":"2018-02-20T14:36:50.000Z","comments":true,"path":"categories/index.html","permalink":"https://setyourmindpark.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-02-05T10:53:55.000Z","updated":"2018-02-05T10:53:55.000Z","comments":true,"path":"tags/index.html","permalink":"https://setyourmindpark.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"portainer","slug":"docker/docker-8","date":"2018-05-15T06:45:27.000Z","updated":"2018-05-15T07:50:06.509Z","comments":true,"path":"2018/05/15/docker/docker-8/","link":"","permalink":"https://setyourmindpark.github.io/2018/05/15/docker/docker-8/","excerpt":"","text":"portainerdocker swarm 또는 single machine 환경에서 monitoring과 gui 환경을 제공하여 설정할수있는 image를 알게되었다.더군다나 swarm 환경에서는 stack, service, container, network, node, secret등 모든 설정을 gui 환경으로 설정할수있으며 더이상 명령어로 해결하지않음에 너무 만족스럽다.불과 1년전만해도 tool이 없어 굉장히 불편했던 기억이있다. 아니나다를까 docker hub portainer 최초 build 일자를 확인해보니 https://hub.docker.com/r/portainer/portainer/tags/ 지금으로부터 6개월전, 2017년 11월쯤 최초 publish 된듯하다.사실 portainer를 접하기전 docker swarm 관련 환경 설정은 둘째치고 monitoring 할수있는 tool을 열심히 찾다 종종 granfa를 사용하였는데 생각보다 마음에 들지않았다.( 너무 개발자같은 ui와 가끔 node의 container resources 가 watching 되지않는 문제등 )얼마전까지도 kubernetes의 dashboard 가 마냥 부럽기도 했던기억이.. setup portainerofficial portainer document를 보면 setup은 정말 너무도 쉽다.https://portainer.readthedocs.io/en/latest/deployment.html12$ curl -L https://portainer.io/download/portainer-agent-stack.yml -o portainer-agent-stack.yml$ docker stack deploy --compose-file=portainer-agent-stack.yml portainer docker swarm 환경에서 단 2줄로 portainer가 구동된다.그리고 swarm 환경의 아무 node-ip:9000 접속시 ( overlay network의 ingress로 동작 ) admin password 설정후 다음과 같은 화면이 나타난다.설치후 이것저것 설정해보니 정말 모든 명령어를 더이상 입력하여 설정하지않을만큼 편리했다. 견해monitoring 관련해서는 살짝 아쉬운 부분이 있긴하다. container 단위로 cpu, memory, network 단위로 watching 하여 graph로 표현이된다. node 단위나 stack, 또는 service 단위로 resources들을 확인할수있으면 더욱 편리할것같다.그래도 gui 환경으로 모든 설정이 가능한게 어디인가.. portainer 개발자분들께 감사의 말씀을 드리고싶다.필자는 사실 docker 기반으로 service를 배포하거나 production 환경에서 운영해본적이없다.언젠가 docker 환경에서 service를 운영해볼날이 있지않을까 생각해본다.","categories":[{"name":"portainer","slug":"portainer","permalink":"https://setyourmindpark.github.io/categories/portainer/"}],"tags":[]},{"title":"the way to parallel processing using callback, Promise and async await","slug":"nodejs/nodejs-6","date":"2018-05-14T04:03:42.000Z","updated":"2018-05-16T04:26:40.463Z","comments":true,"path":"2018/05/14/nodejs/nodejs-6/","link":"","permalink":"https://setyourmindpark.github.io/2018/05/14/nodejs/nodejs-6/","excerpt":"","text":"the way to parallel processing, using async, await and Promisecallback hell을 회피하기위해 Promise, async await 과 같은 라이브러리들을 사용함으로 어느정도 callback hell에대한 이슈들을 커버한다.필자는 callback 보다는 promise를, promise 보다는 async await을 이런 생각들에 사로잡혀 패턴을 수시로 바꾸곤하였다.특히 async await은 동기코드로 작성할수있는 효과와 무엇보다 가독성이 좋아서 매우 만족스러웠다.하지만 ‘async await이 무조건 좋다’ 보다는 상황에 맞게 써야한다는것을 새삼깨달았다.특히 병렬처리( 다중 비동기처리 ) 를 수행함에있어서 async await는 사용하지말아야할 것임은 분명하다.코드를통해 어떻게 동작하는지, 그리고 어떤상황에 효율적으로 써야하는지 정리하려한다. Promise 그리고 bluebird병렬처리라고 하면 Promise.all 또는 bluebird.map을 활용하여 처리할수있다.다음과같이 기본적인 병렬처리로직을 구성할수있다.12345678910111213141516171819202122232425262728293031323334353637383940414243const bluebird = require('bluebird');function task(&#123; num &#125;) &#123; return new Promise((resolve, reject) =&gt; &#123; const second = 1000; const tenSeconds = 10 * second; const timeoutSeconds = tenSeconds - (num * second); // num 이 1이면 timeout = 9초, num이 2이면 timeout = 8초 setTimeout(() =&gt; &#123; console.log(num + ' 실행'); resolve(num); &#125;, timeoutSeconds); &#125;)&#125;;const list = [ &#123; num: 0 &#125;, &#123; num: 1 &#125;, &#123; num: 2 &#125;, &#123; num: 3 &#125;, &#123; num: 4 &#125;, &#123; num: 5 &#125;, &#123; num: 6 &#125;, &#123; num: 7 &#125;, &#123; num: 8 &#125;, &#123; num: 9 &#125;,];( async () =&gt; &#123; const result1 = await bluebird.map(list, item =&gt; task(item)) // bluebird style const result2 = await Promise.all(list.map(item =&gt; task(item))) // promise style // 9 실행 // 8 실행 // 7 실행 // 6 실행 // 5 실행 // 4 실행 // 3 실행 // 2 실행 // 1 실행 // 0 실행 // [ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 ]&#125;)(); 결과는 동일하며 num 이 0이 최대 10초가걸리므로 10개의 프로세스가 10초내에 모두 완료되어 병렬처리가된것을 확인할수있다. 어느상황에 적합한가 ?server에서 scheduler를 통해 mobile에 알림을 push 한다던지, 특정 조건에 만족하는 사용자에게 mail을 send 한다던지 같은 비지니스로직내에서 수신주체가 다를경우 유용하게 사용할수있다.예를들어 10000명에게 특정 mail을 send한다고 가정하면 async await을 사용시 동기로 동작하기에 1번 사용자 발송 완료되면 2번 그리고 3번 … 10000 번 사용자까지 반복할것이다.이렇게되면 10000번 사용자는 메일발송 로직이 수행되고나서 한참뒤에 메일을 받게되어 서비스상 문제가될수있다.그래서 다음과 같이 정의할수있다.123456789101112131415161718192021222324252627282930313233343536373839404142434445const bluebird = require('bluebird');const nodemailer = require('nodemailer');const config = reqlib('/config');const &#123; service, user, passwd, from &#125; = config.setting.sender.mail;const sender = nodemailer.createTransport(&#123; service: service, auth: &#123; user: user, pass: passwd &#125;&#125;);function send(&#123; to, subject, text, html &#125;)&#123; return new Promise(( resolve, reject ) =&gt; &#123; const options = &#123; from: from, to: to, subject: subject &#125;; if (text) options.text = text; else if (html) options.html = html; else if (attachments) options.attachments = attachments; sender.sendMail(options, ( error, response ) =&gt; &#123; let sended = true; if (error) sended = false; resolve(&#123; sended: sended, mail: to &#125;) sender.close(); &#125;); &#125;) &#125;const resources = [ &#123; subject: 'title', to: 'setyourmindpark@gmail.com', text: 'hello world', &#125;, // &#123; &#125;.. ]( async () =&gt; &#123; const resultArr = await bluebird.map(resources, item =&gt; send(item)); // [ &#123; sended: true, mail: 'setyourmindpark@gmail.com' &#125;, ... ] // 추가로직 수행&#125;)(); 메일발송 자체는 callback style로 구성하고 전체적인 동작 자체는 Promise 사용하여 발송여부와 함꼐 메일정보를 리턴한다.그리고 마지막으로 async await으로 전체 수행결과를 기다린다. ( 메일발송 결과기반 추가로직수행위해 )이렇게되면 10000명의 사용자에게 메일발송 동작은 병렬로 처리되며 발송 성공/실패 여부와 상관없이 전체 target 사용자에게 메일발송 로직은 처리할수있다.이렇게 발송결과 데이터를 기반으로 발송실패 사용자를 대상으로 추가적인 비지니스로직을 수행하면될것이다. ( 다른방법으로 정보를알리든지, log를 저장하여 분석한다든지 )","categories":[{"name":"nodejs","slug":"nodejs","permalink":"https://setyourmindpark.github.io/categories/nodejs/"}],"tags":[]},{"title":"replace session on front side( json web token )","slug":"jwt/jwt","date":"2018-05-13T06:35:15.000Z","updated":"2018-05-14T11:42:35.415Z","comments":true,"path":"2018/05/13/jwt/jwt/","link":"","permalink":"https://setyourmindpark.github.io/2018/05/13/jwt/jwt/","excerpt":"","text":"replace session on front side전통적으로 웹사이트 로그인부분 처리와 관련해서 server side에서 session에 담는게 일반적이다.모든사이트가 그렇진않지만 요즘은 RESTFUL 기반 client와 분리하여 독립적으로 개발하는곳도 심심지않게 볼수있다.이렇게 server side와 client side를 분리하게되면 분명 다양한 이점이 존재한다.첫번쨰로는 불필요한 빌드를 방지할수있으며( 수정사항 side만 재배포 )두번째로는 독립적으로 서비스로직이 존재하므로 확장과 시스템자원을 효율적으로 관리할수있다.( backend frontend 담당 역활이 분명히 나뉘어져있어 아키텍처 관점으로 이해하기쉽다. )이렇게 분리된 backend 와 fronted가 존재할시 로그인관련처리 역시 session을 사용하게되는데 backend가 n 의 클러스터기반에서는 session을 관리하는 server를 따로두어야할것이다.( redis session을 사용한다던지.. )session을 쓰지않고 로그인 handling을 할수없을까… 하다가 생각한게 jwt( json web token ) 이다. jwt( json web token )일단 jwt가 무었인지부터 알아보자.jwt는 oath2 의 인증 방식을 대체하기위해 나온 메커니즘으로 인증 token을 인증서버에 요청하여 누가 요청하였는지 identify하는 oatuh2와는 달리 token에 존재하는 사용자정보를 기반으로 인증 server를 거치지않고 service backend에서 바로 비지니스로직처리를 가능하게한다.하지만 oauth2의 장점은 보안에 용이하지만 인증서버를 구축해야한다는점. 시스템적인 자원의 소모가 큰 반면 jwt는 시스템적인 자원은 효율적으로 사용하되 보안 관련 이슈가 존재한다.다음의 jwt token을 https://jwt.io/ 에서 확인하면 token의 내포된 header와 payload가 확인된다. eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb21lSWQiOjEsImlhdCI6MTUyNjE5NTMyNCwiZXhwIjoxNTI2MTk1Mzg0fQ.0B_HNaVdJ2AgUJOQctYMKt56_2fLEXnldBpSBzcNMiU 그러면 제3자가 악용하여 token 내용을 변조하여 api server에 요청할수있지않을까?결론적으론 요청은할순있지만 변조된 token으로 server side에서 checking이 가능하다.jwt의 token 구조는 header.payload.signature로 구성되어있으며 header와 payload를 base64로 인코딩된 값을 server side에서 소유하고있는 secret key를 통해 hashing 한다. 그리고 hashing 된 hash 값을 base64로 인코딩하게되면 signature 가 된다.만약 누군가 token을 변조하였다면 signature부분의 값이 달라지기에 변조된 token임을 알수있다. ( secret key로 hashing된 값이 달라지기에 ) delegate login process on front sidejwt를 사용하게되면 사용자 인증( authe ntication )이 server side에서 담당하게될 역활중 하나이며 이런 인증은 보통 client에서 header에 token 정보를 함께 요청( request ) 한다.다시말해 인증되지않은 사용자는 server side로 api call을 할수없다.그럼 인증과 login 처리 session은 무슨 관련이있는것인가.session은 일반적으로 만료시간( expire time )이 정해져있다. ( 언제든지 접속할때마다 로그인이 되어있다면 server side에서 엄청난 자원낭비가될것이다 )하지만 jwt나 oauth2 방식을 이용하면 무한적으로 로그인되어있는 웹사이트도 개발가능하다.인스타그램이나 페이스북, 또는 국내 특정 코인 거래소에서 이런 현상을 볼수있는데 ( 사이트를 들어갈때마다 로그인이 자동으로 되어있다. )아마도 그런 대기업들은 jwt 보다는 보안적으로좋은 oauth2 방식을 이용하는듯하다 ( 추측 ).필자는 jwt를 기반으로 client side에서 일정시간 또는 무한정 로그인 처리를 할수있도록 다음과같이 프로세스를 정의하였다. client가 로그인한다. server에서 expire time이 작은 accesstoken과 accesstoken보다 expire time이 긴 refreshtoken을 발급한다. client는 server로부터 발급받은 accesstoken과 refreshtoken을 localstorage에 저장한다. client는 모든 api call시 accesstoken 을 header에 실어서 요청한다. accesstoken이 만료되면 refreshtoken을 통해 accesstoken과 refreshtoken을 재발급 받는다. 예를들어 accesstoken expire time이 발급일로부터 3일 이라고 가정 refreshtoken expire time이 발급일로부터 7일 이라고 가정 client 사용자 로그인 -&gt; accesstoken refreshtoken localstorage 저장 3일동안은 client가 api call을 통해 비지니스로직을 수행 3일이후 어느순간부터 server로 부터 error code와 함께 expire token message를 response errorcode에 따른 분기로 refreshtoken을 header에 넣고 body에 accesstoken을 request server는 accesstoken과 refreshtoken 재발급 -&gt; accesstoken refreshtoken localstorage 저장 무한반복 ( 그렇다면 token 발급일로부터 8일이상 client가 어떠한 api call을 요청하지않으면 ? -&gt; 재로그인 ) token expire time에 따라 재 로그인 term이 달라지겠지만 refreshtoken의 expire time이 길면 무한정 로그인된 상태도 가능하다하지만 refreshtoken이 너무긴 expire time 보다는 일정기간동안 접속하지않으면 재로그인하는 방식으로 유도하는것이 좋을듯하다 참고jwt의 expire date 는 utc timezone 기반으로 동작하여 local timezone에 관한 이슈는 생각하지않아도 될듯하다.jwt는 npmjs https://www.npmjs.com/package/jsonwebtoken 에서 가능하다. https://velopert.com/2389 https://blog.outsider.ne.kr/1160 http://bcho.tistory.com/999 https://github.com/setyourmindpark/nodejs-api-server-boilerplate","categories":[{"name":"jwt","slug":"jwt","permalink":"https://setyourmindpark.github.io/categories/jwt/"}],"tags":[]},{"title":"kubernetes pod, deployment and service","slug":"kubernetes/kubernetes-1","date":"2018-03-25T04:43:51.000Z","updated":"2018-03-26T12:57:48.226Z","comments":true,"path":"2018/03/25/kubernetes/kubernetes-1/","link":"","permalink":"https://setyourmindpark.github.io/2018/03/25/kubernetes/kubernetes-1/","excerpt":"","text":"kubernetes pod, deployment and servicekubernetes 의 pod, deployment 그리고 service에대해 알아보려한다.일단 pod는 쉽게말해서 container 와 같고 deployment는 docker swarm 의 service와 상응하는 기능으로 보인다.햇갈릴수있는 부분이, docker swarm 에서 지칭하는 service 와 kubernetes에서 지칭하는 service의 혼동이다.docker swarm 에서의 service는 n개의 replicas container의 집합과 관리를 담당 ( rollback , update 등 ) 하는반면kubernetes의 service는 deployment 들을 expose( 노출 ) 하던지 load balancing을 위한 용도로 지칭하는듯 하다.또 다른 특이점으론 docker swarm 에서 master node는 –availability active mode로 동작하는 반면 kubernetes master node는 기본적으로 –availability drain mode로 동작한다. ( 사용자가 생성한 pod, deployment등 container들을 master node에서 생성하지않는다. )nginx를 이용하여 간단히 테스트를 진행한다. deploymentdeployment( pods ) 생성한다.1234567891011121314151617181920212223242526$ vi nginx-deployment.yamlapiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2kind: Deploymentmetadata: name: nginx-deploymentspec: selector: matchLabels: app: nginx replicas: 2 # tells deployment to run 2 pods matching the template template: # create pods using pod definition in this template metadata: # unlike pod-nginx.yaml, the name is not included in the meta data as a unique name is # generated from the deployment name labels: app: nginx spec: containers: - name: nginx# image: nginx image: setyourmindpark/debian-nginx ports: - containerPort: 80$ kubectl create -f nginx-deployment.yaml 1234$ kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODEnginx-deployment-5599c844c4-7qxkm 1/1 Running 0 12m 10.244.1.18 node1nginx-deployment-5599c844c4-lzmxj 1/1 Running 0 12m 10.244.1.19 node1 123$ kubectl get deploymentsNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEnginx-deployment 2 2 2 2 12m service앞에서 언급한 바와같이 deployment로 생성된 pods 들은 기본적으로 cluster 내부에서만 접근가능하므로 외부에 expose( 노출 ) 을 한다.12345678910111213141516$ vi nginx-service.yamlapiVersion: v1kind: Servicemetadata: name: nginx-servicespec: ports: - name: nginx-service port: 80 targetPort: 80 type: NodePort selector: app: nginx$ kubectl create -f nginx-service.yaml 1234$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 6hnginx-service NodePort 10.100.189.254 &lt;none&gt; 80:32035/TCP 15m 확인expose 확인필자의 pods들은 10.10.80.12 node에 생성되었다.브라우저에서 10.10.80.12:32035 접속시 정상적으로 nginx default page가 로드된다.cluster 외부에서는 node-ip:32035cluster 내부에서는 10.100.189.254:80 load balancing 확인생성된 2개의 pod에 접속하여 access log를 확인한다.기본적으로 access log는 주석처리되어있으므로 설정변경과 nginx를 reload 한다 .master node에서 kubectl exec … 로 pods( container ) 접근하여 수정하거나 해당 node에서 docker exec …로 생성된 pods로 직접접속한다.12345678$ kubectl exec -it nginx-deployment-5599c844c4-7qxkm /bin/bash$ kubectl exec -it nginx-deployment-5599c844c4-lzmxj /bin/bash$ vi /etc/nginx/conf.d/default.conf...access_log /var/log/nginx/host.access.log main; # 다음과같이 주석해제...$ nginx -s reload$ tail -f /var/log/nginx/host.access.log # watching access log node1에서 service ip로( 10.100.189.254:80 ) curl 을 날린다.12345678$ curl 10.100.189.254 # default port 80$ curl 10.100.189.254$ curl 10.100.189.254$ curl 10.100.189.254$ curl 10.100.189.254$ curl 10.100.189.254$ curl 10.100.189.254$ curl 10.100.189.254 2대의 pods( container )에서 access log를 확인한다. nginx-deployment-5599c844c4-7qxkm1234510.244.1.1 - - [24/Mar/2018:18:21:58 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.29.0\" \"-\"10.244.1.1 - - [24/Mar/2018:18:21:59 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.29.0\" \"-\"10.244.1.1 - - [24/Mar/2018:18:22:00 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.29.0\" \"-\"10.244.1.1 - - [24/Mar/2018:18:22:01 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.29.0\" \"-\"10.244.1.1 - - [24/Mar/2018:18:22:02 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.29.0\" \"-\" nginx-deployment-5599c844c4-lzmxj12310.244.1.1 - - [24/Mar/2018:18:21:59 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.29.0\" \"-\"10.244.1.1 - - [24/Mar/2018:18:22:00 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.29.0\" \"-\"10.244.1.1 - - [24/Mar/2018:18:22:06 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.29.0\" \"-\" load balancing도 잘 수행된다. 견해간단히 kubernetes를 살펴본 느낌으로는 docker swarm 보다 기능이 디테일하게 세분화되어있는 느낌을 받았다.예를들어 docker swarm에서 default로 생성되는 overlay network 라든지 외부접근 관련된 service expose 라던지..좀더 살펴봐야하겠지만. docker swarm 과 비슷하면서도 난해한부분이 없지않아 있는듯하다. 특히 ingress 관련 … 참고http://lng1982.tistory.com/m/270http://bryan.wiki/288","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://setyourmindpark.github.io/categories/kubernetes/"}],"tags":[]},{"title":"kubernetes","slug":"kubernetes/kubernetes","date":"2018-03-19T10:53:47.000Z","updated":"2018-03-24T10:49:39.383Z","comments":true,"path":"2018/03/19/kubernetes/kubernetes/","link":"","permalink":"https://setyourmindpark.github.io/2018/03/19/kubernetes/kubernetes/","excerpt":"","text":"install kuberneteskubernetes 는 docker 기반으로 동작하는 컨티에너 오케스트레이션( container orchestration ) 플랫폼이다.이전포스트들에서 봐왔듯이 필자는 docker에 관심이많다. 자연스레 kubernetes에 대해 접하게되었고 요즘들어 인기가더욱 많은듯하다.필자는 주로 docker swarm cluster로 컨테이너 관리를 해왔지만 kubernetes가 관리측면에서는 docker swarm cluster 보다는 훨씬더 나은듯 보였다. ( 익숙하다면.. )일단 kubernetes에서가장 눈에띄는 점은 container 관리 dashboard가 존재한다는 점과 container auto scale 이가능하다는 점이다.아직 많은 부분을 알진못하지만 학습하면서 하나둘 알아보려한다.필자는 centos 7 2대의 vm으로 진행하였다. yum update1$ sudo yum update -y install docker and start service12$ sudo yum install -y docker$ sudo systemctl enable docker &amp;&amp; systemctl start docker register repository123456789$ sudo bash -c 'cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpgEOF' diable selinux1$ sudo setenforce 0 install kubelet kubeadm kubectl and start service12$ sudo yum install -y kubelet kubeadm kubectl$ sudo systemctl enable kubelet &amp;&amp; sudo systemctl start kubelet config iptables and apply12345$ sudo bash -c 'cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOF'$ sudo sysctl --system disalbe firewalld12$ sudo systemctl disable firewalld$ sudo systemctl stop firewalld swap off1$ sudo swapoff -a kubernetes 설치가 모두 끝이났다. 뭐가이렇게 설치하고 설정할게 많은지.. 시작부터불길한예감이 .. kubernetes cluster config[ 참고 ] 탭 kubernetes official guide 참고 init ( master node )이제 kubernetes 를 본격적으로 사용하기위해 kubernetes cluster를 master node에서 초기화한다. 123$ sudo kubeadm init --pod-network-cidr 10.244.0.0/16 --apiserver-advertise-address 10.10.80.11# --pod-network-cidr pod 들이 사용하는 ip 대역# --apiserver-advertise-address master ip( node에서 master ip에 join 할것이기에 명시적으로 적어준다 ) 다음과같이 설정정보가 나타난다.12345678910111213141516Your Kubernetes master has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of machines by running the following on each nodeas root: kubeadm join --token 02bcf2.f10d2bf5defde80b 10.10.80.11:6443 --discovery-token-ca-cert-hash sha256:2a849134a0c617684e42b701afd43074bbe9a11422e8b6dd51e879a28875fd1c 제시된 설정정보를 입력한다.123$ mkdir -p $HOME/.kube$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config$ sudo chown $(id -u):$(id -g) $HOME/.kube/config config flannel network ( master node )docker swarm cluster 구성시 default로 생성되는 overlay network라고 생각하면 될듯하다.설정을 진행한다.1$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml check pod status ( master node )1234567891011$ kubectl get pods --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system etcd-master 1/1 Running 0 4mkube-system kube-apiserver-master 1/1 Running 0 4mkube-system kube-controller-manager-master 1/1 Running 0 4mkube-system kube-dns-6f4fd4bdf-ppmgr 3/3 Running 0 9mkube-system kube-flannel-ds-b4v6s 1/1 Running 0 1mkube-system kube-flannel-ds-g958w 1/1 Running 0 1mkube-system kube-proxy-4j29q 1/1 Running 0 9mkube-system kube-proxy-n58wv 1/1 Running 0 4mkube-system kube-scheduler-master 1/1 Running 0 4m kube-system kube-dns-6f4fd4bdf-ppmgr 3/3 Running 0 9mPending -&gt; ContainerCreating -&gt; Running 상태변경을 확인한다. join cluster( worker node )master node에서 kubernetes init 시 나온 token 정보를 worker node에 입력한다.1sudo kubeadm join --token 02bcf2.f10d2bf5defde80b 10.10.80.11:6443 --discovery-token-ca-cert-hash sha256:2a849134a0c617684e42b701afd43074bbe9a11422e8b6dd51e879a28875fd1c check node status ( master node )1234$ kubectl get nodes NAME STATUS ROLES AGE VERSIONmaster Ready master 11m v1.9.4node1 Ready &lt;none&gt; 5m v1.9.4 node state가 NotReady에서 Ready로 변경된것을 확인할수있다. 참고https://kubernetes.io/docs/setup/independent/install-kubeadm/https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#24-initializing-your-masterhttps://blog.tekspace.io/setup-kubernetes-cluster-on-centos-7/https://www.assistanz.com/steps-to-install-kubernetes-cluster-manually-using-centos-7/","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://setyourmindpark.github.io/categories/kubernetes/"}],"tags":[]},{"title":"non-stop deploy","slug":"deploy/deploy","date":"2018-03-02T11:47:45.000Z","updated":"2018-03-10T05:01:27.000Z","comments":true,"path":"2018/03/02/deploy/deploy/","link":"","permalink":"https://setyourmindpark.github.io/2018/03/02/deploy/deploy/","excerpt":"","text":"non-stop deploy개발한 서비스를 무중단 배포( non-stop deploy ) 하는일은 필자가 생각하기에 매우 중요한 요소중 하나라고 생각된다.우선 필자의 목적은 재배포 ( redeploy )할시 그와동시에 client request에 대한 error 를 뱉지않는것을 목표로 잡았다.docker swarm으로 services들을 구성하니 container replicas ( docker service update시 rolling )으로 당연히 non-stop deploy가 이루어진줄 알았다.혹시나하는 마음으로 docker service update( redeploy )시 api call을 일정한 타임주기로 꾸준히 날려보았다.그런데 error가 …. 역시 직접 테스트해보기전까진 모른다.필자가 시도해본방법을 정리해보려한다. docker HEALTHCHECK처음 api call error를 마주하는순간 ‘아 ~ HEALTHCHECK command를 Dockerfile에 안넣어서 그렇구나’ 생각하여 health check api를 만들고 HEALTHCHECK command를 삽입하였다.12345678910FROM ......HEALTHCHECK --interval=3s --timeout=3s --retries=5 CMD curl -f http://localhost:4000/api/smpl/healthcheck || exit 1# --interval = container가 시작된후 검사( health check api call )의 interval 주기# --timeout = 검사의 실행시간 ( health check api call 후 timeout 시간내에 응답이없으면 실패로 간주 )# --retries = 재시도 횟수( 연속적인 실패시 not healty로 간주 ) # = docker container가 시작된후 health check api call을 interval( 3초 ) 마다 실행. # 실행에 대한 결과가 5초내에 이루어지지않을경우 실패로간주, 연속적으로 5번 실패시 not healthy로 간주...CMD ... 이렇게 dockerize 환경구성후 1.0 version을 배포, 2.0 version을 만들어 배포하는중 0.3초주기로 api를 call 무한으로 날려보았다.123456789101112...&#123; version: '1.0', reqIdxVal: '1481' &#125;&#123; version: '1.0', reqIdxVal: '1482' &#125;&#123; version: '1.0', reqIdxVal: '1483' &#125;&#123; version: '1.0', reqIdxVal: '1484' &#125;&#123; version: '1.0', reqIdxVal: '1485' &#125;&#123; version: '1.0', reqIdxVal: '1486' &#125;&#123; version: '1.0', reqIdxVal: '1487' &#125;&#123; version: '1.0', reqIdxVal: '1488' &#125;&#123; version: '1.0', reqIdxVal: '1489' &#125;&#123; RequestError: Error: connect ECONNREFUSED 10.10.10.11:4000... 에러가 발생하였다.‘필자는 HEALTHCHECK 가 왜 아무소용이없지? 뭘 잘못했나 ? ‘ 라는 생각이 먼저들었으나 docker container를 확인해보니 못보던 정보가 생겨났다.12345$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES3c0824ed0191 setyourmindpark:5000/setyourmindpark_backend:180301213341 \"pm2-docker bin/www.…\" 5 minutes ago Up 5 minutes (healthy) 4000/tcp setyourmindpark_service_backend.3.rf4ejsaqz2sp5t89euz7if6ly2660ee52065c setyourmindpark:5000/setyourmindpark_backend:180301213341 \"pm2-docker bin/www.…\" 5 minutes ago Up 5 minutes (healthy) 4000/tcp setyourmindpark_service_backend.2.z6jx7lati4wqttw7rnu8plk78deb1020a9b31 setyourmindpark:5000/setyourmindpark_backend:180301213341 \"pm2-docker bin/www.…\" 6 minutes ago Up 6 minutes (healthy) 4000/tcp setyourmindpark_service_backend.1.wrhf6tbpe8x5lj7wierju493m https://docs.docker.com/engine/reference/builder/#healthcheckSTATUS COLUMN에 healty라는 정보가 새롭게 생겼으며 service update시 rolling중에는 (health: starting) 과 같이 나타난다.docker swarm 내부적인 load balancer가 해당 service의 traffic을 처리하는데에있어 healthcheck를 사용하지않는듯하다.위에서 필자가 시도한 HEALTHCHECK command 역시 개발자에게 healty 또는 unhealthy 정보를 확인할수있는 지표만을 제공하는듯하다. ( 혹시 제가 잘못알고있을수도있으니 의견부탁드립니다. ) nginx load balancingdocker HEALTHCHECK 방법이 실패하고 ‘그럼 nginx를 쓰면되지’ 라는생각과 주저없이 nginx 세팅에 들어갔다.필자는 왜 그렇게 생각한지 모르지만 nginx를 load balancer로 사용시 당연히 health check는 기본으로 동작하면서 되는줄알았다. ( 정말 왜그렇게 생각했는지 모르겠다 )12345678910111213141516171819202122$ vi /etc/nginx/default.confupstream target-server &#123; least_conn; server 10.10.10.11:4000 max_fails=3 fail_timeout=30s; # max_fails = 쵀대 실패횟수 # fail_timeout = 최대 요청실패 임계시간 # = 30초동안 3번의 요청 실패시 dead로 판단&#125;server &#123; listen 80; charset utf-8; access_log /etc/nginx/log/access.log; error_log /etc/nginx/log/error.log; location / &#123; proxy_redirect off; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_pass http://target-server; &#125;&#125; 여기서 ‘load balancer로 쓰는데 backend server가 1대야 ?’ 라고 생각하실분들이 계실것같아 설명을 하자면 docker service replicas( 5개의 container )로 구성되어있어 동작하는 backend는 5개( docker swarm load balancer )로 backend server는 1대로 구성하였다.docker swarm 포스트의 이미지를 보면 이해가 쉬울것이다.123456789...&#123; version: '1.0', reqIdxVal: '1862' &#125;&#123; version: '1.0', reqIdxVal: '1863' &#125;&#123; version: '1.0', reqIdxVal: '1864' &#125;&#123; version: '1.0', reqIdxVal: '1865' &#125;&#123; version: '1.0', reqIdxVal: '1866' &#125;&#123; version: '1.0', reqIdxVal: '1867' &#125;&#123; StatusCodeError: 502 - \"&lt;html&gt;\\r\\n&lt;head&gt;&lt;title&gt;502 Bad Gateway&lt;/title&gt;&lt;/head&gt;\\r\\n&lt;body bgcolor=\\\"white\\\"&gt;\\r\\n&lt;center&gt;&lt;h1&gt;502 Bad Gateway&lt;/h1&gt;&lt;/center&gt;... 역시에러가 발생하였다.( 당연히될줄알았는데.. ) haproxy load balancing필자의 생각으론 nginx 같은 앞단의 load balancer를 쓰면서 proxy target server의 health check를 해줄 녀석이 필요하다고 판단했다.그렇게 알게된것이 haproxy 이며 ‘이제정말 되겠지’ 라는 마음으로 공부하며 적용해보았다.12345678910111213141516171819202122232425262728293031$ vi/etc/haproxy/haproxy.cfgdefaults mode http option httplog option dontlognull option redispatch option forwardfor option http-server-close retries 3 maxconn 20480 timeout connect 5s timeout server 50s timeout client 50s timeout http-keep-alive 3000frontend http_in bind *:80 reqadd X-Forwarded-Proto:\\ http default_backend serverbackend server mode http balance roundrobin default-server inter 3s rise 5 fall 5 # default-server = health check 조건 # inter = interval 주기 ( 3초 ) # rise = 요청횟수 ( 3번 ) # fail = 실패횟수 # = 3초 주기로 5번요청 성공시 healthy, 5번 실패시 dead로 판단 server s1 10.10.10.11:4000 check =&gt; 3초 주기로 5번요청 성공시 정상작동간주, 트래픽을 연결한다.api call response 결과는 다음과 같다.123456789101112131415161718&#123; version: '1.0', reqIdxVal: '980' &#125;&#123; version: '1.0', reqIdxVal: '981' &#125;&#123; version: '1.0', reqIdxVal: '982' &#125;&#123; version: '1.0', reqIdxVal: '983' &#125;&#123; version: '1.0', reqIdxVal: '984' &#125;&#123; version: '1.0', reqIdxVal: '985' &#125;&#123; version: '2.0', reqIdxVal: '986' &#125;&#123; version: '2.0', reqIdxVal: '987' &#125;&#123; version: '1.0', reqIdxVal: '988' &#125;&#123; version: '2.0', reqIdxVal: '989' &#125;&#123; version: '2.0', reqIdxVal: '990' &#125;&#123; version: '1.0', reqIdxVal: '991' &#125;&#123; version: '2.0', reqIdxVal: '992' &#125;&#123; version: '2.0', reqIdxVal: '993' &#125;&#123; version: '2.0', reqIdxVal: '994' &#125;&#123; version: '2.0', reqIdxVal: '995' &#125;&#123; version: '2.0', reqIdxVal: '996' &#125; 무중단 배포( non-stop deploy ) 가 이루어졌다. 환경docker version = Docker version 17.12.0-ce, build c97c6d6nginx version = nginx version: nginx/1.12.2haproxy version = HA-Proxy version 1.8.4-1~bpo8+1test backend replicas container = 5개 참고사이트https://seokjun.kim/haproxy-and-nginx-load-balancing/","categories":[{"name":"deploy","slug":"deploy","permalink":"https://setyourmindpark.github.io/categories/deploy/"}],"tags":[]},{"title":"haproxy","slug":"haproxy/haproxy","date":"2018-03-01T08:30:01.000Z","updated":"2018-03-02T07:19:17.000Z","comments":true,"path":"2018/03/01/haproxy/haproxy/","link":"","permalink":"https://setyourmindpark.github.io/2018/03/01/haproxy/haproxy/","excerpt":"","text":"haproxy줄곧 nginx를 사용해오다가 간과한점이 하나있었다.nginx의 health check는 유료버전인 nginx plus에서만 가능하다는것.물론 대중적으로 nginx를 많이쓰긴하지만, 필자에게 있어 health check 기능은 너무도 중요한 요소중 하나다.그리하여 열심히 찾아보다가 .. haproxy라는 녀석을 만나게되었다.haproxy를 처음 사용해봄으로서 간단하게 진행한 테스트를 글로 남기려한다. setup haproxy필자는 host os는 centos를 docker container에서 사용할 guest os는 주로 debian os를 사용한다.설치는 haproxy 1.8 version과 debian jessie os 기준으로 설명한다.123456789$ echo deb http://httpredir.debian.org/debian jessie-backports main | \\ tee /etc/apt/sources.list.d/backports.list$ curl https://haproxy.debian.net/bernat.debian.org.gpg | \\ apt-key add - $ echo deb http://haproxy.debian.net jessie-backports-1.8 main | \\ tee /etc/apt/sources.list.d/haproxy.list$ apt-get update$ apt-get install -y haproxy=1.8.\\* -t jessie-backports$ service haproxy start 필자가 만든 docker image를 사용하려면 다음과 같이 사용할수있다.https://hub.docker.com/r/setyourmindpark/debian-haproxy/123456$ docker pull setyourmindpark/debian-haproxy:1.8$ docker run -d --name haproxy \\ -v /your/path:/etc/haproxy \\ -p default_port:80 \\ -p ssl-port:443 \\ setyourmindpark/debian-haproxy:1.8 config haproxy이제 설치된 haproxy 설정을 시작한다. 1. url endpoint에 따른 redirect12345678910111213141516171819202122232425262728$ vi /etc/haproxy.cfgdefaults mode http option httplog option dontlognull option redispatch option forwardfor option http-server-close retries 3 maxconn 20480 timeout connect 5s timeout server 50s timeout client 50s timeout http-keep-alive 3000frontend http_in bind *:80 reqadd X-Forwarded-Proto:\\ http acl naver path_end -i /naver redirect location http://www.naver.com if naver acl daum path_end -i /daum redirect location http://www.daum.net if daum acl google path_end -i /google redirect location http://www.google.com if google$ service haproxy restart /naver =&gt; redirect http://www.naver.com /daum =&gt; redirect http://www.daum.net /google =&gt; redirect http://www.google.com 2. load balancingnginx 포스트 nginx load balancing 와 같이 haproxy에서도 load balancing을 진행한다. 123456789101112131415161718192021222324252627282930313233$ vi /etc/haproxy.cfgdefaults mode http option httplog option dontlognull option redispatch option forwardfor option http-server-close retries 3 maxconn 20480 timeout connect 5s timeout server 50s timeout client 50s timeout http-keep-alive 3000frontend http_in bind *:80 reqadd X-Forwarded-Proto:\\ http default_backend serverbackend server mode http balance roundrobin default-server inter 3s rise 3 fall 5 # default-server = health check 조건 # inter = interval 주기 ( 3초 ) # rise = 요청횟수 ( 3번 ) # fail = 실패횟수 # = 3초 주기로 3번요청 성공시 healthy, 5번 실패시 dead로 판단 server s1 server1_ip:server1_port cookie check server s2 server2_ip:server2_port cookie check $ service haproxy restart =&gt; 3초 주기로 3번요청 성공시 정상작동간주, 트래픽을 연결한다. log 확인1$ haproxy -d -f /etc/haproxy/ 견해필자가 health check를 중요하게 생각하는 이유는 무중단 서비스 배포시 client는 connection에 대한 영속성을 지녀야한다는 점이다.client request에 대한 error가 아닌 실시간으로 반영될수있는 아주 중요한 요소중 하나이기 때문이다.고객사의 서비스 개발시 항상 redeploy 에 관한 사항이 중요한 이슈중 하나였다.‘언제 몇시에 서버 재기동합니다.’ 라는 말과함께 작업시 redeploy 하는 개발자 입장에서도 엄청난 부담이 될수밖에없다고 생각한다.다음 포스트에서는 필자가 여러방면으로 테스트한 무중단 배포에 관한 글을 적어보려한다.","categories":[{"name":"haproxy","slug":"haproxy","permalink":"https://setyourmindpark.github.io/categories/haproxy/"}],"tags":[]},{"title":"fluentd","slug":"fluentd/fluentd","date":"2018-02-26T06:16:55.000Z","updated":"2018-03-01T11:47:21.000Z","comments":true,"path":"2018/02/26/fluentd/fluentd/","link":"","permalink":"https://setyourmindpark.github.io/2018/02/26/fluentd/fluentd/","excerpt":"","text":"fluentd이전 포스트 build docker service with jenkins 에서 jenkins를 활용하여 docker swarm build를 하는 방법에 대해서 알아보았다.n 개의 node에서 분산되어있는 service container에 쌓이는 log를 한곳으로 취합할수있는 방법이 없을까 찾아보다가 fluentd를 만나게되었다.참고로, docker swarm service 에서 shell 상에서 실시간 log를 확인하려면 다음의 명령어를 사용하면된다.1$ docker service logs -f --raw setyourmindpark_service_backend 이번 포스트에서는 fluentd를 사용하여 log를 어떻게 취합하는지 알아보자. setup fluentdfluentd 공식 사이트에서 https://docs.fluentd.org/v0.12/categories/installation os환경에따라 설치를 진행할수있다.필자는 역시 docker 기반으로 사용할것이기에 https://docs.fluentd.org/v0.12/articles/install-by-docker fluentd docker official image로 진행하였다. 설정은 다음과 같다.12345678910111213141516171819202122$ vi ~/fluentd/volume/config/fluentd.conf&lt;source&gt; type forward port 24224 # listening port&lt;/source&gt;&lt;match app&gt; # tag name type copy &lt;store&gt; type file append true path /fluentd/log/app # to store log path time_slice_format %Y%m%d # log file format time_slice_wait 10m time_format %Y-%m-%dT%H:%M:%S %z # log time format compress gzip # compress format # file will create at 00:00 everyday #utc # if do not use utc log time format, defailt local timezone &lt;/store&gt;&lt;/match&gt; 12345678$ docker run -d --name fluentd \\ -p 24224:24224 \\ -v ~/fluentd/volume/config:/fluentd/etc \\ -v ~/fluentd/volume/plugins:/fluentd/plugins \\ -v ~/fluentd/volume/log:/fluentd/log \\ -e FLUENTD_CONF=fluentd.conf \\ -e TZ=Asia/Seoul \\ fluent/fluentd:v0.12-debian 이제 fluentd에 log를 날려보자. dependencies ( nodejs )nodejs module중 고맙게도 fluentd에 관련된 dependency를 제공한다. fluent-logger winston 필자는 winston module과 fluent-logger modeul을 같이 사용하였다. 설정은 다음과같다.1234567891011121314151617const winston = require('winston');const fluentTransport = require('fluent-logger').support.winstonTransport();const logger = new (winston.Logger)(&#123; level : 'debug', // error: 0, warn: 1, info: 2, verbose: 3, debug: 4, silly: 5 transports: [new fluentTransport('app', &#123; // tag name host: '10.10.10.11', // host port: 24224, // port timeout: 3.0 &#125;), new (winston.transports.Console)(&#123; colorize : true &#125;)]&#125;);logger.debug('debug log');logger.info('info log');logger.error('error log'); fluentd에 log 가 제대로 들어왔는지 확인해보자.1234567$ cd ~/fluentd/volume/log$ lsapp.20180226.b566194875a717ee6$ cat app.20180226.b566194875a717ee62018-02-26T18:17:09 +0900 app &#123;\"level\":\"debug\",\"message\":\"debug log\",\"meta\":&#123;&#125;&#125;2018-02-26T18:17:09 +0900 app &#123;\"level\":\"info\",\"message\":\"info log\",\"meta\":&#123;&#125;&#125;2018-02-26T18:17:09 +0900 app &#123;\"level\":\"error\",\"message\":\"error log\",\"meta\":&#123;&#125;&#125; 참고fluentd를 사용하여 file system에 log를 취합하는 방법을 알아보았다.fluentd는 이와 관련하여 file system뿐만아니라 db에도 trigging 할수도있다.https://docs.fluentd.org/v0.12/articles/out_copy그밖에도 지원되는 plugin이 굉장히 많으므로 구성하고자 하는 환경에따라 plugin을 사용하면될듯하다. ( 공부해야겠다….. )또한 elastic search 와 관련하여 ELK( elastic search + logstash + kibana )를 대신하여 EFK ( elastic search + fluentd + kibana ) stack을 요즘은 더 선호하는듯하다.","categories":[{"name":"fluentd","slug":"fluentd","permalink":"https://setyourmindpark.github.io/categories/fluentd/"}],"tags":[]},{"title":"build docker service with jenkins","slug":"jenkins/jenkins-5","date":"2018-02-23T06:44:27.000Z","updated":"2018-02-24T08:04:42.000Z","comments":true,"path":"2018/02/23/jenkins/jenkins-5/","link":"","permalink":"https://setyourmindpark.github.io/2018/02/23/jenkins/jenkins-5/","excerpt":"","text":"build docker service with jenkins이전 포스트 에서 docker registry, service 그리고 stack 에 대해서 알아보았다.docker private registrydocker swram servicedocker stack이제 jenkins를 사용하여 효과적으로 빌드하는지 알아보려한다. 물론 주관적인 필자의 생각이다. Managed Scriptsdocker swarm 의 service들을 효과적으로 배포하기위해서 shell script를 적극적으로 사용하여 로직구성을 해야되겠다는 판단이 들었다.jenkins의 plugin 중 Managed Scripts 에 관한 정보는 이전포스트 jenkins로 배포하기 - nodejs-2 를 참고하자.일단 전체적인 프로세스는 다음과 같다. 배포하고자하는 프로젝트에 Dockerfile 을 생성한다 ( dockerize ) 형상관리( git ) 에 변경사항 반영한다. empty 프로젝트를 기반으로 service를 생성한다 ( service update를 위해 ) jenkins에서 build 시 Dockerfile을 기반으로 image build를 시작한다. build 된 image를 docker private registry에 push 한다. docker private registry에 push된 image를 기반으로 service update한다. empty image service 를 생성후 jenkins 에서 service 를 update 를 하는 이유는, service update시 stack에 구성된 환경설정은 그대로 유지되고 image만 교체되기때문에 stack에 정의된 같은 환경으로 구성된 service들과의 동일 설정정보를 항상 유지할수있는 이점이 있어서다. builddocker stack은 다음과 같이 작성하였다.123456789101112131415161718192021222324version: \"3.5\"networks: net: driver: overlay attachable: trueservices: # ... other services here backend: image: setyourmindpark/debian-node:8 ports: - 4000:4000 environment: - TZ=Asia/Seoul networks: - net tty: true deploy: replicas: 3 placement: constraints: - node.labels.type == backend # ... other services here 1$ docker stack deploy -c service-stack.yml setyourmindpark_service 위에서 언급한 프로세스의 shell script를 다음과 같이 작성하였다.1234567891011121314151617181920212223# env ------------------------------------------------------------------------------------------------------------------------DOCKER_REGISTRY_PROTOCOL=httpsDOCKER_REGISTRY_DOMAIN=setyourmindparkDOCKER_REGISTRY_PORT=5000DOCKER_REGISTRY_USER=setyourmindparkDOCKER_REGISTRY_PASSWD=0000APP_NAME=setyourmindpark_backendSERVICE_NAME=setyourmindpark_service_backendAPP_VERSION=`date +\"%y%m%d%H%M%S\"`# build ------------------------------------------------------------------------------------------------------------------------docker build --tag $APP_NAME:$APP_VERSION .docker login -u $DOCKER_REGISTRY_USER -p $DOCKER_REGISTRY_PASSWD $DOCKER_REGISTRY_DOMAIN:$DOCKER_REGISTRY_PORTdocker tag $APP_NAME:$APP_VERSION $DOCKER_REGISTRY_DOMAIN:$DOCKER_REGISTRY_PORT/$APP_NAME:$APP_VERSIONdocker push $DOCKER_REGISTRY_DOMAIN:$DOCKER_REGISTRY_PORT/$APP_NAME:$APP_VERSIONdocker rmi -f $APP_NAME:$APP_VERSIONdocker rmi -f $DOCKER_REGISTRY_DOMAIN:$DOCKER_REGISTRY_PORT/$APP_NAME:$APP_VERSIONdocker service update \\ --image $DOCKER_REGISTRY_DOMAIN:$DOCKER_REGISTRY_PORT/$APP_NAME:$APP_VERSION \\ --with-registry-auth \\ $SERVICE_NAME 최종 build 결과는 다음과 같다.123456789101112131415overall progress: 0 out of 3 tasks1/3: 2/3: 3/3: overall progress: 0 out of 3 tasksoverall progress: 1 out of 3 tasksoverall progress: 2 out of 3 tasksoverall progress: 3 out of 3 tasksverify: Waiting 5 seconds to verify that tasks are stable...verify: Waiting 4 seconds to verify that tasks are stable...verify: Waiting 3 seconds to verify that tasks are stable...verify: Waiting 2 seconds to verify that tasks are stable...verify: Waiting 1 seconds to verify that tasks are stable...verify: Service convergedFinished: SUCCESS 정상적으로 service 배포가되었는지 확인해보자.123456789$ docker service ps setyourmindpark_service_backendID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSl27yump8rk60 setyourmindpark_service_backend.1 setyourmindpark:5000/setyourmindpark_backend:180213123413 backend1 Running Running less than a second ago ksblqm51aqne \\_ setyourmindpark_service_backend.1 setyourmindpark/debian-node:8 backend1 Shutdown Shutdown less than a second ago p7a28d4rq4t3 setyourmindpark_service_backend.2 setyourmindpark:5000/setyourmindpark_backend:180213123413 backend1 Running Running less than a second ago 2qv9ll1qmz2q \\_ setyourmindpark_service_backend.2 setyourmindpark/debian-node:8 backend1 Shutdown Shutdown less than a second ago tlbf6gdgijvv setyourmindpark_service_backend.3 setyourmindpark:5000/setyourmindpark_backend:180213123413 backend1 Running Running less than a second ago iy5l21fe264l \\_ setyourmindpark_service_backend.3 setyourmindpark/debian-node:8 backend1 Shutdown Shutdown less than a second ago 정상적으로 update 되었다. 참고dockerize란 배포하고자 하는 프로젝트를 image화 하는 작업이라 지칭한다.필자는 dockerize 구성을위한 환경 nodejs 환경기반으로 배포진행하였으며 프로젝트 참고는 다음과 같다.https://github.com/setyourmindpark/nodejs-skeletone-v3","categories":[{"name":"jenkins","slug":"jenkins","permalink":"https://setyourmindpark.github.io/categories/jenkins/"}],"tags":[]},{"title":"docker stack","slug":"docker/docker-7","date":"2018-02-21T04:49:25.000Z","updated":"2018-02-27T06:15:11.000Z","comments":true,"path":"2018/02/21/docker/docker-7/","link":"","permalink":"https://setyourmindpark.github.io/2018/02/21/docker/docker-7/","excerpt":"","text":"docker stackdocker swarm 을 사용하지않고 단일 물리 머신에서는 docker run … 과같은 명령어들을 docker compose 에 설정하여 사용하는 방법이있었다.docker swarm 에서도 역시 이런 유사한 방법이 있는데 docker service create … 들을 정의하여 사용하는방법이 있으며 docker stack 이라고 지칭한다.docker stack 을 어떻게 사용하는지 알아보자 . configure yml file이전포스트 docker swarm service 에서 제시된 service 를 생성하는 명령어는 다음과 같다.다음과 같은 service를 생성하는 명령어가 있다고 가정하자.1234567$ docker service create --name nginx \\--network setyourmindpark_net \\--constraint \"node.labels.type == nginx\" \\-p 80:80 \\--mount type=bind,src=/home/docker/nginx/volume,dst=/etc/nginx \\--replicas 3 \\setyourmindpark/debian-nginx 다음과 같이 docker-stack.yml 을 정의한다. ( 파일명은 제약사항이없다. )1234567891011121314151617181920212223version: \"3.5\"networks: net: driver: overlay attachable: trueservices: nginx: image: setyourmindpark/debian-nginx ports: - 80:80 environment: - TZ=Asia/Seoul volumes: - /home/docker/nginx/volume:/etc/nginx networks: - net deploy: replicas: 3 placement: constraints: - node.labels.type == nginx 이제 stack 명령어를 사용하여 services 들을 생성한다.1234567# docker stack deploy -c 파일명 stack명$ docker stack deploy -c docker-stack.yml setyourmindparkCreating network setyourmindpark_netCreating service setyourmindpark_nginx$ docker service lsID NAME MODE REPLICAS IMAGE PORTSvbdj9p1h58fg setyourmindpark_nginx replicated 3/3 setyourmindpark/debian-nginx:latest *:80-&gt;80/tcp 정상적으로 생성이 완료되었다.추가적으로 stack으로 생성된 services들은 stack 명령어로도 확인이 가능하다. 123456789$ docker stack ls NAME SERVICESsetyourmindpark 1$ docker stack ps setyourmindparkID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSvl95vusgaqcp setyourmindpark_nginx.1 setyourmindpark/debian-nginx:latest nginx Running Running 8 minutes ago da0107q7fwiv setyourmindpark_nginx.2 setyourmindpark/debian-nginx:latest nginx Running Running 8 minutes ago v8om2wwr4ba4 setyourmindpark_nginx.3 setyourmindpark/debian-nginx:latest nginx Running Running 8 minutes ago 해당 yml 파일에 기술한 nginx service 외에도 필요한 service를 같이 기술하면, stack deploy시 정의된 모든 services들이 같이 올라올것이다.그밖에도 cpu나, memory limit 과 같은 추가적인 옵션들을 제공하며,( 당연히 docker service create … 시에도 똑같이 사용할수있다. ) 제공하는 옵션은 docker Compose file version 3 reference 에서 확인가능하다. 견해docker 를 잠시동안 놓게되면 정말 엄청나게 빨리 version up 이 되는듯하다.새로생긴 옵션도 많아지고, 점점더 진입장벽이 생기는듯하다.새로 눈여겨볼 부분은 service create 시 -v 가 먹히지않는다는점.. docker document에서는 권장사항이라고 언급하고있으나 필자의 version에서는 해당명령어를 사용할수없었다.물론 –mount 옵션이 좀더 많은 기능을 제공하는듯하다. nas와 같은 volume container를 설정할수있는 옵션이 추가된듯하다 자세히 살펴보지않았지만…꾸준히 docker document를 살펴보는 습관을 길러야겠다.","categories":[{"name":"docker","slug":"docker","permalink":"https://setyourmindpark.github.io/categories/docker/"}],"tags":[]},{"title":"docker swarm service","slug":"docker/docker-6","date":"2018-02-19T04:56:07.000Z","updated":"2018-02-23T10:31:59.000Z","comments":true,"path":"2018/02/19/docker/docker-6/","link":"","permalink":"https://setyourmindpark.github.io/2018/02/19/docker/docker-6/","excerpt":"","text":"docker swarm servicedocker swarm에서 n개의 node 관련 컨테이너 환경설정등을 swarm에서 service라 지칭한다.단일 docker 환경에서는 run 명령어로 컨테이너 환경설정을 구성해보았다면 service 명령어도 무척 쉽게느껴질것이다.run 명령어와 크게 다르지않으며 -p host_port:container_port -e key:value 등 기존의 run 명령어에서 사용한 옵션들을 그대로 사용가능하다.( -v 는 –mount 로 service에서 권장사항으로 변경이되었다. )어떻게 n개의 node 에서 컨테이너를 관리하는지 알아보자. create service가장 기본적인 service 생성은 다음과 같다.1$ docker service create -d --name nginx setyourmindpark/debian-nginx 정상적으로 생성되었는지 확인하자.123$ docker service ls ID NAME MODE REPLICAS IMAGE PORTSwwl53p7zh25r nginx replicated 1/1 setyourmindpark/debian-nginx:latest *:80-&gt;80/tcp replicas 옵션을 설정하지않는다면 default 1개로 생성이된다.어느 node에서 생성되었는지, process 상세정보를 얻고자한다면 다음명령어를 통해 확인할수있다.123$ docker service ps nginxID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSyivhgl2pk2jd nginx.1 setyourmindpark/debian-nginx:latest master Running Running 2 minutes ago master node 에서 1개의 컨테이너가 실행중임을 알수있다. ingress networkingress network를 binding 을 해보자. n 개의 service container들은 비지니스 로직에따라 3자의 container를 연결하거나, 호출할필요가 분명존재할것이다.다음과같이 ingress network를 생성한후 2개의 service를 생성하여 진행해보자.먼저 ingress network 를 생성한다.123456$ docker network create --driver overlay setyourmindpark_net$ docker network ls NETWORK ID NAME DRIVER SCOPE...kowrro91wn5v setyourmindpark_net overlay swarm... service 를 생성한다.( dns ping test를 진행하기위해 mariadb와 nginx 이미지로 진행한다. 추가적인 volume mount나 port는 생략한다. ) 12345678910$ docker service create -d --name nginx --network setyourmindpark_net setyourmindpark/debian-nginx$ docker service create -d --name mariadb --network setyourmindpark_net -e MYSQL_ROOT_PASSWORD=0000 mariadb$ docker service ps nginxID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS6rc28lwmg6v7 nginx.1 setyourmindpark/debian-nginx:latest nginx Running Running 34 seconds ago $ docker serbvice ps mariadbID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSnd1pucccgrzj mariadb.1 mariadb:latest master Running Running about a minute ago nginx container는 nginx node에, mariadb container는 master 에 각각 1개씩 생성된것을 확인할수있다.이제 각자의 container에 접속하여 등록된 dns로 ping을 날려보자.생성된 각각의 컨테이너로 접속한다.1234567891011121314151617181920212223# nginx node$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb8c1858d2f50 setyourmindpark/debian-nginx:latest \"/entrypoint.sh\" 5 minutes ago Up 5 minutes 80/tcp, 443/tcp nginx.1.6rc28lwmg6v7vars1w7xs93z3$ docker exec -it b8c1858d2f50 /bin/bash$ ping mariadbPING mariadb (10.0.0.13): 56 data bytes64 bytes from 10.0.0.13: icmp_seq=0 ttl=64 time=0.095 ms64 bytes from 10.0.0.13: icmp_seq=1 ttl=64 time=0.087 ms64 bytes from 10.0.0.13: icmp_seq=2 ttl=64 time=0.284 ms# master node$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe04095504a51 mariadb:latest \"docker-entrypoint.s…\" 7 minutes ago Up 7 minutes 3306/tcp mariadb.1.nd1pucccgrzje2xdkm833lgmg$ docker exec -it e04095504a51 /bin/bash$ ping nginxPING nginx (10.0.0.15): 56 data bytes64 bytes from 10.0.0.15: icmp_seq=0 ttl=64 time=0.061 ms64 bytes from 10.0.0.15: icmp_seq=1 ttl=64 time=0.135 ms64 bytes from 10.0.0.15: icmp_seq=2 ttl=64 time=0.072 ms 서로다른 물리 node 지만 ingress network로 바인딩되어 컨테이너 생성시 같은 network로 구성된 서비스들이 자동으로 dns로 등록된것을 확인할수있다. replicasswarm 을 사용하는 가장중요한 이유중 하나가 replicas 를 사용하기위해서 이기도 하다.replicas option 을 사용하여 container scale up 을 해보자 .123456789101112131415$ docker service scale nginx=2nginx scaled to 2overall progress: 2 out of 2 tasks 1/2: running [==================================================&gt;] 2/2: running [==================================================&gt;] verify: Service converged $ docker service ls ID NAME MODE REPLICAS IMAGE PORTS86h0rksc6100 nginx replicated 2/2 setyourmindpark/debian-nginx:latest $ docker service ps nginx ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS6rc28lwmg6v7 nginx.1 setyourmindpark/debian-nginx:latest nginx Running Running 11 minutes ago st00fxl6khpm nginx.2 setyourmindpark/debian-nginx:latest master Running Running 9 seconds ago master node 와 nginx node에 1개씩 container가 생성된것을 확인할수있다. create replicated service in specific nodedocker swarm service의 replicas는 개발자가 직접 설정하지않으면 node에 분산되어 container 가 생성된다.특정 node에서만 container 들을 replicas 하도록 설정해보자.먼저 node의 type을 설정해주도록한다. ( 당연히 leader node에서 진행한다 )1234567$ docker node update --label-add type=nginx nginx$ docker node inspect nginx...\"Labels\": &#123; \"type\": \"nginx\" &#125;,.. 다음과 같이 설정이 완료된후, scaleup 한다 .123456789101112$ docker service create -d --name nginx --constraint \"node.labels.type == nginx\" --replicas 3 setyourmindpark/debian-nginx$ docker service ls ID NAME MODE REPLICAS IMAGE PORTSxmtddrtwibb2 mariadb replicated 1/1 mariadb:latest dv5xvtwpf7np nginx replicated 3/3 setyourmindpark/debian-nginx:latest $ docker service ps nginxID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSmvt47mwyyhwo nginx.1 setyourmindpark/debian-nginx:latest nginx Running Running 29 seconds ago fqppx2a4m01f nginx.2 setyourmindpark/debian-nginx:latest nginx Running Running 29 seconds ago dxygv37emol6 nginx.3 setyourmindpark/debian-nginx:latest nginx Running Running 29 seconds ago 특정 node에 replocas된 service 가 생성된것을 확인할수있다. create service with docker private registrydocker swarm 을 service 하는 기업입장에서 private registry에 올려놓은 image들로 서비스를 구성할것이다. docker 는 private registry에서 pull 하는 옵션으로 --with-registry-auth 를 제공한다.docker private registry 구성은 docker private registry 를 참고한다.1$ docker service create -d --name my-private-image --replicas 5 --with-registry-auth setyourmindpark:5000/my-private-image:1.0 service updateservice를 update시 무정지 배포도 가능하다.사용중인 image version1.0 에서 2.0 으로 update 시 다음과 같이 사용할수있다.1234567891011121314$ docker service update --image setyourmindpark:5000/my-private-image:2.0 --with-registry-auth my-private-image$ docker service ps my-private-imageID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSo8g58unqwsrx my-private-image.1 setyourmindpark:5000/my-private-image:2.0 master Running Running 48 seconds ago 0kh6v9dk10go \\_ my-private-image.1 setyourmindpark:5000/my-private-image:1.0 nginx Shutdown Shutdown 42 seconds ago ozmcs99v3pl4 my-private-image.2 setyourmindpark:5000/my-private-image:2.0 master Running Running about a minute ago t6l1hjh6j72y \\_ my-private-image.2 setyourmindpark:5000/my-private-image:1.0 master Shutdown Shutdown about a minute ago bvi8da63ruxl my-private-image.3 setyourmindpark:5000/my-private-image:2.0 nginx Running Running 5 seconds ago rmeyhhexod6k \\_ my-private-image.3 setyourmindpark:5000/my-private-image:1.0 nginx Shutdown Shutdown 6 seconds ago d1db4z35ejk5 my-private-image.4 setyourmindpark:5000/my-private-image:2.0 nginx Running Running 29 seconds ago 6abz8v3ghj9e \\_ my-private-image.4 setyourmindpark:5000/my-private-image:1.0 nginx Shutdown Shutdown 30 seconds ago n8lxnj7raj45 my-private-image.5 setyourmindpark:5000/my-private-image:2.0 master Running Running 24 seconds ago kzea4jr0w9kf \\_ my-private-image.5 setyourmindpark:5000/my-private-image:1.0 master Shutdown Shutdown 25 seconds ago 해당 명령어를 실행해보면 알겠지만, 현재 실행중인 container들을 하나씩 차례로 shutdown하고 1.0 version에서 2.0 version으로 update 하는것을 확인할수있다. ( rolling 한다 ) service rollbackservice를 update하였으나, 어떠한 문제로인해 service가 update되지않는경우, update 하기전 상태로 rollback 할수있는 기능을 제공한다. 명령어는 무척이나 간단하다 .12345678910111213141516171819$ docker service rollback my-private-image$ docker service ps my-private-imageID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSsiaw130oi5cl my-private-image.1 setyourmindpark:5000/my-private-image:1.0 master Running Running 47 seconds ago o8g58unqwsrx \\_ my-private-image.1 setyourmindpark:5000/my-private-image:2.0 master Shutdown Shutdown 47 seconds ago 0kh6v9dk10go \\_ my-private-image.1 setyourmindpark:5000/my-private-image:1.0 nginx Shutdown Shutdown 4 minutes ago tne09aocufyk my-private-image.2 setyourmindpark:5000/my-private-image:1.0 master Running Running 59 seconds ago ozmcs99v3pl4 \\_ my-private-image.2 setyourmindpark:5000/my-private-image:2.0 master Shutdown Shutdown 59 seconds ago t6l1hjh6j72y \\_ my-private-image.2 setyourmindpark:5000/my-private-image:1.0 master Shutdown Shutdown 5 minutes ago tm2nsmei9r50 my-private-image.3 setyourmindpark:5000/my-private-image:1.0 nginx Running Running about a minute ago bvi8da63ruxl \\_ my-private-image.3 setyourmindpark:5000/my-private-image:2.0 nginx Shutdown Shutdown about a minute ago rmeyhhexod6k \\_ my-private-image.3 setyourmindpark:5000/my-private-image:1.0 nginx Shutdown Shutdown 4 minutes ago y9oujmebhdyc my-private-image.4 setyourmindpark:5000/my-private-image:1.0 nginx Running Running about a minute ago d1db4z35ejk5 \\_ my-private-image.4 setyourmindpark:5000/my-private-image:2.0 nginx Shutdown Shutdown about a minute ago 6abz8v3ghj9e \\_ my-private-image.4 setyourmindpark:5000/my-private-image:1.0 nginx Shutdown Shutdown 4 minutes ago z7jfvtn7v4if my-private-image.5 setyourmindpark:5000/my-private-image:1.0 master Running Running about a minute ago n8lxnj7raj45 \\_ my-private-image.5 setyourmindpark:5000/my-private-image:2.0 master Shutdown Shutdown about a minute ago kzea4jr0w9kf \\_ my-private-image.5 setyourmindpark:5000/my-private-image:1.0 master Shutdown Shutdown 4 minutes ago 2.0 version에서 rollback 실행후 1.0으로 되돌아간것을 확인할수있다.","categories":[{"name":"docker","slug":"docker","permalink":"https://setyourmindpark.github.io/categories/docker/"}],"tags":[]},{"title":"nginx tcp load balancing","slug":"nginx/nginx-2","date":"2018-02-12T05:13:39.000Z","updated":"2018-02-23T10:36:41.000Z","comments":true,"path":"2018/02/12/nginx/nginx-2/","link":"","permalink":"https://setyourmindpark.github.io/2018/02/12/nginx/nginx-2/","excerpt":"","text":"tcp load balancingdb replication으로 구성한 n개의 db를 failover 를 사용하는법은 찾아보다가, 우연히 nginx에서 tcp load balancing을 사용할수있다는것을 알게되었다.이전포스트 nginx load balancing 에서 http protocol의 load balancing을 살펴보았다면 이번 포스트에서는 tcp protocol 을 사용한 load balancing 을 소개하려한다. nginx confignginx http protocol load balancing을 구성해보았다면 무척이나 쉽게 느껴질것이다./etc/nginx/nginc.conf 를 다음과 같이 수정한다.nginx를 설치하면 가장기본적으로 다음과 같이 설정되어있을것이다.http protocol은 사용하지않을 것이므로 다음과같이 주석이나 삭제하고 stream 설정을 추가한다.123456789101112131415161718192021$ vi/etc/nginx/nginx.conf...# 삭제 또는 주석# http &#123;# ...# &#125;# 추가stream &#123; upstream db &#123; server db1_host:db_port; server db2_host:db_port; &#125; server &#123; listen 3306; proxy_pass db; proxy_connect_timeout 1s; # detect failure quickly &#125;&#125; 다음과 같이 설정을 마친후 nginx를 reload한다.1$ nginx -s reload 해당 설정방법은 nginx documenthttps://www.nginx.com/blog/mysql-high-availability-with-nginx-plus-and-galera-cluster/를 보고 참고하였으며, 생각보다 tcp load balancing을 사용하여 db 이중화를 구성하는 개발자분들이 많은것 같다.nginx 를 통해 mysql workbench나 기타 dbms tool로 접속하면 정상적으로 접속되는것을 확인할수있다. 참고docker를 통한 nginx구성은https://hub.docker.com/r/setyourmindpark/debian-nginx/를 참고하자.","categories":[{"name":"nginx","slug":"nginx","permalink":"https://setyourmindpark.github.io/categories/nginx/"}],"tags":[]},{"title":"mysql or mariadb replication","slug":"database/database-1","date":"2018-02-09T06:24:57.000Z","updated":"2018-02-23T10:32:46.000Z","comments":true,"path":"2018/02/09/database/database-1/","link":"","permalink":"https://setyourmindpark.github.io/2018/02/09/database/database-1/","excerpt":"","text":"replicationmysql 또는 mariadb의 db replication 에 대해서 살펴보려한다.서비스가 커지면서 db traffic에 대한 이슈, 또는 데이터 장애에 대한 대비책으로 replication 가 갖는 이점들이 분명 존재한다.하지만 db replication 를 한다고 해서 무조건 좋은것은 아니며 그만큼 관리가 힘든 부분도 존재하는것 같다.이전에 설치한 setup mariadb 롤 통해 replication 를 진행해보려한다. configreplication 를 하기위해 2대의 db로 테스트를 진행하며, 필자는 양방향 replication으로 서로가 master 와 slave가 되는 방법으로 진행하려한다.먼저 db의 고유 server-id를 부여해야하며 db1에서 다음과 같이 설정하였다.1234567891011121314# db1 shell$ vi /etc/my.cnf...[mysqld]## replication configlog-bin=binlogrelay-log=relayloglog-slave_updates # master에서 받아온 변경사항을 자신의 log에 기록server-id=1 # 각 db는 반드시 고유한 server-id를 가져야한다. ## replication config...$ service maridb restart db2에서도 다음과 같이 진행하며 server-id 값만 다를뿐 다른 설정은 동일하다.1234567891011121314# db2 shell$ vi /etc/my.cnf...[mysqld]## replication configlog-bin=binlogrelay-log=relayloglog-slave_updates # master에서 받아온 변경사항을 자신의 log에 기록server-id=2 # 각 db는 반드시 고유한 server-id를 가져야한다. ## replication config...$ service maridb restart db1의 log position을 확인한다.1234567# db1 shell$ mysql &gt; SHOW MASTER STATUS;+---------------+----------+--------------+------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB |+---------------+----------+--------------+------------------+| binlog.000001 | 325 | | |+---------------+----------+--------------+------------------+ db2에서 db1의 master 정보를 설정하여 db1을 db2의 master로 설정한다.slave가 된 db2를 시작한다.12345678# db2 shellmysql &gt; CHANGE MASTER TOMASTER_LOG_FILE='binlog.000001',MASTER_LOG_POS=325,MASTER_HOST='db1 host',MASTER_USER='db1 user',MASTER_PASSWORD='db1 user password';START SLAVE; slave가 된 db2의 정보확인12345678# db2 shellmysql &gt; SHOW SLAVE STATUS\\G;*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event... Slave_IO_Running: Yes Slave_SQL_Running: Yes... db2 가 db1을 master로 동작하는 slave로 된것을 확인할수있다.db1에서도 db2의 master 정보를 설정하여 db2을 db1의 master로 설정한다.db2의 log position을 확인한다.1234567# db2 shell$ mysql &gt; SHOW MASTER STATUS;+---------------+----------+--------------+------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB |+---------------+----------+--------------+------------------+| binlog.000001 | 325 | | |+---------------+----------+--------------+------------------+ db1에서 db2의 master 정보를 설정하여 db2을 db1의 master로 설정한다.slave가 된 db1를 시작한다.12345678# db1 shellmysql &gt; CHANGE MASTER TOMASTER_LOG_FILE='binlog.000001',MASTER_LOG_POS=325,MASTER_HOST='db2 host',MASTER_USER='db2 user',MASTER_PASSWORD='db2 user password';START SLAVE; slave가 된 db1의 정보확인12345678# db1 shellmysql &gt; SHOW SLAVE STATUS\\G;*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event... Slave_IO_Running: Yes Slave_SQL_Running: Yes... db1과 db2의 양방향 replication 설정이 완료되었다.참고로 필자는 replication을 하기전 사전에 양쪽db 모두 (db1, db2) 의 계정 정보를 사전에 생성, 권한부여 까지 마친 상태로 진행하였으며 position 초기화 후 진행하였다.1$ mysql &gt; RESET MASTER; sync test위의 설정대로라면 db1이나 db2의 어떤 쪽에서도 DDL 이나 DML이 발생할경우 양쪽db1 모두 sync가 되어 동기화가 이루어져야한다.123456789101112131415161718192021222324# db1 shell$ mysql &gt; create database test_db;# db2 shell$ mysql &gt; show databases;+--------------------+| Database |+--------------------+| ... || test_db || ... |+--------------------+$ mysql &gt; use test_db;$ mysql &gt; create table test_tbl( id int primary key );$ mysql &gt; insert into test_tbl values(1);# db1 shell$ mysql &gt; use test_db;$ select * from test_tbl;+----+| id |+----+| 1 |+----+ 양쪽모두 sync 가되어 정상적으로 동기화가 되는것을 확인할수있다. position skip장애가 일어난 상황에서 sync 가 깨진경우 master log position을 skip한후 slave 재시작하게되면 그동안 master에 쌓인 log가 slave에 반영된다.123$ mysql &gt; STOP SLAVE;$ mysql &gt; SET GLOBAL SQL_SLAVE_SKIP_COUNTER=1;$ mysql &gt; START SLAVE;","categories":[{"name":"database","slug":"database","permalink":"https://setyourmindpark.github.io/categories/database/"}],"tags":[]},{"title":"docker swarm","slug":"docker/docker-5","date":"2018-02-07T08:10:31.000Z","updated":"2018-02-23T10:32:11.000Z","comments":true,"path":"2018/02/07/docker/docker-5/","link":"","permalink":"https://setyourmindpark.github.io/2018/02/07/docker/docker-5/","excerpt":"","text":"docker swarmdocker에 대해 글을 쓰는게 많아지는 요즘 이번에는 docker swarm에 대해서 글을 써보려 한다.N 개의 물리서버를 clustering 할수있는 환경을 재공하며 나아가 rollback, scaling, 무중단 배포까지 가능 하다.결론부터 말하면 환경구성하기는 무척이나 쉽다. 그리고 신기하다 . 환경구성docker swarm을 구성하기전에 당연히 시스템에 docker가 설치되어있어야한다.docker install에서 설치를 진행해도되고, 필자는 github gist를 이용해 아주 쉽게(?) 설치를 진행하였다.1$ curl -s https://gist.githubusercontent.com/setyourmindpark/fdbac4f4eab71b6a03b13660cd064710/raw/9d0f68f6150a57205b7a6df7ddbdab68131335a3/install-docker-ce-centos | bash 설치 완료를 하면 다음과 같은 버전정보를 확인할수있다 . ( 현 201802 )12$ docker --versionDocker version 17.12.0-ce, build c97c6d6 docker swarm 환경구성을 위해서는 clustering이나 N개의 서버 환경이 필요하다.따라서 필자는 이전포스트 vagrant 에서 진행한 가상환경 centos 2개의 환경에서 진행할것이기에 2개의 환경모두 같은 docker version으로 설치를 진행하였다. docker swarm먼저 docker swarm 을 구성하기전에 leader node 로 사용할 서버에서 다음의 명령어를 입력한다.leader node는 N개의 node를 관리하는 관리자 node 라고 생각하면된다.123456789[server1 (leader) ]$ docker swarm init --advertise-addr 10.10.10.11Swarm initialized: current node (sfx2ywb1o3cgaehkk1recc78n) is now a manager.To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-2wo52bbwsw2v350eyjwx2xwof5rk8m3t9ki0ax02jwkgyo14kt-cefzjz5ky4mf3qvyxuxdwuahr 10.10.10.11:2377To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. –advertise-addr 옵션뒤 leader node( 현재서버 ip )를 적어주게되면, leader node를 join할 명령어 token을 친절하게 알려준다.이제 leader node에서 생성한 token 정보로, 다른 node에서 join을 해보자.123[server2 (worker) ]$ docker swarm join --token SWMTKN-1-2wo52bbwsw2v350eyjwx2xwof5rk8m3t9ki0ax02jwkgyo14kt-cefzjz5ky4mf3qvyxuxdwuahr 10.10.10.11:2377This node joined a swarm as a worker. 정상적으로 swarm join이 되었는지 leader node에서 확인해보자.1234$ docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUSsfx2ywb1o3cgaehkk1recc78n * server1 Ready Active Leadern2awbegivqvw6dhc6npazwc5s server2 Ready Active leader(server1)와 worker(server2) 모두 정상적으로 clustering 이되었다. ingressdocker swarm 을 활용하기위해서 ingress의 개념부터 잘 알아야할것 같다.docker swarm으로 cluster를 구성하게되면 ingress network가 default로 생성된것을 확인할수있다.12345678$ docker network ls NETWORK ID NAME DRIVER SCOPE08e25cf17147 bridge bridge localc589e1859f70 docker_gwbridge bridge localbd3a091f88d0 dockerregistry_default bridge localeac1648255dc host host local05jt5qn72nvy ingress overlay swarmed2fcc6343f2 none null local scope의 swarm으로 된 ingress 가 보인다.ingress란 아주 쉽게 말해 nginx를 load balancer로 사용한 효과와 같다고 생각하면 된다.다시말해서, 1번 node에 어떠한 service 컨테이너가 존재하고, 2번 node에는 존재하지않는다고 가정한다면, client 입장에서는 1번 node 또는 2번 node 둘중 아무 node를 호출(ex 10.10.10.11:service port 또는 10.10.10.12:service port ) 하여도 같은 결과를 return 한다.ingress가 존재하는 container를 찾아 자동으로 load balancing 하여 마치 한몸처럼 동작하게 되는것이다 .설명하는것보다 다음그림을 보는것이 이해하기 편할것이다.https://docs.docker.com/engine/swarm/ingress/이제 ingress를 테스트해보자 .container의 unique id를 확인할수있는 image를 받는다.1$ docker pull jwilder/whoami image pull은 leader node에서 수행하여야 한다. 해당 image를 N개의 node에서 관리할것이므로.document는 https://hub.docker.com/r/jwilder/whoami/ 에서 확인가능하다.이제 service를 생성하자.12[server1 (leader) ]$ docker service create --name whoami -p 8000:8000 jwilder/whoami service를 확인한다.1234567$ docker service lsID NAME MODE REPLICAS IMAGE PORTSqnueloqeyi63 whoami replicated 1/1 jwilder/whoami:latest *:8000-&gt;8000/tcp$ docker service ps whoamiID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSmmcok9zpdfso whoami.1 jwilder/whoami:latest server1 Running Running about a minute ago 1개의 container 가 server1(leader node) 에 생성된것을 확인할수있다.이제 document에 제시된바와같이 curl을 날려보자 .123server1( leader node )$ curl server1:8000I'm f8754dbd7fea ingress가 제대로 동작된다면 server2(worker node) 에서도 동작되어야한다.( worker node 에는 container가 없다 )123server1( worker node )$ curl server2:8000I'm f8754dbd7fea server2 ( worker node )에는 container가 존재하지않지만 ingress 가 해당 서비스가 존재하는 node를 찾아가는것을 확인할수있다.혹시 ingress가 동작하지않는다면 다음과 같이 docker service를 restart 해준다 .1$ service docker restart 약간의 버그가 있는듯하다. 해당 issue와 관련해 질문이 많은것을 알수있는데 https://github.com/moby/moby/issues/32195 를 참고하도록 하자 .docker swarm 환경에서 ingress의 동작원리를 간단하게 살펴보았다.docker swarm으로 서비스를 구성한다면 traffic에 관련된 처리라던지, 장애에대한 rollback, 그리고 scaling등의 옵션도 있으니 각자가 추구하는 방향으로 서비스를 구성하면 될것이다. 견해필자는 docker swarm 을 알고나서 google의 kubernetes나 , apache의 mesos와 같은 ocastration 에 관심이 더욱더 많아졌다.kubernetes가(docker 기반) 요즘 핫하긴한데. 약간의 진입장벽이 있는듯하다.docker를 좀더 친숙하게 다룬후 kubernetes를 다시 도전해보려한다. 참고사이트https://medium.com/@zh3w4ng/docker-swarm-with-private-registry-for-micro-services-behind-corporate-proxy-bafb349d0b9chttps://subicura.com/2017/02/25/container-orchestration-with-docker-swarm.htmlhttps://docs.docker.com/engine/swarm/https://docs.docker.com/engine/swarm/ingress/","categories":[{"name":"docker","slug":"docker","permalink":"https://setyourmindpark.github.io/categories/docker/"}],"tags":[]},{"title":"docker private registry","slug":"docker/docker-4","date":"2018-02-06T05:33:35.000Z","updated":"2018-02-23T10:31:30.000Z","comments":true,"path":"2018/02/06/docker/docker-4/","link":"","permalink":"https://setyourmindpark.github.io/2018/02/06/docker/docker-4/","excerpt":"","text":"docker private registrydocker를 주로 사용하는 개발자분이라면 한번쯤은 docker hub를 사용하여 public images를 주로 사용하거나, 직접 이미지를 만들어서 사용해보았을 것이다.docker hub와 같은 공개된 이미지 사용이아닌, 나만의 docker hub를 구축하는 방법에 대해 소개하려한다.회사내부 서비스를 개발하거나, 공개되어선 안될 이미지를 나만의 저장소에 보관하고있다가 필요할때 사용할수있는 아주 유용한 방법이다.docker 에서는 이런 저장소를 registry라고 표현하며 어떻게 docker private registry를 구축하는지 알아보자. ssldocker private registry는 보안상 http를 지원하지않는다. local에서 단순히 테스트용도로 사용한다면 문제없지만, 인증관련하여 remote docker private registry에 접근하고자하면 http로는 불가능하다.따라서 https 를 사용하기위한 ssl을 생성하자 .필자는 이전포스트 vagrant 로 환경구성한 centos 2대의 가상환경으로 구성, 테스트를 진행하였다.사설 ssl을 사용해도 무방하지만, 필자는 self signed ssl을 을 사용할것이기에 다음과 같이 생성하였다 .1234567$ mkdir -p ~/docker-registry/cert$ cd ~/docker-registry/cert$ openssl genrsa -des3 -out server.key 2048$ openssl req -new -key server.key -out server.csr$ openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt$ cp server.key server.key.origin$ openssl rsa -in server.key.origin -out server.key 필자는 다음과 같이 진행하였다 . 12345678910111213141516171819202122232425262728293031323334353637383940$ openssl genrsa -des3 -out server.key 2048Generating RSA private key, 2048 bit long modulus..+++............................................................+++e is 65537 (0x10001)Enter pass phrase for server.key:Verifying - Enter pass phrase for server.key:$ openssl req -new -key server.key -out server.csrEnter pass phrase for server.key:You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter '.', the field will be left blank.-----Country Name (2 letter code) [XX]:State or Province Name (full name) []:Locality Name (eg, city) [Default City]:Organization Name (eg, company) [Default Company Ltd]:Organizational Unit Name (eg, section) []:Common Name (eg, your name or your server's hostname) []:setyourmindparkEmail Address []:Please enter the following 'extra' attributesto be sent with your certificate requestA challenge password []:An optional company name []:[root@server1 cert]# openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crtSignature oksubject=/C=XX/L=Default City/O=Default Company Ltd/CN=setyourmindparkGetting Private keyEnter pass phrase for server.key:[root@server1 cert]# lsserver.crt server.csr server.key[root@server1 cert]# cp server.key server.key.origin[root@server1 cert]# openssl rsa -in server.key.origin -out server.keyEnter pass phrase for server.key.origin:writing RSA key[root@server1 cert]# 여기서 반드시 짚고 넘어가야할 점은 전자서명 파일 생성시 기입하게 되는 정보중Common Name (eg, your name or your server’s hostname) []:해당정보를 반드시 registry에서 사용하게될 도메인 name명과 같아야한다는 점이다.필자는 setyourmindpark로 기입하였다. 전자서명 시스템 업데이트docker private registry에 로그인하려면 로그인하는 시스템에서 docker private registry를 구성할때 사용한 ssl server.crt 전자서명 시스템을 업데이트해야한다.docker private registry를 구축한 현재의 서버에서도 당연히 로그인을 할것이기에 update를 해준다.( remote client 에서 해당 docker private registry를 사용하려면 당연히 현재의 작업을 똑같이 수행햐주어야한다 . )참고해야할 점은 시스템에따라 전자서명 update하는 방식이 조금씩은 다르다는 점이다 . ubuntu123$ cp ~/docker-registry/cert/server.crt /usr/share/ca-certificates/$ echo server.crt &gt;&gt; /etc/ca-certificates.conf$ update-ca-certificates centos12$ cp ~/docker-registry/cert/server.crt /etc/pki/ca-trust/source/anchors/ $ update-ca-trust mac os ( client local 개발 환경시 )1$ security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain ~/docker-registry/cert/server.crt 마지막으로 docker를 재시작한다1$ service docker restart docker private registry login passworddocker private registry에서 사용할 인증정보 ( username과 password ) 를 생성한다.12345$ mkdir -p ~/docker-registry/auth$ cd ~/docker-registry/auth$ docker run \\ --entrypoint htpasswd \\ registry -Bbn setyourmindpark 0000 &gt; htpasswd 필자는 username setyourmindparkpassword 0000로 생성하였다.사용하고자하는 인증정보를 기입하면 되겠다. execute docker private registryssl과 , 인증정보를 생성하였으니 이제 docker registry를 container로 실행시켜보자.123456789101112131415$ mkdir -p ~/docker-registry/volume$ docker run -d \\ -p 5000:5000 \\ --restart=always \\ --name registry \\ -v ~/docker-registry/auth:/auth \\ -e \"REGISTRY_AUTH=htpasswd\" \\ -e \"REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm\" \\ -e REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY=/data \\ -v ~/docker-registry/volume:/data \\ -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \\ -v ~/docker-registry/cert:/certs \\ -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/server.crt \\ -e REGISTRY_HTTP_TLS_KEY=/certs/server.key \\ registry 또는 docker-compose를 사용할경우 다음과 같이 docker-compose.yml을 기술하여 사용한다.1234567891011121314151617181920212223$ vi docker-compose.ymlversion: '2'services: docker-registry: image: registry restart: always ports: - \"5000:5000\" volumes: - ~/docker-registry/auth:/auth - ~/docker-registry/cert:/certs - ~/docker-registry/data:/data environment: - REGISTRY_AUTH=htpasswd - REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm - REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY=/data - REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd - REGISTRY_HTTP_TLS_CERTIFICATE=/certs/server.crt - REGISTRY_HTTP_TLS_KEY=/certs/server.key container_name: registry$ docker-compose up -d 주의할점은 위에서 차례대로 생성한 ssl 전자서명 과 htpasswd 파일들의 경로를 volume 으로 guest os 에 mount되기에-v ~/docker-registry/auth:/auth \\-e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \\-v ~/docker-registry/cert:/certs \\-e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/server.crt \\-e REGISTRY_HTTP_TLS_KEY=/certs/server.key \\host os 에 mount 될 경로를 정확히 적어주어야한다. ( -v host os path : guest os path ) host 추가이제 docker private registry에 login할 dns를 등록하자.docker private registry에 로그인시 일반 ip 정보로는 로그인할수없기에 이것또한 반드시 수행되어야할 작업이다 .( remote client 해당 docker private registry를 사용하려면 당연히 현재의 작업을 똑같이 수행햐주어야한다 . )12$ vi /etc/hosts10.10.10.11 setyourmindpark docker private registry login이제 모든 실제로 docker private registry에 로그인해보자 .1234$ docker login setyourmindpark:5000Username (setyourmindpark): Password: Login Succeeded docker private registry custom image pushdocker private registry 구축을 완료하였으니 사용하고자하는 이미지를 만들어 push 해보자 .테스트용도로 간단한 이미지를 만들어서 private registry에 push 한다.12From setyourmindpark/debian-utf8CMD [\"echo\",\"my private image\"] 123$ docker build --tag my-private-image .$ docker tag my-private-image setyourmindpark:5000/my-private-image$ docker push setyourmindpark:5000/my-private-image 이미지를 build 한후, 생성된 이미지를 tag로 alias준후 최종적으로 registry에 push 한다. docker private registry image 확인위에서 my-private-image를 registry에 push하였다.정상적으로 registry에 push가 되었는지 curl 로 확인해보자.12$ curl -k -u 'setyourmindpark:0000' -X GET https://setyourmindpark:5000/v2/_catalog&#123;\"repositories\":[\"my-private-image\"]&#125; 정상적으로 push가 된것을 확인할수있다. remote client에서 docker private registry 이용이제 remote client에서 방금 구축한 docker private registry를 이용하여 이미지 pull 또는 push 하기위해 remote client 설정을 진행한다.위에서 진행한 docker private registry server를 server1으로 지칭하고,remote client 를 server2 라고 지칭하도록한다 .일단 위에서 언급한바와같이 docker private registry가 사용중인 전자서명정보가 똑같이 remote client 에도 존재해야한다.ftp를 사용하든지, scp를 통해 파일을 client에 파일을 넘겨주도록 한다.필자는 scp 명령어를 사용하였다.server1에서 다음 명령어를 사용하였다.12345678910111213141516[server1]$ scp ~/docker-registry/cert/server.crt root@10.10.10.12:[전송받을경로][server2]$ cp server.crt /etc/pki/ca-trust/source/anchors/$ update-ca-trust$ vi /etc/hosts10.10.10.11 setyourmindpark$ service docker restart$ docker login setyourmindpark:5000Username (setyourmindpark): Password: Login Succeeded 참고로 필자는 remote client도 centos를 사용하였기에 전자서명 업데이트를 server1에서 update한 방법과 같다.remote client 가 ubuntu, centos, mac os ( local 개발 환경시 )등은 위의 전자서명 설정 부분에 기술해두었으니 참고하도록한다.정상적으로 remote docker private registry 에 로그인되었다. docker private registry image pull and execute정상적으로 remote docker private registry에 로그인되었다면 아까 만든 my-private-image를 pull 하여 실행해보자12345$ docker pull setyourmindpark:5000/my-private-image$ docker run --name my-private-image setyourmindpark:5000/my-private-imagemy private image[root@server2 ~]# 견해docker private registry 를 구축하는 방법에대해서 알아보았다.개인서버가 없다면 amazone s3에 올리는 방법도 있으니 참고하도록 하자 .필자는 실 서버가 없기에 vagrant환경 centos 2대로 테스트를 진행하였으며, 서비스가 존재하는 회사나 이미지 버전관리가 필요하다면 docker private registry를 직접 구축하여 사용하는것이 좋을것 같다. 참고사이트http://longbe00.blogspot.kr/2015/03/docker_55.htmlhttps://docs.docker.com/registry/deploying/#restricting-accesshttps://docs.docker.com/engine/security/certificates/#creating-the-client-certificateshttp://manuals.gfi.com/en/kerio/connect/content/server-configuration/ssl-certificates/adding-trusted-root-certificates-to-the-server-1605.html","categories":[{"name":"docker","slug":"docker","permalink":"https://setyourmindpark.github.io/categories/docker/"}],"tags":[]},{"title":"vagrant","slug":"vagrant/vagrant","date":"2018-02-05T07:31:30.000Z","updated":"2018-02-23T10:37:32.000Z","comments":true,"path":"2018/02/05/vagrant/vagrant/","link":"","permalink":"https://setyourmindpark.github.io/2018/02/05/vagrant/vagrant/","excerpt":"","text":"vagrant업무상 고객사의 서버환경을 살펴보면, 대부분 단일 서버로 모든 트래픽을 처리하는 곳은 아직까지 보지못하였다. ( 어찌보면 당연하다. )Xen Server를 사용하여 가상화를 통해 서버자원을 극대화하던지, 실제 물리서버를 2대로 운영을하던지, 그러던도중 vagrant라는 놈을 만나게되었다 .Xen Server나 vmware와 가장 두드러지게 다른점이라면, 환경구성하고자하는 os를 직접 준비할 필요가 없다는점이다. 필요한 os를 설정파일에 설정하게되면 아주간단히 가상환경을 구축할수있다.필자는 앞으로 소개할 docker swarm cluster 를위해 vagrant로 환경구성을 해보려한다 . virtual box 와 vagrnat 설치vagrnat 가상환경을 만들기위해 기본적으로 virtualbox와 vagrant 가 설치되어있어야한다 .virtualboxvagrant설치가 진행되었다면 vagrant 로 환경구성 을 진행해보자 . vagrant 설정1$ vagrant init VagrantFile 이 생성된다 ( DockerFile 이생각나지왜 .. )VagrantFile을 열어보면 다음과같이 설정되어있고, 기타설정들은 모두 주석처리되어있는것을 확인할수있다 .1config.vm.box = \"base\" 필자는 가상화서버 2대를 centos7로 사용할것이기에 다음과 같이 진행하였다 . server1123config.vm.box = \"centos/7\"config.vm.hostname = \"server1\"config.vm.network \"private_network\", ip: \"10.10.10.11\" server2123config.vm.box = \"centos/7\"config.vm.hostname = \"server2\" config.vm.network \"private_network\", ip: \"10.10.10.22\" 필자는 가상화 서버 hostname 설정 =&gt; config.vm.hostname고정아이피 설정 =&gt; config.vm.network그리고 사용할 os는 centos 7를 사용하였다 .구성하고자 하는 os는 다음 사이트에서 확인할수있다.vagrant support os vagrant 가상환경 구축설정이 끝났다면 이제 설정된 정보 기반으로 vagrant 가상환경을 구축한다.VagrantFile 이 위치한 경로에서 다음의 명령어를 사용한다.1$ vagrant up 설치진행이 모두 완료되었다면 다음의 명령어로 가상환경으로 접속한다.12$ vagrant ssh[vagrant@server1 ~]$ 구성한 설정정보로 server1의 hostname으로 접속된것을 확인할수있다. 1$ yum update 깔끔하게 centos package update 최신으로 업데이트 하고, 각자가 원하는 추가구성 또는 설치를 진행한다. vagrant 명령어1234567$ vagrant up =&gt; 구축$ vagrant ssh =&gt; 접속$ vagrant destroy =&gt; 삭제$ vagrant halt =&gt; 종료$ vagrant suspend =&gt; 일시중지 ( 현재상태저장 )$ vagrant resume =&gt; 다시시작 ( suspend된 후 ) $ vagrant reload =&gt; 재시동 ( 설정파일변경 등 ) 견해vagrant로 정말 간단하게 원하는 환경을 쉽고 빠르게 구축할수있었다.간간히 구글링을하면 vagrant 와 docker 비교관련 키워드를 심심치 않게 마주치게되는데,그럼 docker 와 vagrant는 무엇이다를까 ?docker와 vagrant 모두 독자적인 os 환경을 구성할수있다는 공통점이 있지만 docker는 가상환경이라닌 ‘격리’ 라는 표현을 사용한다.host os의 linux 자원을 geust os 가 공유하여 사용함으로 vagrant와 같이 가상화를 수행하지않고도 빠르게 가상환경과 같은 효과를 낼수있다 .window에서 docker 환경을 구성한다면 가상환경을 사전에 먼저 구성해야 할것이다. ( host linux os 로부터 자원을 공유받아야하므로 .. )","categories":[{"name":"vagrant","slug":"vagrant","permalink":"https://setyourmindpark.github.io/categories/vagrant/"}],"tags":[]},{"title":"javascript callback mechanism","slug":"nodejs/nodejs-5","date":"2018-02-04T07:03:42.000Z","updated":"2018-02-06T05:28:50.000Z","comments":true,"path":"2018/02/04/nodejs/nodejs-5/","link":"","permalink":"https://setyourmindpark.github.io/2018/02/04/nodejs/nodejs-5/","excerpt":"","text":"callback비동기 프로그래밍에서 callback을 사용하여 어떻게 데이터를 처리하는지 글을 써보려한다.고전적으로 javascript에서 callback 을 처리하는 방법과, es6 공식탑재된 promise, 그리고 es7 에 탑재될예정(?) 인 async await 을 사용하여 어떻게 callback을 처리하는지 알아보자.다음에서 제시될 tasks(task1, task2, task3 )들은 어떠한 로직들을 처리하는 함수들이며(비동기효과를 위해 timeout으로 .. ), doProcess 함수에서 해당 tasks들을 처리하여 결과를 얻도록 구성하였다 . classic한번쯤은 callback hell 이라는 용어를 들어보았을것이다. 중첩된(nested) callback으로 개발 유지보수의 어려움과 코드 가독성을 현저히 떨어뜨리게 되는데 우선 고전적인 javascript callback의 사용법부터 알아보자.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768function task1(value , callback)&#123; if(value)&#123; setTimeout(() =&gt; &#123; console.log('task1 done'); value += 1; callback(value); &#125;, 1000); &#125;else&#123; callback() &#125;&#125;function task2(value, callback)&#123; if (value) &#123; setTimeout(() =&gt; &#123; console.log('task2 done'); value += 2; callback(value); &#125;, 2000); &#125; else &#123; callback() &#125;&#125;function task3(value, callback) &#123; if (value) &#123; setTimeout(() =&gt; &#123; console.log('task3 done'); value += 3; callback(value); &#125;, 3000); &#125; else &#123; callback() &#125;&#125;function doProcess(value)&#123; task1(value, function(result1)&#123; if(result1)&#123; task2(result1, function(result2)&#123; if(result2)&#123; task3(result2, function(result3)&#123; if(result3)&#123; console.log('####################### process done #####################') console.log(result3) &#125;else&#123; console.log('something wrong ! in task3'); &#125; &#125;) &#125;else&#123; console.log('something wrong ! in task2'); &#125; &#125;) &#125;else&#123; console.log('something wrong ! in task1'); &#125; &#125;)&#125;doProcess(1); 해당프로세스가 정상적으로 수행되었다면 6초뒤 7이라는 값이 찍히도록 구성하였다 .일단 doProcess() 를 주의깊게보면, 사전적으로 수행되어야할 task가 처리될때까지 기다린후 해당 task를 실행한다. 따라서 task들 각각의 수행중 발생할수있는 에러나, 결과 리턴에 대한 데이터 핸들링이 모두 필요할수밖에없다. 일단 가독성 측면으로 보았을때 상당히 좋지않다. 더나아가 처리될 프로세스가 더 많을 경우 더욱더 난해해질것이다. ( 전통적인 callback hell 을 보여준다 ) promise고전적인 callback hell을 벗어나, promise가 나오게된후 개발이 무척이나 편해지고 코드가 깔끔해졌다. 다음의 코드를 보자 . 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253function task1(value)&#123; return new Promise((resolve, reject) =&gt; &#123; if(value)&#123; setTimeout(() =&gt; &#123; console.log('task1 done'); value += 1; resolve(value) &#125;, 1000); &#125;else&#123; reject('something wrong in task1'); &#125; &#125;)&#125;function task2(value) &#123; return new Promise((resolve, reject) =&gt; &#123; if (value) &#123; setTimeout(() =&gt; &#123; console.log('task2 done'); value += 2; resolve(value) &#125;, 2000); &#125; else &#123; reject('something wrong in task2'); &#125; &#125;)&#125;function task3(value) &#123; return new Promise((resolve, reject) =&gt; &#123; if (value) &#123; setTimeout(() =&gt; &#123; console.log('task3 done'); value += 3; resolve(value) &#125;, 3000); &#125; else &#123; reject('something wrong in task3'); &#125; &#125;)&#125;function doProcess(value) &#123; task1(value) .then(result1 =&gt; task2(result1)) .then(result2 =&gt; task3(result2)) .then(result3 =&gt; &#123; console.log('####################### process done #####################') console.log(result3); &#125;) .catch(err =&gt; console.log(err));&#125;doProcess(1); doProcess() 함수를 유심히보면, 위의 고전적인 방법보다 코드가 엄청나게 많이 줄어든것을 확인할수있다. 또한 task들의 error 핸들링은 한곳에서 처리하니, 무척이나 사용이 간편해지고 또한 중첩된 함수가 없으므로 가독성이 월등히 좋아졌다. (chain 형식으로 비동기데이터를 핸들링한다. ) async awaitasync await은 promise를 기반으로 동작하기에 위의 promise tasks들과 크게다르지않다. 단지 ‘이 함수는 비동기적으로 수행하는 함수야’ 라고 async키워드를 붙이고, 해당 함수를 호출할때는 ‘기다려’ await 키워드를 붙여주기만 하면된다. 다음을 보자. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859async function task1(value) &#123; return new Promise((resolve, reject) =&gt; &#123; if (value) &#123; setTimeout(() =&gt; &#123; console.log('task1 done'); value += 1; resolve(value) &#125;, 1000); &#125; else &#123; reject('something wrong in task1'); &#125; &#125;)&#125;async function task2(value) &#123; return new Promise((resolve, reject) =&gt; &#123; if (value) &#123; setTimeout(() =&gt; &#123; console.log('task2 done'); value += 2; resolve(value) &#125;, 2000); &#125; else &#123; reject('something wrong in task2'); &#125; &#125;)&#125;async function task3(value) &#123; return new Promise((resolve, reject) =&gt; &#123; if (value) &#123; setTimeout(() =&gt; &#123; console.log('task3 done'); value += 3; resolve(value) &#125;, 3000); &#125; else &#123; reject('something wrong in task3'); &#125; &#125;)&#125;async function doProcess(value)&#123; try&#123; const result1 = await task1(value); const result2 = await task2(result1); const result3 = await task3(result2); console.log('####################### process done #####################') console.log(result3); &#125;catch(err)&#123; console.log(err); &#125;&#125;;(async () =&gt; &#123; await doProcess(1);&#125;)() doProcess()함수를 보면 이제는 비동기 로직을 동기코드로 동작하는 효과처럼 보인다.개발자는 비동기함수를 호출한뒤 동기코드를 작성한것처럼 리턴받기만 하면되는데 프로세스 로직구성하기가 promise 보다 더 쉬워지고, promise와 마찬가지로 error 핸들링은 catch에서 처리하면된다 . 참고필자는 현재시점 LTS v8.9.4 버전을 사용하였다 .추가적으로 async await은 항상 쌍으로 사용하는것이 좋다고 생각된다. ( 시작은 당연히 async 부터 .. )사실 위의 promise style의 tasks들은 async await tasks로 사용해도 똑같이 동작한다.promise를 사용한다고해서 암묵적으로 async를 생략하고 사용하는것보다는 명시적으로 async를 붙여주는것이 좋다고 생각한다.","categories":[{"name":"nodejs","slug":"nodejs","permalink":"https://setyourmindpark.github.io/categories/nodejs/"}],"tags":[]},{"title":"setup mariadb","slug":"database/database","date":"2018-02-02T13:17:54.000Z","updated":"2018-02-11T04:28:55.000Z","comments":true,"path":"2018/02/02/database/database/","link":"","permalink":"https://setyourmindpark.github.io/2018/02/02/database/database/","excerpt":"","text":"mariadbdb를 docker official image로만 사용하다보니 mysql 또는 mariadb를 manually하게 설치하는법을 잊고 살았다.그냥 docker ofiicial mariadb를 사용하지 왜 ? 라고 반문한다면, 문득 database와 같은 서비스의 가장 하위 layer level에 있는 서비스들은 다른 의존환경없이 독립된 환경에 존재하는 것이 좋겠다는 생각이 들어서다.막상 설치하려보니, 여기저기서 구글링하여 찾게되어 정리하려한다. installcentos7 환경에서 mariadb를 설치를진행한다.모든 작업은 root로 진행하였으며 다음과 같이 reposotory를 등록후 yum으로 install 한다.1234567891011$ vi /etc/yum.repos.d/MariaDB.repo# MariaDB 10.2 CentOS repository list - created 2018-02-08 13:29 UTC# http://downloads.mariadb.org/mariadb/repositories/[mariadb]name = MariaDBbaseurl = http://yum.mariadb.org/10.2/centos7-amd64gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDBgpgcheck=1$ yum install MariaDB-server MariaDB-client mariadb official document 에 아주 자세히나와 있다.설치가 모두 진행된후 mariadb service를 start 한다.123$ systemctl start mariadbor$ service mariadb start 마지막으로 anonymous user 접속 차단과, root password 설정, local 접속여부, test database 삭제등 추가 설정을 한다 .1$ mysql_secure_installation 접속모든 설치와 설정이 끝이났다. 이제 접속을해보자.1$ mysql -u root -p utf8 설정mariadb를 최초 설치하였다면 database character set이 latin1로 되어있을것이다. 그렇지 않은경우도있으니 다음명령어로 확인한다 . 1234567891011121314151617181920$ mysql &gt; show variables like 'c%';+--------------------------+----------------------------+| Variable_name | Value |+--------------------------+----------------------------+| character_set_client | utf8 || character_set_connection | utf8 || character_set_database | latin1 || character_set_filesystem | binary || character_set_results | utf8 || character_set_server | latin1 || character_set_system | utf8 || character_sets_dir | /usr/share/mysql/charsets/ || check_constraint_checks | ON || collation_connection | utf8_general_ci || collation_database | latin1_swedish_ci || collation_server | latin1_swedish_ci || completion_type | NO_CHAIN || concurrent_insert | AUTO || connect_timeout | 10 |+--------------------------+----------------------------+ character_set_database 이 latin1으로 되어있다면, 다음과같이 utf8 세팅을 추가한다.12345$ vi /etc/my.cnf...[mysqld]character-set-server=utf8... 수정후 mariadb 재시작 한다.1$ service mariadb restart character set 변경사항 적용 확인한다 .1234567891011121314151617181920$ mysql &gt; show variables like 'c%';+--------------------------+----------------------------+| Variable_name | Value |+--------------------------+----------------------------+| character_set_client | utf8 || character_set_connection | utf8 || character_set_database | utf8 || character_set_filesystem | binary || character_set_results | utf8 || character_set_server | utf8 || character_set_system | utf8 || character_sets_dir | /usr/share/mysql/charsets/ || check_constraint_checks | ON || collation_connection | utf8_general_ci || collation_database | utf8_general_ci || collation_server | utf8_general_ci || completion_type | NO_CHAIN || concurrent_insert | AUTO || connect_timeout | 10 |+--------------------------+----------------------------+ 유저생성12345678# 외부 접근가능 $ mysql &gt; create user '사용자'@'%' identified by '비밀번호';# local만 접근가능$ mysql &gt; create user '사용자'@'localhost' identified by '비밀번호';# 특정 ip만 접근가능$ mysql &gt; create user '사용자'@'ip' identified by '비밀번호'; 권한부여12345# 모든db 접근가능$ mysql &gt; grant all privileges on *.* to '사용자'@'host';# 특정 db 접근가능$ mysql &gt; grant all privileges on db이름.* to '사용자'@'host'; grant all privileges .. 은 select, update, insert, delete, create 등 DML, DDL 권한을 줄수도있다. 권한확인1$ mysql &gt; show grants for '사용자'@'host'; 권한삭제1$ mysql &gt; revoke all on *.* from '사용자'@'host' 변경사항 적용grant, create user, drop user 등 query로 수행되는 작업이아닌 명령어로 수행되는 작업들은 reload 함으로서 즉시반영할수있다.1$ mysql &gt; flush privileges; DML(insert, delete, update) 로 수행된작업들은 할필요없다 .","categories":[{"name":"database","slug":"database","permalink":"https://setyourmindpark.github.io/categories/database/"}],"tags":[]},{"title":"compare java and javascript","slug":"nodejs/nodejs-4","date":"2017-05-14T09:28:26.000Z","updated":"2018-02-06T05:28:48.000Z","comments":true,"path":"2017/05/14/nodejs/nodejs-4/","link":"","permalink":"https://setyourmindpark.github.io/2017/05/14/nodejs/nodejs-4/","excerpt":"","text":"서론필자는 아직도 java가 좀더 친숙한면이있다 . java의 꽃이라불리는 상속, 상속을 이용한 메서드 오버라이딩(overriding), 그리고 메서드 오버로딩(overloading) 을 javascript에서 어떻게 사용하는가에 대한 글을 적어보려한다 .필자도 javascript에 입문한지 얼마되지않았지만. 정리하며 익숙해지려 노력중이다 .javascript 만의 장점도 있지만, 자바의 객체지향( Object Oriented Programming ) 이하 OOP는 검증된 디자인 패턴이므로 javascript 에서도 적용해본다 .필자도 여러 블로거님들의 글을 참고하였으며. 하단의 [ 참고 ] 탭에서 필자가 참고한 사이트를 적어놓았으니 참고하면 좋을듯 하다 . java 와 javascript먼저 필자는 간단한 예제를 준비했다 . javaMyParent.java123456789101112//부모클래스public class MyParent &#123; private String myName = \"chahyunpark\"; private String mySex = \"남\"; private int myAge = 57; public void myInfo()&#123; System.out.println(\"myFather name : \" + this.myName); System.out.println(\"myFather sex : \" + this.mySex); System.out.println(\"myFather age : \" + this.myAge); &#125;&#125; MyChild.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647 //자식클래스public class MyChild extends MyParent &#123; private String myName = \"jaehunpark\"; private String mySex = \"남\"; private int myAge = 26; private String myJuminNo = \"921234-5678910\"; private String myLaptop = \"mac book\"; private String myPhone = \"galaxy\"; //private method private String getMyJuminNo()&#123; return \"my juminNo : \" + this.myJuminNo; &#125; public void myFamillyInfo()&#123; super.myInfo(); myInfo(); &#125; //override @Override public void myInfo() &#123; // TODO Auto-generated method stub System.out.println(\"my name : \" + this.myName); System.out.println(\"my sex : \" + this.mySex); System.out.println(\"my age : \" + this.myAge); System.out.println(getMyJuminNo()); &#125; //basic mathod public void myBelongings() &#123; System.out.println(\"my laptop : \" + this.myLaptop); System.out.println(\"my phone : \" + this.myPhone); &#125; //myBelongings overloading public void myBelongings(String myLaptop, String myPhone) &#123; this.myLaptop = myLaptop; this.myPhone = myPhone; System.out.println(\"my new laptop : \" + this.myLaptop); System.out.println(\"my new phone : \" + this.myPhone); &#125;&#125; Run.java1234567891011121314public class Run &#123; public static void main(String [] args )&#123; MyChild child = new MyChild(); System.out.println(\"ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡmyInfoㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\"); child.myInfo(); System.out.println(\"ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡmyBelongingsㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\"); child.myBelongings(); System.out.println(\"ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡmyBelongings(overloading)ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\"); child.myBelongings(\"mac book pro 15\",\"iphone 7\"); System.out.println(\"ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡmyFamillyInfoㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\"); child.myFamillyInfo(); System.out.println(\"ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\"); &#125;&#125; javascriptMyParent.js1234567891011121314module.exports = MyParent;// 부모 클래스function MyParent() &#123; this.myFatherName = 'chahyunpark'; this.myFatherSex = '남'; this.myFatherAge = 57;&#125;MyParent.prototype.myInfo = function () &#123; console.log('myFather name : ' + this.myFatherName ); console.log('myFather sex : ' + this.myFatherSex ); console.log('myFather age : ' + this.myFatherAge );&#125;; MyChild.js123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051module.exports = MyChild;var MyParent = require('./MyParent');// 자식 클래스function MyChild() &#123; this.myName = 'jaehunpark'; this.mySex = '남'; this.myAge = 26; this.myLaptop = 'mac book'; this.myPhone = 'galaxy';&#125;// 부모 클래스 상속MyChild.prototype = new MyParent();// 생성자 설정MyChild.prototype.constructor = MyChild;// 부모 클래스 myInfo overrideMyChild.prototype.myInfo = function () &#123; //closure를 이용한 private method function getMyJuminNo()&#123; return 'my juminNo : 921234-5678910'; &#125; console.log('my name : ' + this.myName ); console.log('my sex : ' + this.mySex ); console.log('my age : ' + this.myAge ); console.log(getMyJuminNo());&#125;;MyChild.prototype.myFamillyInfo = function () &#123; MyParent.prototype.myInfo.call(this); // call() 을 이용해 부모 클래스의 myInfo()메서드를 호출한다. this.myInfo();&#125;;// overloadingMyChild.prototype.myBelongings = function () &#123; switch (arguments.length) &#123; case 0 : console.log('my laptop : ' + this.myLaptop); console.log('my phone : ' + this.myPhone); break; case 2 : this.myLaptop = arguments[0]; this.myPhone = arguments[1]; console.log('my laptop : ' + this.myLaptop); console.log('my phone : ' + this.myPhone); break; &#125;&#125;; Run.js1234567891011var MyChild = require('./MyChild');var child = new MyChild();console.log('ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡmyInfoㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ');child.myInfo();console.log('ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡmyBelongingsㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ');child.myBelongings();console.log('ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡmyBelongings(overloading)ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ');child.myBelongings('mac book pro 15', 'iphone 7');console.log('ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡmyFamillyInfoㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ');child.myFamillyInfo();console.log('ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ'); 실행결과는 java와 javascript 모두 동일하다 .123456789101112131415161718192021ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡmyInfoㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡmy name : jaehunparkmy sex : 남my age : 26my juminNo : 921234-5678910ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡmyBelongingsㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡmy laptop : mac bookmy phone : galaxyㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡmyBelongings(overloading)ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡmy laptop : mac book pro 15my phone : iphone 7ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡmyFamillyInfoㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡmyFather name : chahyunparkmyFather sex : 남myFather age : 57my name : jaehunparkmy sex : 남my age : 26my juminNo : 921234-5678910ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ 설명java는 super라는 keyword가 존재하여 super.myInfo() 를통해 부모 클래스의 메서드를 호출할수있다.하지만 javascript는 부모 클래스의 메서드를 호출할수있는 키워드가 없으므로 , call() 또는 apply() 를 호출하여 부모 클래스에 정의된 함수를 호출한다 .또하나 . javascript 같은경우는 상속을 MyChild.prototype = new MyParent(); 와 같이 정의하기에 정의하는순간부터 context ( this ) 는 부모 클래스와 공유된다 .이러한 이유로 MyParent.js 와 MyParent.java 를 보면 변수명이 다를것이다 .javascript는 상속받은 부모의 변수가 자식의 변수와 겹치면 자식의 this 우선적으로 적용한다. ( 겹치면안된다 . )자바의 private 메서드는 javascript에서 클로저( closure ) 로 구현이가능하며, overloading은 메서드 인자갯수에 따라 구현이가능하다 .간단한 예제로 글을 작성하였지만, 예제가 모든걸 설명해주고 있으니 자주까먹는 필자도 이 예제를 간간히 보며 곱씹어야겠다는 생각이든다 .이 글을 작성하며 javascript가 조금은 더 친숙해진듯 하다 . 참고http://webclub.tistory.com/404http://www.florakid.com/florakid_lib/sub/javascript/apply_call_method.htmlhttps://developer.mozilla.org/ko/docs/Web/JavaScript/Guide/Closureshttps://developer.mozilla.org/ko/docs/Web/JavaScript/Introduction_to_Object-Oriented_JavaScript","categories":[{"name":"nodejs","slug":"nodejs","permalink":"https://setyourmindpark.github.io/categories/nodejs/"}],"tags":[]},{"title":"async await","slug":"nodejs/nodejs-3","date":"2017-05-13T12:22:22.000Z","updated":"2018-02-23T10:37:20.000Z","comments":true,"path":"2017/05/13/nodejs/nodejs-3/","link":"","permalink":"https://setyourmindpark.github.io/2017/05/13/nodejs/nodejs-3/","excerpt":"","text":"async awaites7 공식발표가 되진않았지만 es7에 async와 await 가 잠정적으로 탑재되는 분위기인것 같다 . es6 의 promise가 탑재된듯이 .필자는 nodejs 개발시 Promise ( bluebird module )를 아주 적극적으로 사용하고있다 . 변화를 느끼고자 이전포스트 Promise ReactiveX에서 ReactiveX를 사용해보았지만 코드 가독성이 Promise 보다는 떨어지기에 적극적으로 도입하긴보다는 Promise와 적절히 섞어 쓰는것이 좋겠다 라는 결론을 지었다.이번에는 async await 에대한 글을 적어보려한다. 결론부터 말하자면 굉장히 아주 매우 마음에드는 녀석이다 . more than higher node 7.6async await keyword 는 기본적으로 node 7 이상에서 동작한다 ( v7.6 )필자는 ‘ async await 을 사용해봐야지 ‘ 라고 생각하기전까지는 v6.10.3 를 줄곧사용하였다 .nvm이 정말 유용한 녀석이라고 생각한순간이 바로 이순간이었다. nvm에대해서는 nvm 참고하자 .추가적으로 필자는 지금 이 글을 쓰기위한 node 기반 hexo 를 사용할때는 v6.10.3 로 switch 한다. ( node v.7.10.0 에서 hexo 설치시 버그가 있는듯하다 . ) 설치가능 node version 확인1$ nvm ls-remote node 설치1$ nvm install &lt;node버전&gt; node version 변경1$ nvm alias default &lt;node버전&gt; 확인1$ node --verion 필자는 node 최신버전 v.7.10.0 를 사용한다 . 어떻게 사용하는가필자도 쓰면서 알게되었지만. async await은 내부적으로 기본 Promise가 탑재되어있다는것을 알게되었다 .Promise를 즐겨쓰는 개발자라면 학습하는데에 있어 어렵지않을것이다 .먼저 Promise와 async awawit을 비교해보자 . async tasks다음과 같이 비동기 task 들을 정의하였다 .1234567891011121314151617181920212223function asyncTask1() &#123; return new Promise((resolve, reject) =&gt; &#123; setTimeout(() =&gt; &#123; resolve('data1'); &#125;, 1000); &#125;);&#125;function asyncTask2() &#123; return new Promise((resolve, reject) =&gt; &#123; setTimeout(() =&gt; &#123; resolve('data2'); &#125;, 2000); &#125;);&#125;function asyncTask3() &#123; return new Promise((resolve, reject) =&gt; &#123; setTimeout(() =&gt; &#123; resolve('data3'); &#125;, 3000); &#125;);&#125; Promise1234567891011121314151617181920// single execution ( 단일실행 )function simpleWork()&#123; asyncTask1() .then(data =&gt; console.log(data)) .catch(err =&gt; console.log(err))&#125;//sequential execution ( 순차실행 )function seqWork()&#123; asyncTask1() .then(data =&gt; &#123; console.log(data); return asyncTask2(); &#125;) .then(data =&gt; &#123; console.log(data); return asyncTask3(); &#125;) .then(data =&gt; console.log(data)) .catch(err =&gt; console.log(err))&#125;// unify after execution ( task들 실행후 결과값 합치기 )function unifyWork()&#123; Promise.all([asyncTask1(), asyncTask2(),asyncTask3()]) .then(data =&gt; console.log(data)) .catch(err =&gt; console.log(err))&#125; async await1234567891011121314151617181920212223242526// single execution ( 단일실행 )async function simpleWork()&#123; try&#123; console.log(await asyncTask1()); &#125;catch(err)&#123; console.log(err); &#125;&#125;//sequential execution ( 순차실행 )async function seqWork()&#123; try&#123; console.log(await asyncTask1()); console.log(await asyncTask2()); console.log(await asyncTask3()); &#125;catch(err)&#123; console.log(err); &#125;&#125;// unify after execution ( task들 실행후 결과값 합치기 )async function unifyWork()&#123; try&#123; console.log(await Promise.all([asyncTask1(), asyncTask2(), asyncTask3()])); &#125;catch(err)&#123; console.log(err); &#125;&#125; Promise와 async await 실행결과는 모두 동일하다 .12345simpleWork() // data1seqWork() // data1 // data2 // data3unifyWork() // [ 'data1', 'data2', 'data3' ] Promise와 async await 기본 사용법에대해 알아보았다 .Promise도 분명 간결하나, 실행해야될 로직 전에 사전에 수행되어야할 의존된 로직이있을시에는 Promise chain ( .then() ) 을걸어 정의해야한다.그런부분에 있어서는 async await 은 비동기 코드가 마치 동기 코드로 동작하기때문에 상당히 메리트가 있는것 같다 . async awiat with rxasync await 을 rx와 같이 사용해보자 .async await 도 Promise 기반으로 동작되기도하고, 가독성도 훨씬 좋아졌으니 덧붙여 적절한 상황에서 rx 를 사용하면 금상첨화라고 생각했다 .1234567891011121314151617181920212223async function asyncTask() &#123; return new Promise((resolve, reject) =&gt; &#123; setTimeout(() =&gt; &#123; resolve([1, 2, 3, 4, 5, 6, 7, 8]); &#125;, 2000); &#125;);&#125;async function doWork()&#123; Rx.Observable.from(await asyncTask()) .filter(num =&gt; num % 2 ) .map(num =&gt; num * num ) .subscribe( result =&gt; console.log(result), err =&gt; console.log(err), () =&gt; &#123;&#125; );&#125;doWork(); //1 //9 //25 //49 사실 async await 만을 사용하여도 로직을 처리하는데에 충분이 문제가되지않지만 필자는 rx 역시 로직 처리에있어서는 분명 이점이있는 모듈이라고 생각되어 같이 사용해보았다. 견해front-end 에서 async await 을 적용하려면, 역시 node version ( 7.6 ) 이상과 babel을 필수로 도입해야할것이다 .모든 브라우저가 async await을 지원하지않기에 babel를 통해 es5 코드로 변경된 코드를 적용해야하기 때문이다 .사용을해본후 드는 생각으로는 async await 을 적극적으로 사용해야겠다는 생각과 무엇보다 동기코드로 작성할수있기에 callback hell 에대한 걱정이 없어 좋다 .필자는 node version 7 위한 docker images을 만들어놓았다. setyourmindpark/debian-node:7해당 이미지를 base로 하여 추가적인 node 관리 pm2나, 필요한 모듈을 설치하여 새로운 이미지를 만들어 사용하면될것이다 .","categories":[{"name":"nodejs","slug":"nodejs","permalink":"https://setyourmindpark.github.io/categories/nodejs/"}],"tags":[]},{"title":"ssh scp proxy","slug":"jenkins/jenkins-4","date":"2017-05-11T04:04:37.000Z","updated":"2018-02-23T09:34:43.000Z","comments":true,"path":"2017/05/11/jenkins/jenkins-4/","link":"","permalink":"https://setyourmindpark.github.io/2017/05/11/jenkins/jenkins-4/","excerpt":"","text":"ssh scp필자는 jenkins를 주로 사용하며 빌드는 nodejs 기반의 서버를 배포시에는 ssh 와 scp 를 주로사용한다 .ssh와 scp를 사용하는데에 있어 최종 서버에 접근하기까지 중간에 bridge( jump host )와 같은 host가 존재할때가 있다 .물론 접근하고자하는 서버의 ip를 모두 public 으로 개방하면 쉽게 접근이가능하나 관리가 어려워질뿐만아니라 보안적으로도 좋지않기때문에 public host가 proxy host가 되어 내부 서버로 접근해야한다.필자는 jenkins로 빌드할때 이런 상황이 있었으며 정리를하며 기록으로 남기기위해 글을써보려 한다 . 사전준비필자는 기본적으로 jenkins 를 사용하여 원격지에 배포하므로 원격서버 접속시 jenkins 에게 비밀번호를 요구하지 않도록 rsa 암호화통신을 위한 사전 설정한다 . jenkins1$ cat ~/.ssh/id_rsa.pub proxy host1$ vi ~/.ssh/authorized_keys destination host1$ vi ~/.ssh/authorized_keys jenkins 에서 cat 명령어로 출력된 id_rsa.pub ( public key ) 를 proxy host 와 최종 접속할 destination host 의 ~/.ssh/authorized_keys 에 붙여넣기한다.작업을 마치게되면 jenkins는 proxy host를 거쳐 destination host 에 비밀번호없이 직접적으로 접근이 가능해졌다. ssh scp 일반적인 사용기본적으로 proxy host를 사용하지않고 단일 remote host ssh 와 scp 는 다음과같다.1$ ssh &lt;계정&gt;@&lt;remote host ip&gt; 1$ scp -r &lt;전송할 파일/폴더&gt; &lt;계정&gt;@&lt;remote host ip&gt;:&lt;remote host에 전송받을 경로&gt; 이제 proxy host를 경유하여 destination host에 ssh scp 사용법을 알아보자 . ssh필자의 접속 정보는 다음과 같다 . (예시)proxy host : 121.140.166.90destination host : 10.10.200.3 manual proxy기본적인방법1$ ssh -tt jaehunpark@121.140.166.90 ssh root@10.10.200.3 아주 간단하게 쓸수있는방법이다 .단순히 -tt 옵션과 ssh 명령어를 두번적어주면된다.하지만 추가적인 설정이 필요하다. 위의 명령어는 proxy host( 121.140.166.90 )로 접속후 proxy host가 destincation host( 10.10.200.3 ) 로 접속을 시도하므로 proxy host의 id_rsa.pub (public key)가 destination host 의 ~/.ssh/authorized_keys 에 존재해야 호출하는 머신에서 direct로 접근가능하다.결론적으로 destination host의 authorized_keys 에는 jenkins host의 id_rsa.pub 와 proxy host id_rsa.pub 두개가 존재해야한다 .( jenkins를 사용하지않고 일반적으로 접속하시는것이라면 그냥 비밀번호 치셔도 됩니다 ) proxycommand 사용1$ ssh -o ProxyCommand=&quot;ssh -W %h:%p jaehunpark@121.140.166.90&quot; root@10.10.200.3 proxy host를 통하여 직접적으로 접근하는 proxycommand를 사용한 방법이다 proxy after configuration1$ ssh server-node-1 하단의 configuration 탭의 설정을 마친후 proxy host를 거쳐 destination host에 접근하는 명령어이다.설정만 잘 해놓는다면 정말 간단하게 접근할수있다. scpmanual proxy첫번째방법1$ scp -r -o ProxyCommand=&quot;ssh jaehunpark@121.140.166.90 nc 10.10.200.3 22&quot; /root/.jenkins/workspace/nodejs-skeletone-v2 root@10.10.200.3:/root 두번째방법1$ scp -r -o ProxyCommand=&quot;ssh -W %h:%p jaehunpark@121.140.166.90&quot; /root/.jenkins/workspace/nodejs-skeletone-v2 root@10.10.200.3:/root 첫번째방법과 두번째방법 scp 역시 ssh와 비슷하게 proxycommand 를 사용하여 파일을 전송한다. 명령어가 길다보니 조금은 지저분한 느낌이다 . 아래와같이 간단하게 사용할수있다 proxy after configuration1$ scp -r nodejs-skeletone-v2 server-node-1:/root ssh와 마찬가지로 configuration 탭의 설정을 마친후 proxy host 를 거쳐 destination host 로 파일을 전송한다 configuration1$ vi ~/.ssh/config 12345Host server-node-1HostName 10.10.200.3User rootIdentityFile ~/.ssh/id_rsaProxyCommand ssh -W 10.10.200.3:22 jaehunpark@121.140.166.90 참고proxy host 를 거쳐 destination host 로 ssh 나 scp 를 최초사용시에는 아래와같은 문구를 만나게될것이다.12345root@467c66252dcd:~/.ssh# ssh server-node-1Warning: Permanently added &apos;121.140.166.90&apos; (ECDSA) to the list of known hosts.The authenticity of host &apos;10.10.200.3 (&lt;no hostip for proxy command&gt;)&apos; can&apos;t be established.ECDSA key fingerprint is 88:65:92:d8:f8:75:b4:99:1a:f1:52:e8:b2:03:0e:a6.Are you sure you want to continue connecting (yes/no)? 최초 접속시에는 ~/.ssh/known_hosts 접속 정보가 존재하지 않기 때문에 연결여부를 물어본후 ‘yes’ 를 입력하게되면 ~/.ssh/known_hosts 에 등록하게된다 .개발자가 배포설정을 마치고 jenkins에게 빌드를 시작할때 jenkins는 최초접속이므로 접속정보가 존재하지않아 위의 문구를 만나게되면 가차없이 build fail이 떨어질것이다 .또한 현재 세팅상태로는 proxy host와 destination host 두개의 host 정보가 등록되지않아 두번 요구하게될것이다 .사전에 다음과같이 해결할수있다 . jenkins1$ ssh-keyscan -H 121.140.166.90 &gt;&gt; ~/.ssh/known_hosts proxy host ip를 known_hosts를 등록한후1$ vi ~/.ssh/config 123456Host server-node-1StrictHostKeyChecking noHostName 10.10.200.3User rootIdentityFile ~/.ssh/id_rsaProxyCommand ssh -W 10.10.200.3:22 jaehunpark@121.140.166.90 StrictHostKeyChecking no 를 추가하여 destination host ( 10.10.200.3 ) 의 접속여부를 무시한다 . ( auto yes )1$ ssh server-node-1 최초접속이라도 어떠한 요구도 묻지않고 proxy host를 거쳐 destination host로 접속할수있다 . ( 위의 설정을 마치고나게되면 known_hosts 에 등록되게된다 )","categories":[{"name":"jenkins","slug":"jenkins","permalink":"https://setyourmindpark.github.io/categories/jenkins/"}],"tags":[]},{"title":"nginx ssl 적용하기","slug":"nginx/nginx-1","date":"2017-05-04T11:36:27.000Z","updated":"2018-03-10T05:18:59.000Z","comments":true,"path":"2017/05/04/nginx/nginx-1/","link":"","permalink":"https://setyourmindpark.github.io/2017/05/04/nginx/nginx-1/","excerpt":"","text":"nginx ssl이전 포스트 에서 nginx 를 사용하여 back-end server를 load balancing 하는 법을 알아보았다.이번 포스트에서는 nginx 에 ssl을 적용하여 https를 사용해보자. ssl 생성필자는 발급기관에서 발급한 인증서를 사용하지 않고 openssl로 개인이 생성한 인증서를 기반으로 nginx에 ssl을 적용할것이므로 인증서를 생성하자.1$ openssl req -new -newkey rsa:2048 -nodes -keyout &lt;개인키이름&gt;.key -out &lt;인증요청서이름&gt;.csr 필자는 다음과같이 진행하였다.123456789101112131415161718192021222324252627root@16626249b1d1:/etc/nginx/ssl# openssl req -new -newkey rsa:2048 -nodes -keyout jaehunpark-ssl.key -out jaehunpark-ssl.csrGenerating a 2048 bit RSA private key.....................+++.....................................................................+++writing new private key to &apos;jaehunpark-ssl.key&apos;-----You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter &apos;.&apos;, the field will be left blank.-----Country Name (2 letter code) [AU]:KRState or Province Name (full name) [Some-State]:SeoulLocality Name (eg, city) []:SeoulOrganization Name (eg, company) [Internet Widgits Pty Ltd]:jaehunparkOrganizational Unit Name (eg, section) []:jaehunparkCommon Name (e.g. server FQDN or YOUR name) []:jaehunparkEmail Address []:setyourmindpark@gmail.comPlease enter the following &apos;extra&apos; attributesto be sent with your certificate requestA challenge password []:0000An optional company name []:root@16626249b1d1:/etc/nginx/ssl# lsjaehunpark-ssl.csr jaehunpark-ssl.key 입력을 마치게되면 &lt;개인키이름&gt;.key 와 &lt;인증요청서이름&gt;.csr 파일이 생성된다( 개인키, 인증요청서 ) 이제 생성된 개인키와 인증요청서로 인증서를 만들어보자.1$ openssl x509 -req -days 365 -in &lt;인증요청서이름&gt;.csr -signkey &lt;개인키이름&gt;.key -out &lt;생성할인증서이름&gt;.crt 123456root@16626249b1d1:/etc/nginx/ssl# openssl x509 -req -days 365 -in jaehunpark-ssl.csr -signkey jaehunpark-ssl.key -out jaehunpark-ssl.crtSignature oksubject=/C=KR/ST=Seoul/L=Seoul/O=jaehunpark/OU=jaehunpark/CN=jaehunpark/emailAddress=setyourmindpark@gmail.comGetting Private keyroot@16626249b1d1:/etc/nginx/ssl# ls jaehunpark-ssl.crt jaehunpark-ssl.csr jaehunpark-ssl.key 해당 명령어를 수행하게되면 인증서( 인증서이름.crt )가 생성된다. 개인키에 걸린 비밀번호제거nginx에서 ssl 적용시 &lt;개인키이름&gt;.key와 &lt;인증서이름&gt;.crt 파일로 nginx가 ssl을 적용한다.하지만 개인키에 비밀번호가 걸려있을경우 nginx 재기동시 비밀번호를 요구하므로 비밀번호를 요구하지않도록 사전에 작업을 진행할수있다.12$ cp &lt;생성된개인키이름&gt;.key &lt;생성할개인키복사본이름&gt;.key.secure$ openssl rsa -in &lt;생성된개인키복사본이름&gt;.key.secure -out &lt;재생성할개인키이름&gt;.key 12345root@16626249b1d1:/etc/nginx/ssl# cp jaehunpark-ssl.key jaehunpark-ssl.key.secureroot@16626249b1d1:/etc/nginx/ssl# openssl rsa -in jaehunpark-ssl.key.secure -out jaehunpark-ssl.keywriting RSA keyroot@16626249b1d1:/etc/nginx/ssl# lsjaehunpark-ssl.crt jaehunpark-ssl.csr jaehunpark-ssl.key jaehunpark-ssl.key.secure 필자는 재생성할개인키이름을 기존의 생성된 개인키명과 동일하게주어 기존의 개인키이름을 덮어씌우게 하였다. (새로운 개인키이름으로만들면 파일이 하나더 늘어나기에..)필자같은경우는 최초에 생성한 개인키에 비밀번호가 걸려있지않으므로 간단하게 비밀번호를 없앤 개인키를 다시만들었지만 ( 사실 위의 명령어를 수행할필요가없었다 원래 비밀번호가 걸려있지않았으므로 )실제 비밀번호가 걸려있는 개인키로 작업시 [ writing RSA key ] 문구가 뜨기전에 비밀번호를 요구할것이다.이제 필요한 준비는 모두끝이났다. ssl 적용1$ vi /etc/nginx/conf.d/&lt;서비스명&gt;.conf 필자는 생성된 인증서를 기반으로 다음과같이 설정하였다.123456789101112131415161718192021222324252627# Load Balancingupstream target-server &#123; least_conn; server 10.10.200.3:4000 max_fails=3 fail_timeout=10s; server 10.10.200.4:4000 max_fails=3 fail_timeout=10s;&#125;server &#123; listen 443; server_name 10.10.200.2; charset utf-8; access_log /etc/nginx/log/access.log; error_log /etc/nginx/log/error.log; ssl on; #ssl사용 ssl_certificate /etc/nginx/ssl/jaehunpark-ssl.crt; #생성된 인증서경로 ssl_certificate_key /etc/nginx/ssl/jaehunpark-ssl.key; #생성된 개인키 location / &#123; proxy_redirect off; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_pass http://target-server; &#125;&#125; 1$ service nginx restart 참고필자는 nginx 최신버전 1.12.0 를 사용하고있다.nginx 에서 ssl 을 정상적으로 사용하기 위해서는 http_ssl_module 모듈이 설치되어 있어야한다.포함된 모듈 확인은1$ nginx -V 명령어로 설치된 모듈들을 확인할수있다.ssl이 잘 적용되었는지 확인하기위해 https://&lt;주소&gt;:&lt;포트&gt; 로 접속해보자.정상적으로 잘 동작하는것을 확인할수있다. 참고사이트인증서 개념nginx ssl 적용ssl 생성nginx 추가모듈 설치","categories":[{"name":"nginx","slug":"nginx","permalink":"https://setyourmindpark.github.io/categories/nginx/"}],"tags":[]},{"title":"nginx load balancing","slug":"nginx/nginx","date":"2017-05-03T07:55:41.000Z","updated":"2018-03-01T08:57:18.000Z","comments":true,"path":"2017/05/03/nginx/nginx/","link":"","permalink":"https://setyourmindpark.github.io/2017/05/03/nginx/nginx/","excerpt":"","text":"nginx와 apache필자는 최근 back-end 앞단에 load balancing 을사용하여 worker의 가중치 설정, backup 서버설정, connection 이 적은 worker로 연결 등 다양한 옵션으로 load balancing을 수행하는 nginx에 대해서 글을적어보려고한다.한가지 짚고 넘어가야될 부분은 load balancing을 위해 흔히 사용하는 apache와 nginx는 태생적으로 load balancing을 수행 하기위해 개발된 모듈이 아니라는 점이다.apache와 nginx 모두 본연의 목적은 dynamic web project(jsp,php 등)가 아닌 순수 정적파일(static file)을 load하는 웹서버(web server) 로 사용하기위해 개발되었다.본론으로 돌아가 nginx 를사용하여 nodejs 기반의 서버들을 어떻게 load balancing 하는지 살펴보자. nginx 설치필자는 역시 docker를 사용하여 nginx container를 구성하였으므로, nginx 이미지부터 만들어보자.12345678910111213141516171819FROM setyourmindpark/debian-utf8MAINTAINER jaehunpark \"setyourmindpark@gmail.com\"RUN apt-get update &amp;&amp; \\ apt-get install -y curl &amp;&amp; \\ apt-get install -y vimRUN echo 'deb http://nginx.org/packages/debian/ jessie nginx' &gt;&gt; /etc/apt/sources.list &amp;&amp; \\ echo 'deb-src http://nginx.org/packages/debian/ jessie nginx' &gt;&gt; /etc/apt/sources.list &amp;&amp; \\ curl http://nginx.org/keys/nginx_signing.key | apt-key add - &amp;&amp; \\ apt-get update &amp;&amp; \\ apt-get install -y nginxVOLUME /etc/nginxEXPOSE 80EXPOSE 443CMD [\"nginx\", \"-g\", \"daemon off;\"] 필자가 만든 nginx 이미지파일은 docker hub 에 올려놓았다.( 사용법을 기술해놓았으니 필요하신분은 pull 하셔서 사용하시면됩니다. ) 이미지를 만들었으니, 이제 nginx container를 생성하자.1$ docker run -d --name nginx -p 80:80 -p 443:443 -v &lt;host path&gt;:/etc/nginx setyourmindpark/debian-nginx:apple docker 를사용하지않는 분들은 debian 기준으로1$ vi /etc/apt/sources.list.d/nginx.list 최신 nginx 패키지 경로를 입력한다12deb http://nginx.org/packages/debian/ jessie nginxdeb-src http://nginx.org/packages/debian/ jessie nginx nginx 최신 패키지를 다운받으려면 public key가 필요하므로1$ curl http://nginx.org/keys/nginx_signing.key | apt-key add - public key를 추가.12$ apt-get update$ apt-get install nginx 설치가 완료되었다. 어떻게 사용하는가설치한 nginx를 어떻게 설정하는지 알아보자.123$ vi /etc/nginx/conf.d/default.confor$ vi /etc/nginx/conf.d/&lt;서비스명&gt;.conf default.conf 파일을 열어 해당내용을 수정하던지, 또는 새로운파일명.conf 를만들어 작성해도된다.필자는 다음과 같이 설정하였다.1234567891011121314151617181920212223# Load Balancingupstream target-server &#123; least_conn; server 10.10.200.3:4000 max_fails=3 fail_timeout=10s; server 10.10.200.4:4000 max_fails=3 fail_timeout=10s;&#125;server &#123; listen 80; server_name 10.10.200.2; charset utf-8; access_log /etc/nginx/log/access.log; error_log /etc/nginx/log/error.log; location / &#123; proxy_redirect off; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_pass http://target-server; &#125;&#125; 필자는 사전에 nodejs 서버를 2대를 기동하였으며 nginx default port 인 80 포트로 접근시 target-server redirect 하도록 설정하였다.주의할 사항은 log 경로 부분의 디렉토리가 존재해야한다.target server 에 대한 세부적인 설정(가중치, 백업서버, 폭주 등) 에대한 설명은 load balancing 설정 을 참고하길바란다. 마지막으로 nginx를 재시작한다.123$ service nginx restartor$ /etc/init.d/nginx restart restart 명령어가 정상적으로 수행되었다면 제대로 동작하는지 확인해보자.80 port로 listen하는 nginx 에 정상적으로 접근이 되는것을 확인할수있다.load balancing이 제대로 수행되는지 눈으로 직접 확인하기위해서 필자는 같은 nodejs server 의 swagger 페이지부분에 server-1와 server-2라는 문구를 추가해두었다.자세히보면 JUST FOR SAMPLE 문구 하단의 작은글씨로 API 문서 server-1 와 server-2를 확인할수있다.참고로 필자는least_conn ; 옵션을 걸어두어 가장 클라이언트 연결 갯수가 적은 서버로 전달하는 설정 을 해두었다. 견해nginx를 사용하여 nodejs 기반의 서버를 load balancing 하는 법을 알아보았다.load balancing을 사용하게되면 필자가 생각하는 이점은 다음과 같다. 보안에좋다. ( client들은 nginx host로 접근하므로 back-end의 실제 host와 port를 숨길수있어 보안에 좋다. ) 확장성이 용의하다. ( 서비스가 확장되어 back-end server가 추가될경우 nginx를 통해 관리될수있다 ) 유연하다. ( 각 back-end 서버마다 설정을 다르게하여 하드웨어 스펙에 따라 가중치를 포함한 다양한 옵션을 지원한다 ) 이런한 이점으로 반드시 multi server 를 운영하지 않더라도 단순 single server 를 사용함에 있어서도 앞단에 nginx를 사용하는것이 좋다고 생각된다. 참고사이트nginx 소개debian-8 nginxload balancing 설정apache vs nginx","categories":[{"name":"nginx","slug":"nginx","permalink":"https://setyourmindpark.github.io/categories/nginx/"}],"tags":[]},{"title":"Promise ReactiveX","slug":"nodejs/nodejs-2","date":"2017-04-29T08:02:55.000Z","updated":"2018-02-06T05:29:02.000Z","comments":true,"path":"2017/04/29/nodejs/nodejs-2/","link":"","permalink":"https://setyourmindpark.github.io/2017/04/29/nodejs/nodejs-2/","excerpt":"","text":"ReactiveX필자는 nodejs 서버를 개발할때 주로 Promise 패턴을 사용한다.비동기 패턴중 상당히 유명한 async.js 도 널리사용하는듯하나, bluebird Promise 에서 async.js에서 지원하는 모든 함수들을 커버가가능하고, 가장 마음에드는 부분은 chain 형식으로 문법을 제공하여 Promise 패턴이 async 보다 코드 가독성이 좋다고 판단되서다.bluebird official api다른방식의 비동기 패턴을 써보고싶은마음에 우연히 발견한 ReacticeX 에대해서 소개해보고자 한다. Promise와 ReactiveX우선 ReactiveX 를 줄여서 rx 또는 observer 패턴이라고 표현을 많이들한다.rx 는 Microsoft에서 개발되어 angular 2.0 에서 기본으로 탑재 되기도하였고 현재 많은 개발언어에서 third party library 로 반응형 프로그래밍(observable)을 사용할수있도록 자리를 잡은것 같다.Promise든, ReactiveX 를 학습하게되면 필자가 생각하는 이점은 다음과 같다. 안드로이드 진영에서 Promise 또는 ReactiveX(RxJava)로 개발할수있다. IOS 진영에서 Promise 또는 ReactiveX(RxSwift)로 개발할수있다. javascript 에서 promise.js 또는 rx.js 를 지원하므로 back-end(nodejs) 나front-end 에서 개발할수있다. 결론은 Promise 패턴이나 ReactiveX 를통한 반응형 프로그래밍을 습득하게되면, 모든 플랫폼에 패턴을 통일시킬수 있다.nodejs 로 개발한 서버개발자가 ios 코드를 보게되더라도 패턴의 통일화로 어느정도 로직은 알아볼수있단 뜻이다. (물론 개발언어는 직접습득해야겠죠)한가지 플랫폼에 종속받지않고 다양한 플랫폼을 넘나들면서 개발하는 개발자들은 각자가 추구하는 패턴을 사용하여서 통일시키면 알아보기도 쉽고 개발생산성도 좋아지지않을까 ?본론으로 돌아가 필자는 ReactiveX 반응형 프로그래밍을 사용하기위해 Promise로 작성된 nodejs 서버의 일부 코드를 ReactiveX 코드로 수정하여 사용한 느낌을 적어보도록하겠다. ReacticeX 설치와 사용법일단 nodejs에서는 npm을통해 한줄의 명령어로 아주 쉽게 설치가능하다.필자는 ReactiveX 를 rx 라고 지칭하도록하겠다.1$ npm install rx --save 필자는 nodejs 에서 query를 좀더 쉽게 수행할수있도록 (transaction 등) queryHelper라는 모듈을 만들어 사용중이다.이 Promise로 작성된 queryHelper 모듈을 rx 로 변경해보았다.먼저 기존에 개발된 queryHelper 모듈은 다음과 같다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566function getConnection()&#123; return new Promise((resolve, reject) =&gt; &#123; mysql.createPool(conf.db).getConnection((err, connection) =&gt; &#123; if(err)&#123;reject(err);return false;&#125; resolve(connection); &#125;); &#125;);&#125;;function doQuery(connection, resource)&#123; return new Promise((resolve, reject) =&gt; &#123; connection.query( (typeof(resource.query) === 'function') ? resource.query(resource.data) : resource.query , (err, data) =&gt;&#123; if(err)&#123;reject(err);return false;&#125; if((resource.expect||'many') === 'single')&#123; if(Array.isArray(data)) resolve(data[0]) else resolve(data) &#125;else&#123; if(data.length === 0) resolve(undefined) else resolve(data) &#125; &#125;); &#125;)&#125;function execute(resource)&#123; return new Promise((resolve, reject) =&gt; &#123; getConnection().then((connection) =&gt; &#123; return doQuery(connection, resource).then((data) =&gt; &#123; connection.release(); resolve(data); &#125;).catch((err) =&gt; &#123; connection.release(); reject(err) &#125;); &#125;).catch((err) =&gt; &#123; reject(err); &#125;); &#125;);&#125;function transaction(resources)&#123; return new Promise((resolve, reject) =&gt; &#123; getConnection().then((connection) =&gt; &#123; connection.beginTransaction(err =&gt; &#123;if(err)&#123;reject(err);return false;&#125;&#125;); return Promise.mapSeries(resources, (resource ,index ,length) =&gt; &#123; return doQuery(connection,resource); &#125;).then((data) =&gt;&#123; connection.commit((err)=&gt;&#123; connection.release(); if(err)&#123;reject(err);return false;&#125; resolve(data); &#125;) &#125;).catch((err) =&gt; &#123; connection.rollback(() =&gt;&#123; connection.release(); reject(err); &#125;) &#125;) &#125;).catch((err) =&gt; &#123; reject(err); &#125;); &#125;);&#125; 다음은 ReactiveX로 코드를 변경해보았다.12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576function doQuery(connection, resource)&#123; return new Promise((resolve, reject) =&gt; &#123; connection.query( (typeof(resource.query) === 'function') ? resource.query(resource.data) : resource.query , (err, data) =&gt;&#123; if(err)&#123;reject(err);return false;&#125; if((resource.expect||'many') === 'single')&#123; if(Array.isArray(data)) resolve(data[0]) else resolve(data) &#125;else&#123; if(data.length === 0) resolve(undefined) else resolve(data) &#125; &#125;); &#125;)&#125; function getConnection()&#123; return Rx.Observable.create(observer =&gt; &#123; mysql.createPool(mysqlConfig).getConnection((err, connection) =&gt; &#123; if(err)&#123;observer.onError(err); return false;&#125;; observer.onNext(connection); &#125;); &#125;); &#125;; function execute(resource)&#123; return Rx.Observable.create(observer =&gt; &#123; getConnection().subscribe( connection =&gt; &#123; return Rx.Observable.fromPromise(doQuery(connection, resource)).subscribe( data =&gt; &#123; observer.onNext(data)&#125;, err =&gt; &#123;observer.onError(err); &#125; ) &#125;, err =&gt; &#123; observer.onError(err); &#125; , () =&gt; &#123;&#125; ) &#125;); &#125;function transaction(resources)&#123; return Rx.Observable.create(observer =&gt; &#123; getConnection().subscribe( connection =&gt; &#123; return Rx.Observable.of(resources) .mergeMap(q =&gt; Rx.Observable.forkJoin(q.map(r =&gt; &#123; return doQuery(connection,r); &#125;))) .subscribe( data =&gt; &#123; connection.commit((err)=&gt;&#123; connection.release(); if(err)&#123;observer.onError(err);return false;&#125; observer.onNext(data); &#125;) &#125;, err =&gt; &#123; connection.rollback(() =&gt;&#123; connection.release(); observer.onError(err); &#125;) &#125; ) &#125;, err =&gt; &#123; observer.onError(err); &#125;, () =&gt; &#123; &#125; ) &#125;);&#125; 필자가 생소하기도하고, 어려웠던부분은 다음과같다.두개의 로직을 보면 rx로 작성된 queryHepler에서도 doQuery() 함수부분은 Promise로 작성된 로직과 같은걸 볼수있다.원래 rx의 doQuery()함수는12345678910111213141516function doQuery(connection, resource)&#123; return Rx.Observable.create(observer =&gt; &#123; connection.query( (typeof(resource.query) === 'function') ? resource.query(resource.data) : resource.query , (err, data) =&gt;&#123; if(err)&#123;observer.onError(err); return false;&#125; if((resource.expect||'many') === 'single')&#123; if(Array.isArray(data)) observer.onNext(data[0]) else observer.onNext(data); &#125;else&#123; if(data.length === 0) resolve(undefined) else observer.onNext(data); &#125; &#125;); &#125;);&#125; 이렇게 작성하였으나, 이렇게 작성할경우 execute() 함수는 제대로 실행이되나, transaction() 함수는 제대로 실행되지않았다. 이유는 즉슨 transaction()함수에서12345678function transaction(resources)&#123; ... return Rx.Observable.of(resources) .mergeMap(q =&gt; Rx.Observable.forkJoin(q.map(r =&gt; &#123; return doQuery(connection,r); &#125;))) ...&#125; Promise.all() 과 같은 역활을 하는 Rx.Obervable.forkjoin() 인자값으로는 스트림(stream)들의 위치하게된다.Promise.all 또는 Promise.map 와 Rx.Observable.forkjoin()의 다른점은, Rx.Observable.forkjoin()의 인자값에는 Rx.Observable.of(‘data1’),Rx.Observable.of(‘data2’) … 와같은 순수 스트림 데이터들이 위치하게되는 반면, Promise.all() 또는 Promise.map() 은 인자값으로 [ Promise.resolve(‘data1’), Promise.resolve(‘data2’) ] 와같이 Promise들이 위치되는것은 같으나, 좀더 나아가 각 Promise들 마다 로직을 수행할수있다는 점이다.아래의 코드를 보면 이해가될것이다.123456789101112131415161718192021222324252627282930313233// Promise.allPromise.all( [ Promise.resolve('fakeData1').then(data =&gt; 'realData1'), Promise.resolve('fakeData2').then(data =&gt; 'realData2'), ]) .then(result =&gt; &#123; console.log('Promise result =&gt; ' + result); &#125;).catch((err) =&gt; &#123; next(new Error(err)); &#125;)// Promise style을 rx style로 변경.// Promise style처럼 forkJoin()에서는 각각의 스트림에 세부적인 로직을 정의할수없음 .// 에러뜸Rx.Observable.forkJoin( Rx.Observable.of('fakeData1').subscibe(data =&gt; 'realData1'), Rx.Observable.of('fakeData2').subscibe(data =&gt; 'realData2')).subscribe( result =&gt; console.log(result), err =&gt; console.log(err));// Rx.Observable.forkJoin() 이렇게 사용해야함.Rx.Observable.forkJoin( Rx.Observable.of('realData1'), Rx.Observable.of('realData2')).subscribe( result =&gt; console.log('rx result =&gt; ' + result), err =&gt; console.log(err)); 예제에서 보시다시피 Rx.Observable.forkJoin() 함수는 인자값내에선 적당한 로직이있는 Rx.Observable을 사용할수없으며 로직수행후 처리할 로직이있다면, 로직수행후 callback(onNext()) 내부에서 또다른 Rx.Observable을 구현해야할것이다 .( callback hell 이 생각나지 왜 ..)rxjs forkjoin() 의 sample 코드도 promise를 사용하는것을 확인할수있다.그래서 필자는 실제 쿼리를 수행하는 doQuery()를 Promise로 기존의 코드를 그대로 두고, execute() 함수에서 doQuery()함수 호출시 Promise를 Rx.Observable 로 변환해주는 Rx.Observable.fromPromise() 함수를 사용하였다. 견해이렇게 기존에 Promise로 작성된 queryHelper모듈을 rx style로 변경을 해보았다.필자도 rx를 언젠간 써봐야지 하고 미뤄두다가 써보니 좋은점도 있고 그렇지않은점도 있는것 같다.다음은 극단적인예로, 로직을 처리하기위해 사전에 먼저 수행되어야할 로직이있다면 callback을 받아 새로운 로직을 구성하는 Promise와 rx로 작성해보았다. (의존관계의 로직처리)123456789101112131415161718192021222324252627282930313233// Promise versionPromise.resolve(&#123;'key1':'value1'&#125;).then((data)=&gt;&#123; data.key1 = 'newValue1'; //데이터가공 return Promise.resolve(Object.assign(data,&#123;'key2':'value2'&#125;)) //가공된 데이터가 새로운 Promise에 필요&#125;).then((data) =&gt; &#123; data.key2 = 'newValue1'; //데이터가공 return Promise.resolve(Object.assign(data,&#123;'key3':'value3'&#125;)) //가공된 데이터가 새로운 Promise에 필요&#125;).then((data) =&gt; &#123; console.log(data);&#125;).catch((err) =&gt; &#123; next(new Error(err));&#125;);// rx versionRx.Observable.of(&#123;'key1':'value1'&#125;).subscribe(data =&gt; &#123; data.key1 = 'newValue1'; //데이터가공 Rx.Observable.of(Object.assign(data,&#123;'key2':'value2'&#125;)).subscribe(data =&gt; &#123; //가공된 데이터가 새로운 Rx.Observable에 필요 data.key2 = 'newValue2'; //데이터가공 Rx.Observable.of(Object.assign(data,&#123;'key3':'value3'&#125;)).subscribe(data =&gt; &#123; //가공된 데이터가 새로운 Rx.Observable에 필요 console.log(data); &#125;, err =&gt; new Error(err), () =&gt; &#123;&#125; ) &#125;, err =&gt; new Error(err), () =&gt; &#123;&#125; )&#125;,err =&gt; new Error(err),() =&gt; &#123;&#125;); Promise는 chain 형식으로 문법을 사용할수있어 callback hell을 피할수있으며, 한눈에 보기에도 rx보다 가독성이 좋다반면 rx 의 경우에는 불가피하게 중첩 callback이 생겨 Promise보다는 가독성이 떨어진다.대신 rx 의 장점은 우선 제공되는 함수가 엄청나게 많아 제대로 사용하게되면 불필요한 로직을 대폭 줄일수있을 것 같고, 그에따른 학습의 진입장벽이 조금 있을것 같다.필자는 learn rxjs 를 참고하였다.간단하게나마 rx 를 사용해보았지만, 반응형 프로그래밍이 요즘 뜨고있으니 무작정 사용해야지(필자는 이런생각을 했다.) 라는 생각보다는 상황에 맞게 Promise와 rx를 적절하게 같이 사용하는것이 가장 좋은 방법이라 생각이든다.rx에 대해서 의견이 있으시면 언제든지 댓글을 남겨주시면 저도 열심히 배우도록하겠습니다 ….","categories":[{"name":"nodejs","slug":"nodejs","permalink":"https://setyourmindpark.github.io/categories/nodejs/"}],"tags":[]},{"title":"jenkins로 배포하기 - nodejs-2","slug":"jenkins/jenkins-3","date":"2017-04-23T04:35:25.000Z","updated":"2018-02-23T10:35:43.000Z","comments":true,"path":"2017/04/23/jenkins/jenkins-3/","link":"","permalink":"https://setyourmindpark.github.io/2017/04/23/jenkins/jenkins-3/","excerpt":"","text":"jenkins로 nodejs 서버 배포하기 - 2이전 포스트 jenkins로 nodejs 서버 배포하기 - 1 를 통해 nodejs 서버를 remote 서버로 쉘스크립트 EOF를 사용하여 remote 서버에서 git clone 하여 배포하는 법을 알아보았다.이번 포스트에서는 두번째방법으로 첫번째 방법보다는 좀더 간단한 방법을 소개하려한다. 쉘 스크립트 EOF 와 SCP기본적으로 쉘스크립트 EOF를 사용하여 jenkins가 remote 서버에서 control하는 기본 개념은 같다.이후 첫번째방법에서는 jenkins가 remote 서버 접속후 프로젝트를 다시 clone 하여 job을 수행하는 방법이었으며, 이번에 소개해드릴방법은 jenkins가 test를 수행한 프로젝트 자체를 scp 명령어를 사용해서 remote 서버로 프로젝트를 전송하는 방법이다 .jenkins [ Managed script file ] 플러그인을 다음과 같이 설정하였다.jenkins관리 -&gt; Managed files -&gt; Add a new Config -&gt; Managed script file1234567891011121314151617181920npm installnpm testrm -rf node_modules-#!/bin/shssh root@remote서버ip &lt;&lt;EOF pm2 delete apiServer rm -rf /root/nodejs-skeletone-v2 exitEOFscp -r ../nodejs-skeletone-v2 root@remote서버ip:/root-#!/bin/shssh root@remote서버ip &lt;&lt;EOF cd /root/nodejs-skeletone-v2 npm install --production pm2 start bin/www.js --name apiServer exitEOF git push event hook을 받은 jenkins가 test를 수행할것이므로 패키지를 설치한다 mocha test framework 를 사용하여 프로젝트 test를 진행한다. scp 로 remote 서버로 프로젝트를 전송할 준비작업으로, test를 마친 jenkins가 프로젝트 패키지를 모두 삭제한다. (remote 서버에서는 서비스에 필요한 패키지만 필요하므로) EOF 를 사용하여 remote 서버로 접속후 이미 실행되고있는 nodejs 서버를 중지 한다 . 서비스되고있는 프로젝트를 삭제한다 jenkins에서 방금 test를 수행한 프로젝트를 remote 서버로 프로젝트 전송을 시작한다. EOF로 remote 서버로 재접속 한후, 서비스에 필요한 패키지만 받는다(–production) pm2를 사용하여 서비스를 시작한다. Managed script file 작성이 끝나면 마지막으로 jenkins에 설정정보를 추가한다.[ Build - managed script] 왜 SCP를 사용하는가jenkins로 nodejs 서버 배포하기 - 1 포스트에서는 쉘스크립트 EOF를 사용하여 remote 서버에 jenkins가 접속하여 git clone명령어를 수행하고 프로젝트 서비스하는 방법이었다.그렇다면 결론적으로 remote 서버에서는 git clone 명령어로 프로젝트를 받든, jenkins가 scp를 사용하여 프로젝트를 보내주든, 결국 서비스할 프로젝트는 동일하다 .하지만 필자가 생각하기엔 첫번째 방법에서는 시간차로인한 문제점이 있었다. 다음과 같은 시나리오를 생각해보자. 개발자가 git push를 수행하여 jenkins가 git push event hook을 받았다. jenkins가 test 를 수행한다. jenkins가 test를 수행하는도중 또 다른 개발자가 git push 를 수행하였다. test를 마친 jenkins가 remote 서버에 접속한다. jenkins에서 test 를 거치지않은 방금 또다른 개발자가 push 한 git project를 clone한다. 결론적으로 jenkins에서 수행한 test는 수행할 필요가 없어지게되었다.(시간차로 인한 프로젝트 불일치) 물론 또다른 개발자가 push 한 프로젝트는 jenkins에서 push event hook을 받아 배포를 또다시 수행할테지만, 짧은 시간이나마 remote 서버에 배포된 프로젝트가 오류가 있었다면 서버가 죽게될것이다.(test 과정을 거치지않았기때문)이러한 이유로 필자는 scp를 사용하여 프로젝트를 전송하는 방법을 생각하였다. scp를 사용하면 jenkins에서 test를 수행한 프로젝트를 remote 서버에 프로젝트를 보내기때문에 프로젝트 일관성이 보장되어 위와같은 시나리오는 막을수있다. 또다른 이점jenkins로 nodejs 서버 배포하기 - 1 에서는 remote 서버에서 git clone을 받기에 github에 remote 서버 rsa public key를 등록하였다. 하지만 jenkins에서 scp를 사용하여 프로젝트를 전송하면 이런 과정이 필요없어지게되어 remote 서버에서 rsa 키를 생성할 필요도 없어지게되었고 github에 remote 서버 public key를 등록할 필요도 없어지게되었다. 견해jenkins로 nodejs 서버 배포하기 - 1 와 같이 jenkins가 test 를 수행후 remote 서버에 배포를하는 결과는 동일하다.첫번째방법 remote 서버에서 git clone하여 수행하는 방법은 여러개발자로 인해 빈번하게 push event가 일어나면 위에서 언급한 시간차로인한 프로젝트 불일치 현상이 생길수있는 문제점이므로, 정해진시간(주로 새벽에)에 jenkins가 remote 서버로 배포를 수행한다면 문제가되지않을것이다.(Poll SCM schedule)이렇게 nodejs 서버를 remote 서버로 배포하는 2가지 방법에대해서 알아보았다.또 다른 방법이 분명 있을것이고, 좀더 좋은 방법을 새롭게 알게된다면 새롭게 포스트를 올리도록 하겠다.","categories":[{"name":"jenkins","slug":"jenkins","permalink":"https://setyourmindpark.github.io/categories/jenkins/"}],"tags":[]},{"title":"jenkins로 배포하기 - nodejs-1","slug":"jenkins/jenkins-2","date":"2017-04-22T06:03:53.000Z","updated":"2018-02-23T10:34:41.000Z","comments":true,"path":"2017/04/22/jenkins/jenkins-2/","link":"","permalink":"https://setyourmindpark.github.io/2017/04/22/jenkins/jenkins-2/","excerpt":"","text":"jenkins로 nodejs 서버 배포하기 - 1이전에 포스트한 jenkins로 배포하기 - java 를 통해 java기반의 서버를 jenkins를 활용하여 remote 서버로 배포하는 법을 알아보았다.이번에는 jenkins를 활용해서 nodejs 서버를 remote 서버로 어떻게 배포하는지 알아보자. 무엇이다른가jenkins로 배포하기 - java java 기반의 서버를 배포하는 방법을 살펴보면, git push hook event를 받아 jenkins가 maven build후 정의된 tomcat 인증정보로 war를 redeploy하는 방식이었다.그럼 nodejs 기반의 서버는 tomcat과 같은 was가 존재하지않으므로 어떻게 remote 서버로 배포를할까 ?필자는 nodejs기반의 서버를 jenkins를 활용하여 remote server에 ssh 접속후 job을 수행하는 방식을 사용하였다.기본적인 remote 서버에 배포하는 일은 jenkins가 수행하므로 개념적으로는 다르지않으나, 어떻게 배포하는가에 대한 방법이 조금 다르며, 필자가 시도한 배포 방법에는 2가지가 있으므로 그중 첫번째 방법에대해서 글을 써보려 한다. 쉘 스크립트 EOF 와 git clonejenkins 프로젝트 기본 설정은 앞서 포스트한 jenkins로 배포하기 - java 와 다르지않으니 참고 하도록하며, [ Build ] 설정 부터 수행해야할 job 이 다르니 사전에 jenkins와 git 연동을 마친상태에서 추가적인 설정정보를 진행한다. 사전준비jenkins rsa public key(id_rsa.pub)remote server rsa private key(id_rsa) 와 publick key(id_rsa.pub)jenkins Managed Script plugin 먼저 jenkins에서 remote 서버를 known_hosts 등록한다.1$ ssh-keyscan -H remote서버ip &gt;&gt; ~/.ssh/known_hosts 다음으로 remote 서버에 접속후 rsa 키를 생성한다.1$ ssh-keygen -t rsa 12$ cat ~/.ssh/id_rsa$ cat ~/.ssh/id_rsa.pub private key(id_rsa)와 public key(id_rsa.pub) 키가 생성되었을것이다.remote 서버도 jenkins 로부터 git clone 명령어를 입력받을 예정이기에, github에 public key를 등록한다.github -&gt; Settings -&gt; SSH and GPG Keysrsa_id.pub 값 등록후 Add SSH Keyjenkins에서 현재접속중인 remote 서버에 인증된 client로 접속할것이기에 jenkins container에 사전에 생성된 public key(id_rsa.pub) 키를 remote 서버에 등록한다.1$ vi ~/.ssh/authorized_keys jenkins 컨테이너에서 생성된 public key(id_rsa.pub) 내용를 입력한다.다음으로 github을 known_hosts로 등록을 진행한다.1$ ssh-keyscan -H github.com &gt;&gt; ~/.ssh/known_hosts jenkins에서 이러한 설정을 바탕으로 쉘스크립트 EOF로 remote 서버를 control 할것이기때문에 Managed Script plugin을 설치한다jenkins관리 -&gt; 플러그인관리[ Managed Script plugin ]jenkins관리 -&gt; Managed files -&gt; Add a new Config -&gt; Managed script file1234567891011-#!/bin/shssh root@remote서버ip &lt;&lt;EOF pm2 delete apiServer cd /root rm -rf nodejs-skeletone-v2 git clone git@github.com:setyourmindpark/nodejs-skeletone-v2.git cd nodejs-skeletone-v2 npm install --production pm2 start bin/www.js --name apiServer exitEOF jenkins가 remote 서버 접속후 기존에 돌고있는 nodejs 서버를 종료한다.(필자는 pm2 를 사용한다) /root 경로이동 기존의 서비스중인 nodejs 서버를 삭제한다 git clone을 다시받는다. clone받은 디렉토리로 이동한다 remote 서버는 jenkins에서 진행한 test를 다시 진행할 필요가없으므로 –production 옵션으로 node 패키지를 설치한다 nodejs 서버를 시작한다 . 이제 마지막으로 jenkins 설정을한다. 필자는 다음과같이 설정하였다.git push event hook을 받은 jenkins는 node 패키지 모듈을 설치한후.이전포스트 mocha supertest should 테스트를 진행할것이므로 Execute shell에 정의하고 Execute managed script에는 방금 Managed script 에 등록된 쉘 스크립트를 선택하였다.[ Build ]이렇게 모든 설정이 끝이났다. 이제 배포를 시작해보자. 추가설명jenkins의 rsa public key를 remote서버의 authorized_keys에 추가하는 과정은, 일반적으로 ssh 접속시 접속 서버의 계정 비밀번호를 반드시 입력해야 하며 root 계정으로 접속시에는 추가적으로 root계정 ssh 접속을 위한 설정이 필요하다.하지만 방금진행한 rsa 암호화 통신은 rsa private key와 public key를 통해 인증된 ssh client를 등록하는 과정이기에 비밀번호를 입력할 필요가없어지게된다. 다음으로 jenkins에서는 remote서버를, remote서버에서는 github을 known_hosts에 등록하는 과정은, ssh 를기반으로하는 최초 git clone이나, ssh 접속시 접속하는 호스트에대한 기록이 없으므로 접속여부에대한 메시지가 발생한다.만약 사전에 known_hosts에 등록하지않을시에는다음과 같은 메시지를 보게될것이다. jenkins가 배포를 수행하는도중 이런 메시지를 보게된다면 곤란하므로, 사전에 등록해주는 과정이다. 배포git commit 그리고 pushjenkins 가 다음과같이 수행한다npm test 수행pm2 delete apiServergit clonenpm install –productionpm2 start bin/www.js –name apiServer 참고필자는 jenkins에서 mocha framework를 사용해 npm test 를 진행하였다.nodejs 서버를 배포하는 서버도 docker container로 구성이되어있고, 배포서버의 db 접속정보는 시스템 환경변수를 참조하여 정보를 get 하도록 설정하였다.하지만 jenkins는 배포서버가 서비스하기위해 사용하는 db 접속정보를 알지못하므로, npm test 를수행할수없게된다.필자는 그래서 jenkins container에 배포서버가 접속하는 db 접속정보의 환경변수를 공유하도록하였다.docker-compose.yml의 설정은 다음과같다1234567891011121314151617181920212223242526jenkins: image: jenkins ports: - &quot;89:8080&quot; environment: - TZ=Asia/Seoul env_file: - ./server-node/env/db.env volumes: - /home/jaehunpark/docker/service/jenkins/jenkins-data:/root container_name: jenkinsserver-node: image: server-node ports: - &quot;443:4000&quot; environment: - TZ=Asia/Seoul env_file: - ./server-node/env/db.env - ./server-node/env/service.env volumes: - /home/jaehunpark/docker/service/server-node/server-node-data:/root tty: true depends_on: - mariadb container_name: server-node server-node의 service가 env_file을 두가지 참조한다.한가지는 db에대한 환경변수들이 정의되어있는 파일이고 db.env,다른한가지는 서비스에 수행될 환경변수들이있는 파일이다 service.env .따라서 jenkins에서 npm test를 정상적으로 수행하기위해 server-node 의 db접속정보를 참조하는것을 볼수있다.jenkins를 통해 배포할 프로젝트가 다수이고, 각각의 프로젝트에서 수행할 test가 있을시 다음과같이 프로젝트가 사용하는 db 접속정보를 따로빼어서 jenkins env_file에 하나씩 추가해주면 된다 . 견해nodejs 서버를 jenkins를 통해 배포하는 첫번째방법에대해서 알아보았다.설정자체가 java 기반보다는 손이 많이가는 편이가 가끔 헷갈리기도하지만, 정리하면서 기록으로 남길수있어 이렇게 글로 남겨본다.아직 배워야할것도 많고, 모르는것도 아주 많은 개발자지만 하나하나 배우다보면 언젠가 정상에 오를것이라 믿고있다.이글을 보고 조금이라도 도움이되었으면 좋겠다.다음에는 jenkins를 사용해 nodejs서버를 배포하는 다른방법을 알아보자 .","categories":[{"name":"jenkins","slug":"jenkins","permalink":"https://setyourmindpark.github.io/categories/jenkins/"}],"tags":[]},{"title":"mocha supertest should","slug":"nodejs/nodejs-1","date":"2017-04-19T07:27:27.000Z","updated":"2018-02-23T09:48:47.000Z","comments":true,"path":"2017/04/19/nodejs/nodejs-1/","link":"","permalink":"https://setyourmindpark.github.io/2017/04/19/nodejs/nodejs-1/","excerpt":"","text":"mocha필자가 현재회사에서 하는일은 java 기반의 SI 업무를 담당하고있다.SI 업무를 담당하니 test라는 개념은 별도의 test case를 따로 작성하는것이아니라,요구사항 개발을 진행후 제대로 잘동작하는지에대한 행위를 하고 땡 하는 식이다.mock을 이용한 test case 작성이나, junit을 사용한 test case를 작성하는것은 누군가 강제로 시켜서 하지않는이상, 주도적으로 스스로 하지않을것이다. 요구사항을 빠르게 반영해야하므로 test case 작성은 시간낭비일뿐이니까.필자는 node기반의 서버에도 관심이 많다. nodejs를 활용하여 java의 junit, mock 과 같은 역활을 수행하는 mocha에 대해서 적어보려고한다. 설치test framework mocha는 npm을 통해 아주 쉽게 설치할수있다.1$ npm install mocha -g -g 옵션을 붙이게되면 개발 머신에 global로 설치를 한다.필자는 global 설치보다는 해당 프로젝트내에서 필요할때 mocha를 사용할것이기에1$ npm install mocha --save-dev 프로젝트내의 devDependencies를 추가하였다. mocha와 흔히들 같이사용하는 패키지로는 api 검증의 supertest와api 호출에대한 response 검증을 수행하는 should를 사용하는것이 일반적인것 같다.따라서 필자도해당 패키지들을 mocha와 함께 사용하였다.12$ npm install supertest --save-dev$ npm install should --save-dev 모든 패키지를 설치하였다면 package.json은 다음과같을것이다12345\"devDependencies\": &#123; \"mocha\": \"^3.1.2\", \"should\": \"^11.1.1\", \"supertest\": \"^2.0.1\"&#125; 이제 테스트를위한 준비는 끝이났다. 어떻게 사용하는가전체적인 test구조는 mocha, 각 test case의 api 호출은 supertest가,그리고 api response에대한 검증은 should가 담당한다.mocha docs 의 test case에 대한 pre,post 제공되는 함수는 다음과 같다1234567891011121314151617181920describe('hooks', function() &#123; before(function() &#123; // runs before all tests in this block &#125;); after(function() &#123; // runs after all tests in this block &#125;); beforeEach(function() &#123; // runs before each test in this block &#125;); afterEach(function() &#123; // runs after each test in this block &#125;); // test cases&#125;); 간단하게 살펴보면 before() 모든 테스트들이 수행되기전 단 한번만 실행된다 after() 모든 테스트들이 수행된후 단 한번만 실행된다 beforeEach() 각 테스트들이 수행되기전 실행된다 afterEach() 각 테스트들이 수행된후 실행된다 필자는 mocha를 다음과 같이 사용하였다.123456789101112131415161718192021222324252627282930 //테스트들이 수행되기전 db 연결상태를 체크한다 before(done =&gt; &#123; queryHelper.execute(&#123;query:'SELECT NOW() AS now FROM DUAL', expect : 'single'&#125;).then(result =&gt; &#123; console.log(result.now); done() &#125;).catch(err =&gt; &#123; throw err; &#125;) &#125;); describe('SAMPLE_V2', () =&gt; &#123; //test case를 작성한다. describe('GET /api/smpl/v2/select1/param1', () =&gt; &#123; it('should return sample api', done =&gt; &#123; request(app) .get('/api/smpl/v2/select1/value1') .set('token', token) // header setting .expect(200) .expect(\"Content-Type\", /json/) .end((err, res) =&gt; &#123; if (err) throw err; res.body.should.be.instanceOf(Object).and.have.property('isSuccess').be.equal(true); res.body.should.be.instanceOf(Object).and.have.property('data'); done(); &#125;); &#125;); &#125;); ...&#125;); test case를 수행하기전 연결된 db 상태를 체크한후, test case를 진행하고있다.request(supertest) 를 통하여 api 수행과 응답에 대한 정의와 예상되는 response status와 어떤 data를 request 할지를 정의한다.필자는 token 이라는 이름으로 jwt token을 header에 실어서보내며,post와 같은 body 값은 send({param1 : valu1 ..}) 와 같이 사용할수있다.response에대한 응답으로는 should를 사용하여 response 값들을 검증한다.어떤 property가 있는지, 해당 propery의 값은 무엇인지에대한 검증을 수행한다.이제 작성된 test case를 어떻게 실행하는지 알아보자프로젝트 package.json 상단의 scripts에 test를 정의한다.1234\"scripts\": &#123; \"start\": \"node ./bin/www\", \"test\": \"node_modules/.bin/mocha app/api/**/*.spec.js\"&#125;, 프로젝트내에서 mocha를 실행할것이기에, npm 으로 부터 설치된 mocha 실행 경로를 정의후 경로를 설정한다필자는 app/api/ 경로 내의 모든 spec.js 확장자를 mocha를 이용한 test를 수행할것이므로, 다음과 같이 정의하였다.다음으로 test를 수행한다1$ npm test mocha를 기반으로 supertest와 should를 사용하여 test case를 작성해보았다.필자가 사용해본느낌으로는, error를 뱉을시 error가 왜 발생하였는지에대한 상세한 메시지는 console에 보여주지않는다는점이다.고의로 에러를 발생해보았다.예상되는 수행 결과값에만 근거하여 error를 표출한다.test case를 수행하다 에러가 왜 발생했는지에 대한 메시지를 뿌려주면 좋을것 같은 생각이든다. 견해많이들 test에대한 방법론으로 TDD와 BDD에 대해서 이야기들한다.필자는 내공이 부족하여 정확하게 알지는 못하지만 구글링을 통해 얻게된 정보로는 TDD는 테스트주도 개발방법이고, BDD는 스토리 또는 시나리오기반 테스트 개발방법이라는 글들을 보았다.현재 이글에서 제공하는 방법은 sample api에 대한 테스트를 진행하였으므로 딱히 개발요구사항에대한 스토리는 없으므로 TDD가 적합한듯 하다.하지만 실제 개발시에는 프로세스 스토리와 시나리오 정의후 test case를 작성하여 BDD 개발 방법론을 따르는것이 좋을것 같다.해당 api가 왜 존재해야하는지는 스토리 또는 시나리오를 보며 이해를 높일수있을것 같아서다.쓰지않는 api를 개발을 하진않겠지만 존재하는 이유에 대해서는 BDD를 개발방법을통해 명확히 이해할수있을것 같다.마지막으로 가장 좋은점은 test case를 작성하게되니 따로 api document를 작성하지 않아도되는 느낌이 들기도한다. (물론 작성하면 더욱더 좋겠죠)test case에 정의된 api를 호출과 예상결과값만봐도 해당 api가 대충은 어떤역활을 수행하는 api인지 보여주기도한다.필자는 test case에 익숙하지않지만 분명 좋은 개발 방법론임에는 틀림없는것 같다 . TDD와 BDDhttp://hnsnmn.blogspot.kr/2014/02/tdd-bdd.htmlhttp://asfirstalways.tistory.com/296http://blog.aliencube.org/ko/2014/04/02/differences-between-bdd-and-tdd/","categories":[{"name":"nodejs","slug":"nodejs","permalink":"https://setyourmindpark.github.io/categories/nodejs/"}],"tags":[]},{"title":"docker hub","slug":"docker/docker-3","date":"2017-04-16T07:28:26.000Z","updated":"2018-02-23T10:32:22.000Z","comments":true,"path":"2017/04/16/docker/docker-3/","link":"","permalink":"https://setyourmindpark.github.io/2017/04/16/docker/docker-3/","excerpt":"","text":"docker hub필자는 요즘 docker 에 빠져있다.docker hub에 굉장히 많은 images들을 보면서 신기하기도하고, 직접 모든것을 구현하기보다는 오픈소스를 적극활용하여 개발이 이루어지는 시대가 정착된것 같기도하다.모든 사람의 성격이 다른것처럼. 개개인의 개발자의 성향이 다르지만, 필자의 성격은 다른 개발자가 구현해놓은 것을 가져다 쓰기보다는 직접 구현을 해야 적성이 풀리는 그런 성격을 지녔다.( 고쳐야되는데 .. 그냥 아무생각없이 가져다쓰면 속편할텐데 …)그래서 docker hub에 이미지를 올려보았다. command 중복이전 포스트에서 jenkins Dockefile 을 보자.상단부터 system 언어셋 설정부터, jdk , 그리고 하단의 maven 과 같은 설치를 진행하는 command를 볼수있다.docker를 사용하는데에 있어서 jenkins 만 사용하면 사실 아무런 문제가되지않는다. 그냥 원하는대로 시스템에 설치할 command를 기술하면되니까.그러나 서비스를 하는데에있어서 분명 jenkins 만 존재하지않을것이다. jenkins가 배포를 하기위한 java server가 있을것이고, nodejs를 빌드할테면 node server가 별도로 존재해야할것이다. 그밖에도 필요한 이미지가 다수 존재할수있을것이다.다시말해서 java server를 도커를 통해 만들게된다면, jenkins Dockerfile에 기술된 command를 일부 또는 버전만 바뀐 채 다시 기술해야할것이다. 이미지 분리필자는 가장 기본 os를 debian jessie 버전을 기본적인 os를 사용한다.docker를 처음 접하였을땐 ubuntu를 사용하였지만, 이상하게 docker hub에서 debian 계열의 이미지가 많아 찾아보니, ubuntu 보다 가볍고, ubuntu와 같은 계열이기에 debian을 많이 사용한다고한다.기본os를 base 이미지를 사용하여 시스템 언어셋 설정이 utf8인 docker image를 만들었다1234567891011121314FROM debian:jessieMAINTAINER jaehunpark \"setyourmindpark@gmail.com\"RUN apt-get update &amp;&amp; DEBIAN_FRONTEND=noninteractive apt-get install -y locales &amp;&amp; \\ sed -i -e 's/# en_US.UTF-8 UTF-8/en_US.UTF-8 UTF-8/' /etc/locale.gen &amp;&amp; \\ echo 'LANG=\"en_US.UTF-8\"'&gt;/etc/default/locale &amp;&amp; \\ dpkg-reconfigure --frontend=noninteractive locales &amp;&amp; \\ update-locale LANG=en_US.UTF-8ENV LANG en_US.UTF-8ENV LANGUAGE en_US:enENV LC_ALL en_US.UTF-8CMD [\"/bin/bash\"] 다음으로 이렇게만든 debian-utf8 이미지를 base로 만든 debian-oracle-jdk:8 이미지를 만들었다.1234567891011121314151617181920212223242526FROM setyourmindpark/debian-utf8MAINTAINER jaehunpark \"setyourmindpark@gmail.com\"RUN \\ echo \"===&gt; add webupd8 repository...\" &amp;&amp; \\ echo \"deb http://ppa.launchpad.net/webupd8team/java/ubuntu trusty main\" | tee /etc/apt/sources.list.d/webupd8team-java.list &amp;&amp; \\ echo \"deb-src http://ppa.launchpad.net/webupd8team/java/ubuntu trusty main\" | tee -a /etc/apt/sources.list.d/webupd8team-java.list &amp;&amp; \\ apt-key adv --keyserver keyserver.ubuntu.com --recv-keys EEA14886 &amp;&amp; \\ apt-get updateRUN echo \"===&gt; install Java\" &amp;&amp; \\ echo debconf shared/accepted-oracle-license-v1-1 select true | debconf-set-selections &amp;&amp; \\ echo debconf shared/accepted-oracle-license-v1-1 seen true | debconf-set-selections &amp;&amp; \\ DEBIAN_FRONTEND=noninteractive apt-get install -y --force-yes oracle-java8-installer oracle-java8-set-defaultRUN echo \"===&gt; clean up...\" &amp;&amp; \\ rm -rf /var/cache/oracle-jdk8-installer &amp;&amp; \\ apt-get clean &amp;&amp; \\ rm -rf /var/lib/apt/lists/*ENV JAVA_HOME=/usr/lib/jvm/java-8-oracleENV PATH=$&#123;PATH&#125;:$&#123;JAVA_HOME&#125;/binCMD [\"/bin/bash\"] 이렇게 필요한 부분만 분리하여 base 이미지로 참조하여 추가적으로 필요한 이미지를 지속적으로 만들면된다. 사실 다른 docker hub images들도 모두 이렇게 만들어져있다.다음으로 다시한번 이렇게 생성된 debian-oracle-jdk:8 이미지를 base로 한 debian-maven:apple 이미지를 만들었다1234567891011121314FROM setyourmindpark/debian-oracle-jdk:8MAINTAINER jaehunpark \"setyourmindpark@gmail.com\"ENV MAVEN_VERSION=3.5.0RUN wget --quiet --no-cookies http://apache.tt.co.kr/maven/maven-3/$&#123;MAVEN_VERSION&#125;/binaries/apache-maven-$&#123;MAVEN_VERSION&#125;-bin.tar.gz -O /tmp/maven.tar.gz &amp;&amp; \\ tar xzvf /tmp/maven.tar.gz -C /opt &amp;&amp; \\ mv /opt/apache-maven-$&#123;MAVEN_VERSION&#125; /opt/maven &amp;&amp; \\ rm /tmp/maven.tar.gzENV M3_HOME /opt/mavenENV PATH=$&#123;PATH&#125;:$&#123;M3_HOME&#125;/binCMD [\"/bin/bash\"] 여기서 고민을 많이하였다. 버전명을 apple이라고 준것은, apple이란 버전은 jdk8 과 maven 3.5.0 버전을 사용하기에 버전명을 apple이라고 준것이다.현재 apple 버전만 docker hub에 올라가있는 상태이며, jdk 와 maven의 서로다른 조합의 요구사항이 생길시 새로운 버전의 debian-maven:&lt;새로운버전명&gt; 이미지를 올리도록하겠다. ( 예를들어 jdk7 버전에 maven 3.5.0 버전의 조합일시 사전에 debian-oracle-jdk:7 이미지는 이미 생성되어있겠죠 .. )필자가 docker hub에 올린 이미지들은docker hub setyourmindpark에서 확인할수있다 … 견해‘docker hub에 이미지를 올려야겠다’ 라고 생각하였을땐 jenkins와 node server, java server 와같은 이미지를 올려야겠다고 생각했다. 하지만 이런 이미지들 보다는, 각자가 세팅하고자하는 jdk, maven , node 의 버전이 모두가 다를것이니. 시스템 전역에서 설치되는 설치파일들의 조합을 docker hub를 통해 제공하면 좋을것같아 이미지를 올리게되었다. 이러한 이미지를 base로 하여 각자가 추가적으로 필요한 command를 기술하여 이미지를 생성하면 command들을 조금이나마 줄일수있을것이다.필자가 제공하는 docker hub 이미지들외에 전세계 개발자들이 만들어놓은 이미지들이 무수히 많으니, base이미로 적극활용하는것이 개발 생산성 향상에 도움이될것이다 .","categories":[{"name":"docker","slug":"docker","permalink":"https://setyourmindpark.github.io/categories/docker/"}],"tags":[]},{"title":"jenkins로 배포하기 - java","slug":"jenkins/jenkins-1","date":"2017-04-14T11:24:30.000Z","updated":"2018-02-23T10:33:34.000Z","comments":true,"path":"2017/04/14/jenkins/jenkins-1/","link":"","permalink":"https://setyourmindpark.github.io/2017/04/14/jenkins/jenkins-1/","excerpt":"","text":"jenkins로 배포하기maven으로 remote서버 배포하기 를통해 로컬 개발머신에서 remote서버로 배포하는 법을 알아보았다.jenkins 가 준비되었다면, jenkins를 통하여 remote서버로 어떻게 배포하는지 알아보자. 플러그인설치필자는 빌드결과를 메일로 받고, remote 서버로 배포할것이기에 필요한 2개의 플러그인을 설치해보자.먼저 플러그인을 설치하기전에, 메일발송을위한 smtp 서버와 계정이필요하다.google 계정이있다면 아주손쉽게 해당계정을 smtp 서버에서 메일을 보내는 발신자계정으로 사용할수있다.smtp 설정보안수준설정 해제메일을 보내기위한 smtp 설정을 마쳤다면, 이제 jenkins에 플러그인을 설치해보자.jenkins관리 -&gt; 플러그인관리[ Email Ext Recipients Column Plugin ][ Deploy to container Plugin ]플러그인을 모두 설치하였다면 jenkins에 메일발송 smtp 설정을 하자.jenkins관리 -&gt; 시스템설정 -&gt; 메일로알려줌설정정보를 작성후 [ Test configuration ]테스트메일이 정상적으로 발송된것을 확인할수있다. git과 jenkins 연동jenkins가 git project를 clone 하기위해선 사전에 git과 인증을 마쳐야한다.즉 jenkins가 필요시에는 언제든지 git project를 clone 할수있는 상태를 마쳐야한다.jenkins와 github과의 인증정보를 설정한다.jenkins는 docker container를 이용하여 서비스하고 있으므로, host os든, container os든 둘중하나의 공간에서 rsa 인증방식을 사용할것이기에 private key를 지녀야한다.필자는 docker의 container 레벨에서 rsa key를 생성을 하였다.1$ ssh-keygen -t rsa 생성뒤엔 github에 등록할 rsa public key를 확인한다1$ cat ~/.ssh/id_rsa.pub 해당명령으로 rsa public key를 알아보았다. 이제 github에 등록해보자github -&gt; Settings -&gt; SSH and GPG Keysrsa_id.pub 값 등록후 Add SSH Key 개발중인 프로젝트에서 push 이벤트가 발생하면, jenkins에게도 event를 위임(delegate)하기위해 github의 추가설정을 진행한다.github -&gt; 프로젝트 -&gt; Settings -&gt; Integration &amp; Service다음과 같이 설정을 진행한다.주의사항은. jenkins가 돌고있는 url 뒤에는 /github-webhook/ 을 붙여주도록한다.(Install Notes에 상세히 기술되어있다.)이제 git과 jenkins의 연동과 설정은 모두끝이났다. jenkins는 언제든지 git으로부터 프로젝트를 clone 받을준비가 된것이다. 프로젝트 설정이제 본격적으로 jenkins에서 프로젝트를 설정해보자.jenkins -&gt; 새로운 item -&gt; Freestyle project[ 소스코드 관리 ]Repository URL -&gt; 프로젝트 ssh url을 입력한다.Credentials -&gt; Add조금전에 생성한 id_rsa private key 를 입력하도록한다1$ cat ~/.ssh/id_rsa 조금전에도 설명을 했듯이 jenkins와 git은 rsa 암호화 통신으로 인증을 수행하기에 , public key가 등록된 github와 jenkins는 private key를 등록을 하여야 인증이 성립되는것이다. private key를 입력한다.rsa 알고리즘은 private key와 public key 간의 비대칭 알고리즘이다. rsa 알고리즘 암호화 통신일경우, private key는 public key로부터 암호화된 것을 복호화한다. 당연히 그반대로는 되지않는다 .rsa 개념 참고[ 빌드유발 ]GitHub hook trigger for GITScm pollinggit에서 push event가 발생하면 jenkins가 자동으로 빌드한다.[ Build ]Add build step -&gt; Invoke top-level Maven targets -&gt; 고급jenkins가 maven을 활용하여 프로젝트를 빌드할때, pom.xml 에 기술되어있는 빌드설정을 참고한다. 필자는 다음과 같이 설정하였다.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384 ... &lt;!-- author by jaehunpark 현재 스프링 프로젝트 구조는 maven 기본 아키텍처에서 벗어났으므로, jenkins에서 maven을 빌드하면, src/main/java와 src/main/resources의 기본 아키텍처를 기반으로 빌드를 한다. 그러므로, maven이 기본 아키텍처외에도 자바소스와 resources들을 같이 빌드할수있도록 플러그인을통해 maven에게 경로를 알려주도록한다 --&gt;&lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;build-helper-maven-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;generate-sources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;add-source&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;sources&gt; &lt;!-- src/main/skeletone/java도 함께빌드를 하자 --&gt; &lt;source&gt;src/main/skeletone/java&lt;/source&gt; &lt;/sources&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;add-resource&lt;/id&gt; &lt;phase&gt;generate-resources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;add-resource&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;resources&gt; &lt;resource&gt; &lt;!-- src/main/skeletone/resources도 함께빌드를 하자. 이곳이 비지니스로직이 설정이 전부있다.--&gt; &lt;directory&gt;src/main/skeletone/resources&lt;/directory&gt; &lt;/resource&gt; &lt;resource&gt; &lt;!-- 추가로, 위의 소스레벨에 SQL xml이 함께있으므로, 소스레벨의 패키지를 적어주고 모든 SQL.xml을 검색해서 같이 resources로인식한다 --&gt; &lt;directory&gt;src/main/skeletone/java&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*SQL.xml&lt;/include&gt; &lt;/includes&gt; &lt;/resource&gt; &lt;resource&gt; &lt;!-- src/main/skeletone/java 디렉토리 하위의 *.SQL.xml resources에서 상위 패키지, src/main/java 내의 SQL을 끌어씀으로 .. --&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;includes&gt; &lt;!-- 상위 소스레벨의 SQL도함께 resources로 인식할수있도록 설정 .. --&gt; &lt;include&gt;**/*SQL.xml&lt;/include&gt; &lt;/includes&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;add-test-source&lt;/id&gt; &lt;phase&gt;generate-test-sources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;add-test-source&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;sources&gt; &lt;!-- 해당경로의 unit test를 수행하자 --&gt; &lt;source&gt;src/test/skeletone/java&lt;/source&gt; &lt;/sources&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- maven이 설치된 개발버신의 jdk버전과 빌드 타켓 버전을 설정하도록한다. 설정하지않을시 MojoFailureException 발생. 굉장히 낮은버전의 jdk로 mvn을 빌드하는듯. maven은 자동으로 jdk버전을 인식하지않으므로 꼭 설정해주도록하자 . --&gt;&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.6.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;$&#123;project.build.sourceEncoding&#125;&lt;/encoding&gt; &lt;/configuration&gt;&lt;/plugin&gt; ... 필자의 프로젝트 구조는 default 경로인 src/main/java 와 src/main/resources 경로에는 common 로직과 설정들이 존재한다.실제 프로젝트관련 비지니스로직들은 src/main/skeletone/java와 src/main/skeletone/resources에 존재한다. 따라서 maven에게 추가적인 경로의 소스와 설정들도 같이 빌드하도록 알려주도록 설정되어있다.[ 빌드 후 조치 ]이전에 설치한 플러그인을 사용할것이므로 [ Deploy war/ear to a container ] 추가하고,메일로 빌드결과를 받을것이기에, [ E-mail Notification ] 을 추가한다.이제 마지막으로 원격서버의 정보를 설정하고, 배포경로를 적어준다.필자는 tomcat 배포 경로를 / 로 설정해주었다.추가적으로 원격 서버의 tomcat 인증정보가 존재하여야한다. 이전의 포스트에도 해당내용이 있으니 참고하자.원격서버 인증 설정1$ vi conf/tomcat-users.xml 12&lt;role rolename=\"manager-script\"/&gt;&lt;user username=\"아이디\" password=\"비밀번호\" roles=\"manager-script\"/&gt; 이제 모든 설정이 끝이났다. local 개발환경에서 프로젝트에 commit후 push event를 날려보자. jenkins가 빌드부터 테스트, 배포, 그리고 결과통보까지 메일로 발송해줄것이다. 견해현재 포스트에서는 기본 설치된 플러그인들과 추가로 필요한 플러그인을 설치하여 자동화를 수행하였다. 이밖에도 jenkins에서 지원하는 플러그인이 아주 다양하게있으니, 필요한 플러그인을 추가로 설치하여 추구하는 방향으로 설정하면 jenkins의 가치를 알게될것이다.jenkins와같은 CI(Continuous Integration) 은 jenkins 를 비롯해 아주 다양하게 존재한다.travis CI도 nodejs 쪽에선 상당한 인기를 끄는것으로 알고있으며, 요즘들어 jenkins CI 보다는 다른 플랫폼의 CI를 더 많이 사용하는 느낌이기도하다. 필자도 travis를 한번 사용해봐야겠다는 생각이든다 (말로만 … )아무튼, CI가 어떤것이고 어떤 기능을 수행할수있는지 알게되었다면, 다른CI 들도 좀더 쉽게 접근할수있을것이라 생각된다.","categories":[{"name":"jenkins","slug":"jenkins","permalink":"https://setyourmindpark.github.io/categories/jenkins/"}],"tags":[]},{"title":"maven으로 remote서버 배포하기","slug":"java/java","date":"2017-04-12T06:23:28.000Z","updated":"2018-02-23T09:53:34.000Z","comments":true,"path":"2017/04/12/java/java/","link":"","permalink":"https://setyourmindpark.github.io/2017/04/12/java/java/","excerpt":"","text":"maven을 활용한 remote 서버 배포maven을 활용하여 로컬 개발환경 eclipse 또는 STS 환경에서 에서 remote로 원격배포가 가능하다.이런 경우는 대게 보안적으로 크게 신경쓰지않는 프로젝트나, 어떠한 이유에 의해서 jenkins와 같은 third party 배포 프레임웍을 사용하지않거나, 또는 팀단위가 아닌 개인이 간단하게 개발된것을 수시로 배포해야하는 상황이라면 적합하다고 생각한다. maven 설치먼저 배포하기에앞서, 시스템에 maven이 설치되어있어야한다.maven을 설치해보자 apache 에서 쉽게 받을수있다.3.5.0 버전이 최신버전임을 확인할수있다.( 저번주까지 3.3.9 버전이었는데 .. )다운을 받았다면 이제 시스템에 환경변수를 등록하자.필자는 OSX 를 사용하기에, 계정레벨 환경변수에 등록했다1$ vi ~/.bash_profile 123export M3_HOME=/Users/jaehunpark/Documents/jaehunpark/utility/apache-maven-3.5.0export M3=$M3_HOME/binexport PATH=$PATH:$M3 1$ source ~/.bash_profile 환경변수를 적용하였다면 maven 버전을 확인해보자1$ mvn -v maven 버전이 확인이된다면 정상적으로 설치가 된것이다. remote 서버 인증정보 설정로컬 환경에서 원격 서버로 자유롭게 배포하기위해선, 원격서버에서 배포자의 인증정보가 존재해야한다.tomcat의 디렉토리에서1$ vi conf/tomcat-users.xml 12&lt;role rolename=\"manager-script\"/&gt;&lt;user username=\"아이디\" password=\"비밀번호\" roles=\"manager-script\"/&gt; 를 입력하면 인증정보설정이 끝난것이다. 간단하지않은가 ?이밖에도 여러 옵션이있지만, manager-gui — Access to the HTML interface. manager-status — Access to the “Server Status” page only. manager-script — Access to the tools-friendly plain text interface that is described in this document, and to the “Server Status” page. manager-jmx — Access to JMX proxy interface and to the “Server Status” page. 원격서버로 배포하는것이 목적이므로 manager-script 권한만 설정하도록한다.자세한 정보는 tomcat manager doc에 아주 자세히나와있다 … 배포하기이제 모든 준비는 끝났다. 로컬 개발머신에서, 배포하고자하는 프로젝트를 build 하기만하면된다.배포하고자 하는 프로젝트의 maven pom.xml에 플러그인을 추가한다123456789101112&lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;configuration&gt; &lt;path&gt;/&lt;/path&gt; &lt;url&gt;http://remote서버ip:포트/manager/text&lt;/url&gt; &lt;username&gt;아이디&lt;/username&gt; &lt;password&gt;비밀번호&lt;/password&gt; &lt;update&gt;true&lt;/update&gt; &lt;/configuration&gt;&lt;/plugin&gt; 프로젝트 배포 최상의 경로 이름지정 http://remote서버ip:포트/manager/text 필자는 프로젝트 배포를 / 를 최상위로 설정하였다.다음으로 프로젝트 우클릭 -&gt; Run As -&gt; Maven buildGoals에 tomcat7:redeploy -&gt; Run maven이 빌드를 시작하게되고 unit test 까지 마친후 , 최종적으로 BUILD SUCCESS가 떨어졌다.이제 원격서버에 제대로 반영이 되었는지 확인해보자 필자는 page 대신 rest 기반을 프로젝트를 구성하였기에 api호출시 정상적으로 동작하는것을 확인할수있다. 견해필자는 원격서버 tomcat 버전이 7.XX을 사용하므로, tomcat7:redeploy 라는 명령을 사용하였으나,원격서버의 tomcat 버전이 8.XX 인경우에도 무방한것으로 확인된다.보안이 중요하지않은 프로젝트는 없겠지만, 개인이 간단히 테스트용으로 개발결과물을 빠르게 올리기에는 최고의 방법인것 같다.","categories":[{"name":"java","slug":"java","permalink":"https://setyourmindpark.github.io/categories/java/"}],"tags":[]},{"title":"jenkins","slug":"jenkins/jenkins","date":"2017-04-09T04:47:32.000Z","updated":"2018-02-23T09:45:47.000Z","comments":true,"path":"2017/04/09/jenkins/jenkins/","link":"","permalink":"https://setyourmindpark.github.io/2017/04/09/jenkins/jenkins/","excerpt":"","text":"jenkinsjenkins란 흔히들 CI(continuous integration) 이라고들 이야기한다.CI란 개발자가 개발한 결과물 에대해서 수정,배포를 용이하게 하는 일종에 중간에 비서같은 역활을 수행한다.jenkins 공식사이트를 보면, 상단에 중년의 남성이(?) 맞이하는걸 볼수있다. jenkins의 활용jenkins의 주목적은 위에서 언급한 바와같이 ‘지속적인 통합이다’.최종적으로 배포서버에 안정적으로 배포가 가능하도록 하는것이 jenkins가 추구하는 방향이다.그럼 ‘안정적인 배포’는 어떤의미일까.서비스를하는 기업은 개발한 서비스를 소비자가 이용하는데에 있어서 불편함이없어야한다.jenkins를 활용하면 배포전에 jUnit test(java), mocha(nodejs) 등 개발환경에따라 배포전 사전작업을 무엇이든지 시킬수있다.또한 test 수행 결과에 있어서 개발자들에게 메일발송이나, 소스코드 검증과같은 sonar qube 와 같은 third party plugin을 활용할수있다.우리의 든든한 비서는 무엇이든지 수행할수있다. jenkins 시작하기이전포스트에서 docker를 활용하여 jenkins 환경을 구성하였다.이 docker 컨테이너를 이용하여 jenkins를 시작해보자. docker container로 실행된 jenkins를 접속하게되면, password를 입력하라고 나오게된다.이제 jenkins 가 알려준 경로의 비밀번호를 확인해보자먼저 docker container에 접속한다1$ sudo docker exec -it jenkins /bin/bash 접속을하였다면 비밀번호를 출력하자1$ cat /root/.jenkins/secrets/initialAdminPassword 출력되는 비밀번호를 jenkins에 입력한다 . 비밀번호 입력후 jenkins가 제안하는 plugin을 설치한다.jenkins에 익숙한 분들이라면 select plugins to install 에서 플러그인들을 선택해서 원하는 플러그인을 설치해도 무방하다. 설치가 진행이 완료되면, jenkins 에 접속될 계정을 생성한다 이제 jenkins를 시작할 준비를 모두마쳤다 ! 견해jenkins를 사용하지않는다면, 개발한 결과물에대해서는 자체 test를 내부적으로 한번 진행을 하고, 부가적으로 filezilla와 같은 프로그램을통해 원격 서버에 접속을하여 직접 배포를 해야할것이다.특히 한번올리고 끝이아니라, 수정또는 기능추가로 버전이 올라가 잦은 재배포를 해야할경우에 jenkins의 가치가 증명될거라 생각된다.필자는 사실 모든걸 다알기전에는 실천하지않는 이상한 습관이있다.‘배운것을 깔짝 쓸바에야 전부다 알고 제대로 쓰겠다 ‘라는 아주 이상한 마인드가 있다.jenkins를 처음접하는 분들이라면 처음에는 좀 생소할수있으나, 한번두번씩 따라해다보면 금방 익숙해질것이다.","categories":[{"name":"jenkins","slug":"jenkins","permalink":"https://setyourmindpark.github.io/categories/jenkins/"}],"tags":[]},{"title":"docker-compose","slug":"docker/docker-2","date":"2017-04-08T06:17:45.000Z","updated":"2018-02-23T10:32:35.000Z","comments":true,"path":"2017/04/08/docker/docker-2/","link":"","permalink":"https://setyourmindpark.github.io/2017/04/08/docker/docker-2/","excerpt":"","text":"docker-composedocker 시작하기 이전포스트에서 docker의 설치와 이미지파일 생성등을 알아보았다.다음의 명령어는 생성된 이미지파일을 컨테이너로 실행하는 명령이다1$ sudo docker run -d -p 89:8080 --name jenkins -v /home/docker/jenkins/jenkins_data:/root jenkins 그렇다면, 생성된 이미지파일이 다수일경우, 위와같은 명령어를 이미지명과, volume 경로등 개별 옵션들을 변경하여 명령어를 입력해야한다다수의 이미지들을 정해진 설정으로 한번에 컨테이너로 모두 만들수는없을까 ?docker-compose 활용하면, 작성해둔 이미지별 설정정보로 한번에 여러이미지를 컨테이너로 만들수있다.설치부터 한번 알아보자123$ sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.9.0/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose$ sudo chmod +x /usr/local/bin/docker-compose docker-compose를 정상적으로 설치되었다면 버전을 확인해보자1$ sudo docker-compose --version 버전이 표시된다면, 정상적으로 설치가된것이다 . docker-compose 사용하기docker-compose를 사용하는방법으로, 먼저 docker-compose는 yml 확장자를 가진 파일을 생성하도록한다.1$ vi docker-compose.yml 이제 ‘이미지파일을 어떻게 실행할 것인가’ 에대한 설정을 시작한다12345678910111213141516171819202122232425262728293031323334version: &apos;2&apos;services: mariadb: image: mariadb ports: - &quot;3306:3306&quot; environment: - MYSQL_ROOT_PASSWORD=0000 - TZ=Asia/Seoul volumes: - /home/jaehunpark/docker/mariadb/mariadb_data:/var/lib/mysql command: - &quot;mysqld&quot; - &quot;--character-set-server=utf8mb4&quot; - &quot;--collation-server=utf8mb4_unicode_ci&quot; container_name: mariadb jenkins: image: jenkins ports: - &quot;89:8080&quot; volumes: - /home/jaehunpark/docker/jenkins/jenkins_data:/root container_name: jenkins deploy: image: deploy ports: - &quot;33:22&quot; - &quot;88:4000&quot; volumes: - /home/jaehunpark/docker/deploy/deploy_data:/root tty: true depends_on: - mariadb container_name: deploy 필자는 위와같이 docker-compose를 사용한다. mariadb는 순수 docker hub에서 제공하는 이미지를 사용하고, jenkins와 deploy 는 Dockerfile을 만들어 이미 이미지로 생성된 상태로 설정정보를 진행하였다.docker-compose 공식문서를 참고하도록하자 . docker-compose 를사용하여 컨테이너 실행1$ sudo docker-compose up -d -d 백그라운드로 실행 123mariadb is up-to-datedeploy is up-to-dateCreating jenkins 이미지가 이미 컨테이너로 실행되어있으면, up-to-date를, 이미지를 컨테이너로 처음 실행된다면 Creating jenkins를 표시한다.이제 컨테이너가 정상적으로 실행되었는지 확인해보자 .1$ sudo docker ps -a 1234CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES4a3cf0852e58 jenkins &quot;/opt/tomcat/bin/cata&quot; 4 minutes ago Up 4 minutes 0.0.0.0:89-&gt;8080/tcp jenkins5846e1ba049f deploy &quot;/usr/sbin/sshd -D&quot; 47 hours ago Up 47 hours 0.0.0.0:33-&gt;22/tcp, 0.0.0.0:88-&gt;3000/tcp, 0.0.0.0:87-&gt;4000/tcp deploy7a53b6623118 mariadb &quot;docker-entrypoint.sh&quot; 3 months ago Up 4 days 0.0.0.0:3306-&gt;3306/tcp mariadb 실행한후, 컨테이너가 제대로 실행되었는지, STATUS 탭 을 꼭확인하도록하자.만약 Exited 라고 표시되었다면 컨테이너 실행도중 죽은것이므로 docker-compose.yml 파일을 다시한번 체크하도록 하자. (특히 docker-compose.yml 에 기술하는 services들의 문법이 엄격하다 특히 띄어쓰기를 주의해야한다) 견해docker-compose를 통해 다중 이미지들을 컨테이너로 실행하는 법을 알아보았다.현재 글에서 docker-compose 버전은 2를 사용하고있다. 현재 3.1버전까지 나온상태로 확인이된다.음 .. docker-compose 3.x 를 사용한후 익숙해지면 새롭게 포스트를 올리도록 하겠다.","categories":[{"name":"docker","slug":"docker","permalink":"https://setyourmindpark.github.io/categories/docker/"}],"tags":[]},{"title":"docker 시작하기","slug":"docker/docker-1","date":"2017-04-07T14:23:37.000Z","updated":"2018-02-23T10:31:48.000Z","comments":true,"path":"2017/04/07/docker/docker-1/","link":"","permalink":"https://setyourmindpark.github.io/2017/04/07/docker/docker-1/","excerpt":"","text":"docker 설치설치환경은 linux ubuntu 16.04.2 LTS 환경에서 진행한다123$ sudo apt-get update$ sudo apt-get install docker.io$ sudo ln -sf /usr/bin/docker.io /usr/local/bin/docker 정상적으로 설치가 완료되었다면 이제, 버전을 확인해보자1$ sudo docker --version 현재시점의 docker 의 버전은 Docker version 1.12.6, build 78d1802 가 출력되는것을 확인할수있다 이미지 환경패키징docker는 배포에 특화된 기술이라고 이전포스트 에서 언급한바 있다. 한번 만들어놓은 이미지(환경이 패키징된)는 docker가 설치된 환경이라면 쉽게 생성할수있다.그럼 이미지파일은 어떻게 만들까 ?1$ vi Dockerfile vi 편집기로 Dockerfile을 열어, 환경정보를 구성한다이제 실제 환경정보를 기술하자12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273FROM debianMAINTAINER jaehunpark \"setyourmindpark@gmail.com\"RUN apt-get update &amp;&amp; DEBIAN_FRONTEND=noninteractive apt-get install -y localesRUN sed -i -e 's/# en_US.UTF-8 UTF-8/en_US.UTF-8 UTF-8/' /etc/locale.gen &amp;&amp; \\ echo 'LANG=\"en_US.UTF-8\"'&gt;/etc/default/locale &amp;&amp; \\ dpkg-reconfigure --frontend=noninteractive locales &amp;&amp; \\ update-locale LANG=en_US.UTF-8ENV LANG en_US.UTF-8ENV LANGUAGE en_US:enENV LC_ALL en_US.UTF-8# set timezoneRUN ln -sf /usr/share/zoneinfo/Asia/Seoul /etc/localtimeRUN \\ echo \"===&gt; add webupd8 repository...\" &amp;&amp; \\ echo \"deb http://ppa.launchpad.net/webupd8team/java/ubuntu trusty main\" | tee /etc/apt/sources.list.d/webupd8team-java.list &amp;&amp; \\ echo \"deb-src http://ppa.launchpad.net/webupd8team/java/ubuntu trusty main\" | tee -a /etc/apt/sources.list.d/webupd8team-java.list &amp;&amp; \\ apt-key adv --keyserver keyserver.ubuntu.com --recv-keys EEA14886 &amp;&amp; \\ apt-get updateRUN echo \"===&gt; install Java\" &amp;&amp; \\ echo debconf shared/accepted-oracle-license-v1-1 select true | debconf-set-selections &amp;&amp; \\ echo debconf shared/accepted-oracle-license-v1-1 seen true | debconf-set-selections &amp;&amp; \\ DEBIAN_FRONTEND=noninteractive apt-get install -y --force-yes oracle-java8-installer oracle-java8-set-defaultRUN echo \"===&gt; clean up...\" &amp;&amp; \\ rm -rf /var/cache/oracle-jdk8-installer &amp;&amp; \\ apt-get clean &amp;&amp; \\ rm -rf /var/lib/apt/lists/*ENV TOMCAT_VERSION=8.5.13# Get TomcatRUN wget --quiet --no-cookies http://apache.tt.co.kr/tomcat/tomcat-8/v$&#123;TOMCAT_VERSION&#125;/bin/apache-tomcat-$&#123;TOMCAT_VERSION&#125;.tar.gz -O /tmp/tomcat.tar.gz &amp;&amp; \\ tar xzvf /tmp/tomcat.tar.gz -C /opt &amp;&amp; \\ mv /opt/apache-tomcat-$&#123;TOMCAT_VERSION&#125; /opt/tomcat &amp;&amp; \\ rm /tmp/tomcat.tar.gzENV CATALINA_HOME /opt/tomcat# jenkins download#RUN wget --quiet --no-cookies http://mirrors.jenkins.io/war-stable/latest/jenkins.war -O $&#123;CATALINA_HOME&#125;/webapps/jenkins.warRUN wget --quiet --no-cookies http://mirrors.jenkins.io/war-stable/2.7.2/jenkins.war -O $&#123;CATALINA_HOME&#125;/webapps/jenkins.warENV MAVEN_VERSION=3.3.9# Get MavenRUN wget --quiet --no-cookies http://apache.tt.co.kr/maven/maven-3/$&#123;MAVEN_VERSION&#125;/binaries/apache-maven-$&#123;MAVEN_VERSION&#125;-bin.tar.gz -O /tmp/maven.tar.gz &amp;&amp; \\ tar xzvf /tmp/maven.tar.gz -C /opt &amp;&amp; \\ mv /opt/apache-maven-$&#123;MAVEN_VERSION&#125; /opt/maven &amp;&amp; \\ rm /tmp/maven.tar.gzENV M3_HOME /opt/mavenENV PATH=$&#123;M3_HOME&#125;/bin:$&#123;PATH&#125;RUN apt-get update &amp;&amp; \\ apt-get install -y vim &amp;&amp; \\ apt-get install -y git-core &amp;&amp; \\ apt-get install -y curl &amp;&amp; \\ curl -sL https://deb.nodesource.com/setup_6.x | bash - &amp;&amp; apt-get install -y nodejsVOLUME /rootEXPOSE 8080# Launch TomcatCMD [\"/opt/tomcat/bin/catalina.sh\", \"run\"] jenkins 환경구성을 예로든 Dockerfile을 한번 살펴보자.가장 상위에 FROM 이 보인다, 이부분은 상위의 이미지파일을 참조하겠다는 의미로 받아들이면 편할듯하다필자는 얼마전까지 openjdk 를 기반으로 jenkins 환경구성을 패키징하였으나, 스프링과 같은 자바 프레임웍을 빌드하는데있어 openjdk가 좀 찝집한면이 업지않아있어 ..debian 으로 구성하였다.물론 ubuntu와, centos등 지향하는 os를 선택해서 base로 사용할수있다.base 이미지는 dockerhub 에서 제공하는 녀석들만 해당되므로 참고하도록하자환경 구성에 필요한 다양한 명령어에 대한 설명은 docker document를 참고하도록 하자 . 이미지 생성12$ sudo docker build &lt;옵션&gt; &lt;Dockerfile 경로&gt;$ sudo docker build --tag jenkins . –tag : 이미지이름 명령어를 실행하면, docker 가 열심히 이미지를 제작하는것을 확인할수있다생성이 모두 끝났다면,1$ sudo docker images REPOSITORY 부분의 jenkins 가 보일것이다.만약 으로 표시된다면, 이미지생성에 실패했거나, 생성중 에러가 발생한것이므로, Dockerfile에 작성된 명령어를 다시한번 확인하도록 하자 이미지 삭제12$ sudo docker rmi &lt;옵션&gt; &lt;REPOSITORY:TAG or IMAGE ID&gt;$ sudo docker rmi jenkins -f : 이미지 강제삭제 이미지를 컨테이너로 실행12$ sudo docker run &lt;옵션&gt; &lt;REPOSITORY:TAG or IMAGE ID&gt;$ sudo docker run -d -p 89:8080 --name jenkins -v /home/docker/jenkins/jenkins_data:/root jenkins -d : 백그라운드로실행 -p : 포트포워딩 ex) -p 외부:내부 –name : 컨테이너 이름 (NAMES로 바인딩) -v(–volume) : 호스트 디렉토리와 컨테이너 내부디렉토리 공유 ex) –volume /testData:/test (호스트 /testData디렉토리와 컨테이너 /test 디렉토리 공유, 없으면 자동생성) –volumes-from : 이미호스트에 연결된 컨테이너 A를 다른 컨테이너 B,C,D와 연결컨테이너 A만 호스트와 연결하기위해 존재, B,C,D 는 A에 연결하여 사용 ex) volumes-from containerA -e : 환경변수 세팅 실행중인 컨테이너 확인12$ sudo docker ps &lt;옵션&gt;$ sudo docker ps -a -a : 모든 프로세스 보기 실행중인 컨테이너 중지12$ sudo docker stop &lt;NAMES OR CONTAINER ID&gt;$ sudo docker stop jenkins 컨테이너 삭제12$ sudo docker rm &lt;옵션&gt; &lt;NAMES OR CONTAINER ID&gt;$ sudo docker rm -f jenkins -f : 컨테이너 강제종료 컨테이너 접속12$ sudo docker exec &lt;옵션&gt; &lt;NAMES OR CONTAINER ID&gt; bash$ sudo docker exec -it jenkins /bin/bash -i : interactive 한 환경 구성 컨테이너와 상호작용 (input output) -t : 터미널과 같은 환경 조성 (tty) -u : 계정 견해위에서 예로든 Dockerfile 은 필자가 직접 환경을 패키징을 한것이고, 기본적으로 dockerhub 에서 제공하는 이미지파일도 굉장히 많다 (물론 docker hub에서 jenkins도 제공한다)각자가 추구하는 방향에 따라 docker를 사용한다면, 원하는 환경을 빠르게 구성할수있을것이라고 생각된다.docker 에대한 명령어나, 자세한 정보를 알고자면 공식문서 를 참조하는것이 좋겠다.","categories":[{"name":"docker","slug":"docker","permalink":"https://setyourmindpark.github.io/categories/docker/"}],"tags":[]},{"title":"docker","slug":"docker/docker","date":"2017-04-06T14:06:40.000Z","updated":"2018-02-06T05:30:10.000Z","comments":true,"path":"2017/04/06/docker/docker/","link":"","permalink":"https://setyourmindpark.github.io/2017/04/06/docker/docker/","excerpt":"","text":"docker도커란 무엇인가 ? 분산서버에대한 관리와 환경설정의 부담으로 2013년에 나타난 새로운기술 하드웨어 스펙이 좋아짐에따라 분산서버를 구성할 필요성을 덜 느끼게되어 하나의 물리서버에서 분산된 서버구축할수있는 효과를 가져다줌 도커는 서비스 운영환경에 필요한 (node, jdk, mysql 등등) 과 같은 서버환경에 필요한 운영설정등을 패키징된 것이 이미지 이미지파일을 실행한 상태를 컨테이너 서버의 운영과 배포에 특화된 기술 가상머신가 무엇이다른가 ? 가상머신과 달리 게스트 OS를 설치할 필요가없음 가상머신은 호스트 OS 안에 게스트 OS로 구동되는 또하나의 OS 지만, 도커는 설정 정보만 기술하여 이미지생성, 컨테이너 생성후 모든 실행프로세스는 게스트 OS에서 직접 실행됨 (가상이 아닌 격리) 가상머신의 역활을 할뿐만아니라, 컨테이너 간의 환경정보 공유 docker의 장점 하나의 물리서버에서 분산서버를 구축한 효과를 가져다줌 게스트 OS 설치가 없으므로 시간과 불필요한 시스템 자원을 낭비 X 무엇보다도 서버설치에 들어가는 환경구성,설치, 설정 과 같은 작업들을 쉽게 구성할수있을뿐아니라, 설정을통해 만들어진 이미지파일을 손쉽게 재사용이 가능 견해프로젝트를 진행하다, docker와 유사한 XenServer 라는 기술을 얼핏 접하게되었는데,docker와 상당히 유사한점이 많았다.docker와 비교하자면, 리눅스기반 XenServer가 실제 물리서버가 되는것이고 (docker 의 host os) 그 하위의 가상의 os를 설치하는식이었다.이런 VM(virtual machine) 들은 docker와 가장큰 차이를 보이는것은, 생산성이라고 생각된다docker 같은경우는 docker hub와 같은 커뮤니티를 통해 os와 함께 필요한 구성이 패키징된 이미지를 아주쉽게 pull 할수있는반면, XenServer 와같은 VM 들은 가상의 OS를 이미지파일로 새로 설치한다는 점이다. (물론 환경구성도 직접 해야한다)XenServer 를 세부적으로 다뤄보진않았지만 음 .. docker 를 사용하다 XenServer란것도 있구나 … 이정도 ?","categories":[{"name":"docker","slug":"docker","permalink":"https://setyourmindpark.github.io/categories/docker/"}],"tags":[]},{"title":"nvm","slug":"nodejs/nodejs","date":"2017-04-06T10:56:50.000Z","updated":"2018-02-06T05:28:31.000Z","comments":true,"path":"2017/04/06/nodejs/nodejs/","link":"","permalink":"https://setyourmindpark.github.io/2017/04/06/nodejs/nodejs/","excerpt":"","text":"nvmnode 를 기반으로 개발하는 개발자라면 node 를 시스템이 설치할것이다.nvm이란 flexible 하게 node 버전을 관리할수있으므로, 기왕 node 를 설치하고자한다면, nvm을 활용하여 node 버전을 쉽게 관리할수있다. 설치nvm 관련 정보는 이곳 을 참조하면 nvm에대한 다양한 명령어와 정보를 확인할수있다.1$ curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.33.1/install.sh | bash nvm 명령어를 사용하기위해 환경변수 등록 (~/.bashrc, ~/.profile, or ~/.zshrc)한다필자는 계정레벨에서만 nvm을 사용할것이므로 ~/.bash_profile 에 환경변수를 등록한다12export NVM_DIR=&quot;$HOME/.nvm&quot;[ -s &quot;$NVM_DIR/nvm.sh&quot; ] &amp;&amp; . &quot;$NVM_DIR/nvm.sh&quot; # This loads nvm 이제 nvm이 잘 설치되었는지 버전을 확인해보자1$ nvm --version nvm 버전정보가 나온다면 nvm이 제대로 설치된것임을 확인할수있다. node 설치1$ nvm install node 버전정보를 지정해주지 않을시 nvm은 node의 가장 최신버전을 설치한다그렇다면 nvm이 설치가능한 node 모든 버전을 확인할수있을까 ?1$ nvm ls-remote 설치할수있는 모든 node의 버전이 표시된다이제 특정 node 버전을 설치해보자1nvm install &lt;버전&gt; nvm ls-remote 로 확인한 목록중 특정버전을 위와같이 설치할수있다.기본적으로 새롭게 설치한 노드의 버전이 default로 시스템에 잡혀있을것이다.1$ node --version 방금 설치한 node의 버전이 자동으로 시스템에서 사용중임을 알수있다.nvm을 통해 현재 시스템에 설치된 node의 리스트를 확인해보자1$ nvm ls 현재 시스템에 설치된 모든 node 의 버전들이 표시된다.설치된 node 버전중 시스템에서 특정버전을 사용할수있도록 설정해보자1$ nvm use &lt;버전&gt; 1$ node --version 시스템이 사용하는 node 버전이 변경됨을 확인할수있다.but 터미널을 종료하고 새로운 터미널에서 node 버전을 재확인시, nvm을 통해 가장 마지막에 설치한 node의 버전으로 되돌아가있을것이다 ..시스템 node의 버전을 특정 버전으로 고정시키자1$ nvm alias default &lt;버전&gt; 시스템에 특정 node 버전을 default로 사용하도록한다 의견node 7.XX에서는 차후 es7애 탑제될 async, await 문법이 사용가능한것으로 안다.실제로 stackoverflow등에서 이미 es7 문법에대한 커뮤니티가 활발한것을 확인할수있다.node의 버전이 새롭게 나올때마다 새로운 node의 버전을 설치하는것 보다는, node의 버전을 골라서 시스템에서 사용할수있는점이 nvm의 가장큰 매력이라고 생각된다 참고필자는 nvm을 통해 node를 설치하였을시, 자동으로 최신버전인 v7.8.0 이 설치되었다.‘와 벌써 node 버전이 이만큼이나 높아졌네 ? ‘ 라고생각한후, hexo 를 설치하는데… 설치도중 에러가 ….결국 node 공식 사이트에서 제공하는 stable 버전 을 확인한후, (현재시점은 v6.10.1 ) nvm으로 해당버전을 재설치후 hexo를 다시 설치하니 정상적으로 설치가되는 이슈가있었다..이밖에도 hexo 뿐만아니라, node를 기반으로 사용하는 다른 third party 라이브러리에서 node의 최신버전이 지원되지않을수도있으니.. 참고하도록하자","categories":[{"name":"nodejs","slug":"nodejs","permalink":"https://setyourmindpark.github.io/categories/nodejs/"}],"tags":[]}]}